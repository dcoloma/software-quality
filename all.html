<!DOCTYPE html>
<html>
  <head>
    <title>Software Quality - Teaching Notes</title>
    <meta charset='utf-8'>
    <script src='js/respec-w3c-common.js' async class='remove'></script>
    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
    <script class='remove'>
      var respecConfig = {
          specStatus: "unofficial",
          overrideCopyright: "<p class='copyright'> This document is licensed under a <a class='subfoot' href='http://creativecommons.org/licenses/by/3.0/' rel='license'>Creative Commons Attribution 3.0 License</a>. </p>",
          // specification status (e.g. WD, LCWD, WG-NOTE, etc.). If in doubt use ED.
          // the specification's short name, as in http://www.w3.org/TR/short-name/
          shortName:            "xxx-xxx",
          // if your specification has a subtitle that goes below the main
          // formal title, define it here
          // subtitle   :  "an excellent document",
          // if you wish the publication date to be other than the last modification, set this
          // publishDate:  "2009-08-06",
          // if the specification's copyright date is a range of years, specify
          // the start date here:
          // copyrightStart: "2005"
          // if there is a previously published draft, uncomment this and set its YYYY-MM-DD date
          // and its maturity status
          // previousPublishDate:  "1977-03-15",
          // previousMaturity:  "WD",
          // if there a publicly available Editor's Draft, this is the link
          // edDraftURI:           "http://berjon.com/",
          // if this is a LCWD, uncomment and set the end of its review period
          // lcEnd: "2009-08-05",
          // editors, add as many as you like
          // only "name" is required
          editors:  [
              {
                  name:       "Daniel Coloma"
              ,   url:        "https://dcoloma.github.io/"
              ,   mailto:     "danielcoloma@gmail.com"
              ,   company:    "USJ"
              ,   companyURL: "http://www.usj.es/"
              },
          ],
          
          // name of the WG
          wg:           "In Charge Of This Document Working Group",
          
          // URI of the public WG page
          wgURI:        "http://example.org/really-cool-wg",
          
          // name (without the @w3c.org) of the public mailing to which comments are due
          wgPublicList: "spec-writers-anonymous",
          localBiblio:  {
            "MANAGEMENT-VS-QUALITY": {
            title:    "Management Can Make Quality (Im)possible"
            ,   authors:  [
              "DeMarco, T." 
            ]
            },
            "QUALITY-SOFTWARE-MANAGEMENT": {
            title:    "Qality Software Management"
            ,   authors:  [
              "Gerald Weinberg" 
            ]
            },
            "DEBT-ANALOGY": {
            title:    "The Debt Analogy"
            ,   href:     "http://www.codinghorror.com/blog/2009/02/paying-down-your-technical-debt.html"
            },
            "IEEE-QA-TEMPLATE": {
            title:    "IEEE SQA Plan Template"
            ,   href:     "http://www.scribd.com/doc/7428795/IEEE-Software-Quality-Assurance-Plan-Template"
            },
            "INSPECTIONS-AND-REVIEWS": {
            title:    "Inspections and Reviews"
            ,   href:     "http://www.cs.toronto.edu/~sme/CSC444F/slides/L09-Inspections.pdf"
            ,   publisher:  "University of Toronto"
            },
            "TRUTHS-PEER-REVIEWS": {
            title:    "Seven Truths about peer reviews"
            ,   href:     "http://www.processimpact.com/articles/seven_truths.html"
            ,   authors:  [
              "Karl E. Wiegers" 
            ]
            },
            "PERFORMANCE-RB-NVP-SCOP": {
            title:    "Comparative Performability Evaluation of RB, NVP and SCOP"
            ,   href:     "http://bonda.cnuce.cnr.it/Documentation/Papers/file-CBS94-C9402-104.pdf"
            ,   authors:  [
              "Silvano Chiaradonna1" ,
              "Andrea Bondavalli" ,
              "Lorenzo Strigini"
            ]
            },
            "COST-OF-QUALITY": {
            title:    "Cost of Quality Not only failure costs"
            ,   href:     "http://www.isixsigma.com/index.php?option=com_k2&view=item&id=937:cost-of-quality-not-only-failure-costs&Itemid=187"
            },
            "RELIABILITY-MATHS": {
            title:    "Basic Reliability Mathematics"
            ,   href:     "http://infohost.nmt.edu/~olegm/484/Chap3.pdf"
            },
            "WEIBULL-BASICS": {
            title:    "Characteristics of Weibull Distribution"
            ,   href:     "http://www.weibull.com/hotwire/issue14/relbasics14.htm"
            },
            "SOFTWARE-RELIABILITY": {
            title:    "Software Reliability"
            ,   href:     "http://users.ece.cmu.edu/~koopman/des_s99/sw_reliability/"
            },
            "LOC-HISTORY": {
            title:    "A Short History of the LOC Metric"
            ,   href:     "http://namcookanalytics.com/wp-content/uploads/2013/07/LinesofCode2013.pdf"
            },
            "ZEN-AND-THE-ART-OF-MOTORCYCLE-MAINTENANCE": {
            title:    "Zen and the Art of Motorcycle Maintenance"
            ,   href:     "https://en.wikipedia.org/wiki/Zen_and_the_Art_of_Motorcycle_Maintenance"
            ,   authors:  [
              "Robert M. Pirsig " 
            ]
            },
            "CHALLENGES-FAULT-TOLERANT-SYSTEMS": {
            title:    "Challenges in Building Fault-Tolerant Flight Control System for A Civil Aircraft"
            ,   href:     "http://www.iaeng.org/IJCS/issues_v35/issue_4/IJCS_35_4_07.pdf"
            ,   authors:  [
                "M. Sghairi " 
              , "A. de Bonneval"
              , "Y. Crouzet"
              , "J.J. Aubert"
              , "P. Brot"
            ]
            },
            "DEFECT-ANALYSIS-AND-PREVENTION": {
            title:    "Defect Analysis and Prevention for Software Process Quality Improvement"
            ,   href:     "http://www.ijcaonline.org/volume8/number7/pxc3871759.pdf"
            ,   authors:  [
              "Kumaresh Sakhti"
              , "Baskaran R"
            ]
            },
            "SOFTWARE-FAILURE-ANALYSIS-HP": {
            title:    "Software Failure Analysis for High-Return Process Improvement Decisions"
            ,   href:     "http://www.hpl.hp.com/hpjournal/96aug/aug96a2.pdf"
            ,   authors:  [
              "Grady, Robert B" 
            ]
            },
            "ODC": {
            title: "Orthogonal Defect Classification - A Concept For In-Process Measurements "
            ,   href:     "http://www.chillarege.com/articles/odc-concept.html"
            ,   authors:  [
              "Chillarege, Ram" 
            ]
            },
            "DEFECT-PREVENTION-NUTSHELL": {
            title:    "Software Defect Prevention in a nutshell"
            ,   href:     "http://www.isixsigma.com/industries/software-it/software-defect-prevention-nutshell/"
            ,   authors:  [
              "Purushotham Narayana"
            ]
            },
            "ART-OF-TESTING": {
            title:    "The Art of Software Testing"
            ,   authors:  [
              "Glenford J. Myers" ,
              "Corey Sandler" ,
              "Tom Badget"
            ]
            },
            "COST-EFFECTIVE-FAULT-TOLERANCE": {
            title:    "A Cost-Effective and Flexible Scheme for Software Fault Tolerance"
            ,   authors:  [
              "Andrea Bondavalli et al" 
            ]
            },
            "LINUS-SCM-GOOGLE": {
            title:    "Linus Torvalds view on SCM"
            ,   href:     "http://www.youtube.com/watch?v=4XpnKHJAok8"
            ,   authors:  [
              "Torvalds, L." 
            ]
            },
            "SUCCESSFUL-GIT-BRANCHING": {
            title:    "A successful Git branching model"
            ,   href:     "http://nvie.com/posts/a-successful-git-branching-model/"
            ,   authors:  [
              "Vincent Driessen" 
            ]
            },
            "TRAVIS-CI": {
            title:    "Travis Continuous Integration System"
            ,   href:     "https://travis-ci.org"
            },
            "SOFTWARE-ENGINEER-PRACTICIONER": {
            title:    "Software Engineering: A Practitioner's Approach"
            ,   href:     "http://highered.mheducation.com/sites/0078022126/information_center_view0/index.html"
            ,   authors:  [
              "Rogert S. Pressman"
              , "Bruce R. Maxim"
            ]
            }
          },
          
          // URI of the patent status for this WG, for Rec-track documents
          // !!!! IMPORTANT !!!!
          // This is important for Rec-track documents, do not copy a patent URI from a random
          // document unless you know what you're doing. If in doubt ask your friendly neighbourhood
          // Team Contact.
          wgPatentURI:  "",
          // !!!! IMPORTANT !!!! MAKE THE ABOVE BLINK IN YOUR HEAD
      };
    </script>
  </head>
  <body>
    <section id='abstract'>
      <p>
        These are the notes for Sofware Quality at USJ
      </p>
    </section>
    <section id='sotd'>
      <p>
        Early Draft
      </p>
    </section>
    <section> <!-- Unit 1 -->
      <h1> Introduction to Software Quality </h1>
      <section>
        <h2>Why Software Quality matters</h2>
        <p>
          Let's have a look at one of the most famous bugs of the whole
          Software History, the bug that AT&T suffered in 1990.
        </p>
        <section>
          <h3>An example of the sequences of Poor Quality Software</h3>
          <section>
            <h4>What happened?</h4>
            <p>
              At 2:25 PM on Monday, January 15th, network managers at AT&T's 
              Network Operations Centre began noticing an alarming number of red
              warning signals coming from various parts of their network. Within
              seconds, the network warnings were rapidly spreading from one 
              computer-operated switching Centre to another. The managers tried 
              to bring the network back up to speed for nine hours, while 
              engineers raced to stabilize the network, almost 50% of the calls 
              placed through AT&T failed to go through until at 11:30 PM, when 
              network loads were low enough to allow the system to stabilize.
            </p>
            <p>
              AT&T alone lost more than $60 million in unconnected calls. Of 
              course, there were many additional consequences difficult to be 
              measured such as business that could be done because relied on
              network connectivity.
            </p>
          </section>
          <section>
            <h4>The system was failure tolerant, wasn't it?</h4>
            <p>
              AT&T's long-distance network was a model of reliability and 
              strength.  On any given day, AT&T's long-distance service, 
              which at 1990 carried over 70% of the US long-distance traffic.
            </p>
            <p>
              The backbone of this massive network was a system of 114 
              computer-operated electronic switches (4ESS) distributed across 
              the United States. These switches, each capable of handling up to 
              700,000 calls an hour, were linked via a cascading network known 
              as Common Channel Signalling System No. 7 (SS7). When a telephone 
              call was received by the network from a local exchange, the switch
              would asses 14 different possible routes to complete the call. At
              the same time, it passed the telephone number to a parallel 
              signalling network that checked the alternate routes to determine
              if the switch at the other end could deliver the call to it's 
              local company. If the destination switch was busy, the original 
              switch sent the caller a busy signal and released the line. If the
              switch was available, a signal-network computer made a reservation
              at the destination switch and ordered the destination switch to 
              pass the call, after the switches checked to see if the 
              connection was good. The entire process took only four to six 
              seconds.
            </p>
          </section>
          <section>
            <h4>What went wrong?</h4>
            <p>
              The day the bug popped-up, a team of 100 frantically searching 
              telephone technicians identified the problem, which began in New 
              York City. The New York switch had performed a routine self-test 
              that indicated it was nearing its load limits. As standard 
              procedure, the switch performed a four-second maintenance reset 
              and sent a message over the signalling network that it would take
              no more calls until further notice. After reset, the New York 
              switch began to distribute the signals that had backed up during 
              the time it was off-line. Across the country, another switch 
              received a message that a call from New York was on it's way, and 
              began to update its records to show the New York switch back 
              online. A second message from the New York switch then arrived, 
              less than ten milliseconds after the first. Because the first 
              message had not yet been handled, the second message should have 
              been saved until later. A software defect then caused the second 
              message to be written over crucial communications information. 
              Software in the receiving switch detected the overwrite and 
              immediately activated a backup link while it reset itself, but 
              another pair of closely timed messages triggered the same 
              response in the backup processor, causing it to shut down also. 
              When the second switch recovered, it began to route it's 
              backlogged calls, and propagated the cycle of close-timed messages
              and shut-downs throughout the network. The problem repeated 
              iteratively throughout the 114 switches in the network, blocking 
              over 50 million calls in the nine hours it took to stabilize the 
              system.
            </p>
          </section>
          <section>
            <h4>The roots of the issue</h4>
            <p>
              The cause of the problem had come months before. Early December,
              technicians had upgraded the software to speed processing of 
              certain types of messages. Although the upgraded code had been 
              rigorously tested, a one-line bug was inadvertently added to the 
              recovery software of each of the 114 switches in the network. 
              The defect was a C program that featured a break statement 
              located within an if clause, that was nested within a switch 
              clause. In pseudo-code, the program read as follows:
            </p>
            <div class="example">
              <pre class="example highlight prettyprint">
                1  while (ring receive buffer not empty and side buffer not empty) DO
                2    Initialize pointer to first message in side buffer or ring receive buffer
                
                3    get copy of buffer

                4    switch (message) {
                5       case (incoming_message):
                6             if (sending switch is out of service) DO {
                7                 if (ring write buffer is empty) DO
                8                     send "in service" to status map
                9                 else
                10                    break
                              } // END IF
                11            process incoming message, set up pointers to optional parameters
                12            break
                     } // END SWITCH
                13   do optional parameter work
              </pre>
            </div>
            <p>
              When the destination switch received the second of the two 
              closely timed messages while it was still busy with the first 
              (buffer not empty, line 7), the program should have dropped 
              out of the if clause (line 7), processed the incoming message, 
              and set up the pointers to the database (line 11). Instead, 
              because of the break statement in the else clause (line 10), 
              the program dropped out of the case statement entirely and 
              began doing optional parameter work which overwrote the data 
              (line 13). Error correction software detected the overwrite 
              and shut the switch down while it could reset. Because every 
              switch contained the same software, the resets cascaded down 
              the network, incapacitating the system.
            </p>
          </section>
          <section>
            <h4>Lessons Learned</h4>
            <p>
              Unfortunately, it is not difficult for a simple software error 
              to remain undetected, to later bring down even the most reliable 
              systems. The software update loaded in the 4ESSs had already 
              passed through layers of testing and had remained unnoticed 
              through the busy Christmas season. AT&T was fanatical about its 
              reliability. The entire network was designed such that no single 
              switch could bring down the system. The software contained 
              self-healing features that isolated defective switches. The 
              network used a system of "paranoid democracy," where switches and
              other modules constantly monitored each other to determine if they
              were "sane" or "crazy." Sadly, the Jan. 1990 incident showed the 
              possibility for all of the modules to go "crazy" at once, how 
              bugs in self-healing software can bring down healthy systems, and 
              the difficulty of detecting obscure load- and time-dependent 
              defects in software.
            </p>
          </section>
        </section>
        <section>
          <h3>Software Crisis</h3>
          <p>
            But we could think that this bug occurred a while ago and that 
            nowadays we have more advanced technologies, methodologies, 
            training systems and developers.
          </p>
          <p>
            Is this really true? Just partially, it's true Software
            Development has evolved a lot, but the type of problems that are 
            solved via Software has also evolved, every day with try to solve
            more problems and more complex via software.
          </p>
          <p>
            The <i>Software Crisis</i> term was coined by USA Department of 
            Defence years ago in order to describe that the complexity of the 
            problems addressed of software has outpaced the improvements in 
            the software creation process as shown graphically in 
            <a href="#fig-the-software-complexity-evolution"></a>.
          </p>
          <div class="note">
            <p class=""> 
              "Few fields have so large a gap between best current practice 
              and average current practice."
            </p>
            <p class="">
              Department of Defence
            </p>
          </div>
          <figure>
            <img src='images/unit1-fig0-complexity.png'>
            <figcaption>The Software Complexity Evolution</figcaption>
          </figure>
          <p>
            In other words, the software creation process has evolved very little 
            while the problems software is solving are way too much complex 
          </p>
          <div class="note">
            <p class="">
              "We have repeatedly reported on cost rising by millions of 
              dollars, schedule delays, of not months but years, and 
              multi-billion-dollar systems that don't perform as envisioned. 
              The understanding of software as a product and of software 
              development as a process is not keeping pace with the growing 
              complexity and software dependence of existing and emerging 
              mission-critical systems."
            </p>
            <p>
              Government Accounting Office
            </p>
          </div>
          <p> 
            Additionally, as depicted in 
            <a href="#fig-the-software-resources-evolution"></a>, 
            the need of software developers has increased exponentially, 
            because more software is needed as software is used in nearly 
            every product with a minimum of complexity.  Whereas the need of 
            developers has increased exponentially, the availability of 
            developers has unfortunately not grown at the same pace, i.e. 
            there are less developers than what is needed. Due to that, people
            without the right skills have started developing software, with 
            the believe that developing software is an easy task that nearly 
            everybody could do. Developing software with people not properly 
            trained or without the right skills inherently leads to bad 
            quality software.
          </p>
          <figure>
            <img src='images/unit1-fig0-resources.png'>
            <figcaption>The Software Resources Evolution</figcaption>
          </figure>
        </section>
        <section>
          <h3>Legal Warranties</h3>
          <p>
            Mortenson, a construction contractor purchased software from 
            Timberline Software Corporation, which Timberline installed 
            in Mortenson's computers.  Mortenson, relying on the software, 
            placed a bid which was $1.95 million too low because a bug in 
            the software of which Timberline was aware. The State of 
            Washington Supreme Court ruled in favour of Timberline Software. 
            However, a simple bug in the software lead to multiple problems 
            to both companies. In the US Warranty Laws, the Article 2 of the 
            Uniform Commercial Code includes the "Uniform Computer Information
            Transaction Act" UCITA) that allows software manufacturers to: 
          </p>
            <ul>
            <li>
              Disclaim all liability for defects 
            </li>
            <li>
              Prevent the transfer of software from person to person remotely 
            </li>
            <li>
              Disable licensed software during a dispute
            </li>
          </ul>
          <p>
            That act, practically means that software distributors can limit
            their liability through appropriate clauses in the contracts. For
            instance, below is shown the disclaimer of warranties of a
            Microsoft product. Although the law overprotects software
            developers and distributors, using these disclaimers may prevent
            legal problems, but there are multiple additional problems
            related with poor software that are not avoided by them.
          </p>
          <div class="note">
            <p class="">
              DISCLAIMER OF WARRANTIES. TO THE MAXIMUM EXTENT PERMITTED BY
              APPLICABLE LAW, MICROSOFT AND ITS SUPPLIERS PROVIDE TO YOU THE
              SOFTWARE COMPONENT, AND ANY (IF ANY) SUPPORT SERVICES RELATED
              TO THE SOFTWARE COMPONENT ("SUPPORT SERVICES") AS IS AND WITH
              ALL FAULTS; AND MICROSOFT AND ITS SUPPLIERS HEREBY DISCLAIM
              WITH RESPECT TO THE SOFTWARE COMPONENT AND SUPPORT SERVICES ALL
              WARRANTIES AND CONDITIONS, WHETHER EXPRESS, IMPLIED OR
              STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY (IF ANY)
              WARRANTIES OR CONDITIONS OF OR RELATED TO: TITLE, NON-
              INFRINGEMENT, MERCHANTABILITY, FITNESS FOR A PARTICULAR
              PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS OF
              RESPONSES, RESULTS, LACK OF NEGLIGENCE OR LACK OF WORKMANLIKE
              EFFORT, QUIET ENJOYMENT, QUIET POSSESSION, AND CORRESPONDENCE
              TO DESCRIPTION. THE ENTIRE RISK ARISING OUT OF USE OR
              PERFORMANCE OF THE SOFTWARE COMPONENT AND ANY SUPPORT SERVICES
              REMAINS WITH YOU.
            </p>
          </div>
          <p>
            Although the law overprotects software developers and
            distributors, using these disclaimers may prevent legal problems,
            but it is just a way to avoid the legal problems of having bad
            quality software not solving the real problem that is what affect
            and frustrates end users.
          </p>
        </section>
      </section>
      <section>
        <h2> What is Software Quality?</h2>
        <p>
          Many people have tried to define what does Software Quality mean.
          However, it is not an easy task. Quality in general (not only in
          Software) is such a subjective topic that trying to define it
          formally is extremely challenging.
        </p>
        <p>
          There is a very interesting book called "Zen and the Art of
          Motorcycle Maintenance" [[ZEN-AND-THE-ART-OF-MOTORCYCLE-MAINTENANCE]]
          in which the narrator talks about the process of creative writing,
          and specially about quality.  The quality of a written text is
          difficult to define. If you ask people to rank essays (or programs)
          from best to worst it is very likely they reach a consensus "they
          have an intuitive understanding that one essay has more quality than
          another" but it's much more difficult to identify the parts of the
          essay that give it quality. 
        </p>
        <p>
          In Zen and the Art of Motorcycle Maintenance, Pirsig (the author)
          explores the meaning and concept of quality, a term he deems to be
          undefinable. Pirsig's thesis is that to truly experience quality one
          must both embrace and apply it as best fits the requirements of the
          situation. According to Pirsig, such an approach would avoid a great
          deal of frustration and dissatisfaction common to modern life.
        </p>
        <p>
          Let's think about another example of how the situation determines
          the quality. For instance, a master chef has prepared an exquisite
          meal and invited a group of friends to share it at her restaurant
          on a lovely summer evening. Unfortunately the air conditioning
          isn't working at the restaurant, the waiters are surly, and two
          of the friends have had a nasty argument on the way to the
          restaurant that dominates the dinner conversation. The meal itself
          is of the highest quality but the experiences of the diners are
          not.
        </p>
        <p>
          You could think that writing code is very different to writing an
          essay, but that is not the case. Usually, when you have a look at a
          piece of code it is easy for you to determine if you like it or
          not, but it becomes quite complicated to assess why.
        </p>
        <section>
          <h3>View 1: Formal Definition</h3>
          <p>
            Software quality may be defined as conformance to explicitly
            stated functional and performance requirements, explicitly
            documented development standards and implicit characteristics
            that are expected of all professionally developed software.
          </p>
          <p>
            This definition emphasis from three points:
          </p>
          <ol>
            <li>
              Software requirements are the foundations from which quality is
              measured: Lack of conformance to requirement is lack of
              quality.
            </li>
            <li>
              Specified standards define a set of development criteria that
              guide the manager is software engineering: If criteria are not
              followed lack of quality will almost result.
            </li>
            <li>
              A set of implicit requirements often goes unmentioned, like for
              example ease of use, maintainability, etc.: If software
              confirms to its explicit requirement but fails to meet implicit
              requirements,software quality is suspected.
            </li>
          </ol>
          <p>
            For the first item, explicit software requirements, it is going
            to be relatively easy to check objectively the conformance to
            them, for the second one, it is going to be more complicated and
            depends on how documented those standards are, for the implicit
            characteristics expected, it is going to be even tougher, as
            measuring conformance to something that is implicit, is, by
            definition, impossible.
          </p>
        </section>
        <section>
          <h3>View 2: The Human Point of View</h3>
          <p> 
            Those "implicit" requirements mentioned in the formal definition
            are a hint to indicate that there is something more about
            Software that goes beyond the explicit requirements. At the end
            of the day, software is going to be used by people, which do not
            care about the requirements but about their expectations. Hence,
            the need to look for another point of view.
          </p>
          <p>
            "A product's quality is a function of how much it changes the
            world for the better." [[MANAGEMENT-VS-QUALITY]] or "Quality is
            value to some person" [[QUALITY-SOFTWARE-MANAGEMENT]]. Both
            definitions stress that the quality may be subjective. I.e.
            different people are going to perceive different quality in the
            same software.  The software developers should also think about
            end users and asking themselves questions such as "How are users
            going to use the software?".
          </p>
          <p>
            In order to provide a more complete picture, IEEE standard 
            610.12-1990 combines both views in their definitions of quality:
          </p>
          <div class="note">
            <p>
              Software quality is
            </p>
            <ul>
              <li>
                The degree to which a system, component, or process meets 
                specified requirements.
              </li>
              <li>
                The degree to which a system, component or process meets 
                customer or user needs or expectations.
              </li>
            </ul>
          </div>
        </section>
        <section>
          <h3>View 3: Internal vs. External Quality</h3>
          <p>
            There is another dimension of Software Quality that depends on
            whether we focused on the part of the Software that is exposed to
            the users or on the part of the Software that is not.
          </p>
          <p>
            External Quality is the fitness for purpose of the software, i.e.
            does the software what it is supposed to do?. The typical way to
            measure external quality is through functional tests and bugs
            measurement. 
          </p>
          <p>
            Usually this is related to the conformance requirements that
            affect end-users (formal definition) as well as to meeting the
            end-user expectations (human point of view).
          </p>
          <p>
            Some of the properties that determine the external quality of 
            software are:
          </p>
          <ul>
            <li>
              Conformance to the product specifications and user expectations.
            </li>
            <li>
              Reliability: Is the software working with the same level 
              performance under different conditions and during all the time.
            </li>
            <li>
              Accuracy: Does the software do exactly what is supposed to do.
            </li>
            <li>
              Ease of use and comfort: Is the software easy to use and 
              responds in an amount time according to user expectations?
            </li>
            <li>
              Robustness: Does the software adapt to unforeseen situations, 
              e.g. invalid input parameters, connectivity lost... 
            </li>
          </ul>
          <p>
            Internal Quality is everything the software does but is never
            seen directly by the end-user. It's the implementation, which the
            customer never directly sees. Internal quality can be measured by
            conformance requirements (not focused on end-users but on
            software structure), software analysis and adherence to
            development standards or best practices.
          </p>
          <p>
            If it is not visible to end-user, and our target is make
            customers happy, we could ask ourselves if Internal Quality is
            something we should pay attention to.
          </p>
          <p>
            Internal quality is related with the design of the software and
            it is purely in the interest of development. If Internal quality
            starts falling, the system will be less amenable to change in the
            future.  Due to that, code reviews, refactoring and testing are
            essential as otherwise the internal quality will slip. 
          <p>
            An interesting analogy with debts and bad code design was
            developed Ward [[DEBT-ANALOGY]]. Sometimes companies need to get
            some credit from the banks in order to be able to invest,
            however, it is also critical to understand that is impossible to
            ask for credit continuously as the paying interest will kill the
            company financially. The same could be used for software,
            sometimes it is good to assume some technical debt to achieve a
            goal, for instance, meeting a critical milestone to reach users
            before our competitors, but it is important to understand that
            assuming technical debt endlessly would kill the project as it
            will make the product unmaintainable.
          </p>
          <p>
            Sometimes, after achieving the target External Quality, we need
            to refactor our code to improve the Internal Quality. Software
            Quality is sometimes the art of a continuous refactor.
          </p>
          <p>
            Let's go back to the analogy of writing an essay or a paper, in
            that case most people write out the first draft as a long
            brain-dump saying everything that should be said. After that, the
            draft is constantly changed (refactored) until it is a cohesive
            piece of work.
          </p>
          <p>
            When developing software (for instance in University assignments
            :-D) the first draft is often finished when it meets the general
            requirements of the task. So, after that, there is an immediate
            need to refactor the work into a better state without breaking
            the external quality. Maybe writing software is also kind of an
            art?
          </p>
          <p>
            This is universally true, and the danger of not paying attention
            to refactor your code is bigger on a larger project where poor
            quality code can lose you days in debugging and refactoring. 
          </p>
          <p>
            Some of the properties that enable the process of product with 
            good internal quality are:
          </p>
          <ul>
            <li>
              Concision: The code does not suffer from duplication.
            </li>
            <li>
              Cohesion: Each [module|class|routine] serves a particular
              purpose (e.g. it does one thing) and does it well.
            </li>
            <li>
              Low coupling: Minimal inter-dependencies and interrelation
              between objects and modules.
            </li>
            <li>
              Simplicity: The software is always designed in the simplest
              possible manner so that errors are less likely to be
              introduced.
            </li>
            <li>
              Generality: Specific solutions are only used if they are really
              needed. 
            </li>
            <li>
              Clarity: the code enjoys a good auto-documentation level so
              that it is easy to be maintained. 
            </li>
          </ul>
          <p>
            The external quality is sometimes compared with "Doing the
            right things" as opposed to "Doing the things right"
            which should define what internal quality is.
          </p>
          <p>
            Usually, the problems with the external quality characteristics
            (correctness, reliability...) are simply visible symptoms about
            software problems, that usually are related with internal quality
            attributes:  program structure, complexity, coupling,
            testability, reusability, readability, maintainability...
            Sometimes, when the internal quality is bad, external quality can
            be met during a short period of time, but in the longer term, the
            external quality will be affected.
          </p>
          <p>
            An excellent analogy is the Quality Iceberg created by Steve 
            McConell (see <a href="#fig-the-software-quality-iceberg"></a>).
          </p>
          <figure>
            <img src='images/unit1-fig1-iceberg.png' height='500px' width='700px'>
            <figcaption> The Software Quality Iceberg</figcaption>
          </figure>
        </section>
        <section>
          <h3>View 4: ISO 9126</h3>
          <p>
            ISO 9126 defines Software Quality as the totality of
            characteristics of an entity that bears on its ability to satisfy
            stated and implied needs.
          </p>
          <p>
            It recognizes that quality is not only determined by the software
            itself but also by the process used for software development and
            the use made of the software. Hence, the following identities are
            defined:
          </p>
          <ul>
            <li>
              Quality of the process of constructing software: process
              quality
            </li>
            <li>
              Quality of software product in itself: internal and
              external quality
            </li>
            <li>
              Quality of software product in use: quality in use
            </li>
          </ul>
          <figure>
            <img src='images/unit1-fig2-iso9126.png'>
            <figcaption>ISO 9126 View on Software Quality</figcaption>
          </figure>
          <p>
            ISO identified 6 characteristics of the software quality that are
            sub-divided into sub-characteristics:
          </p>
          <ul>
            <li>
              Functionality: A set of attributes that bear on the existence
              of a set of functions and their specified properties. The
              following sub-characteristics were defined: Suitability,
              Accuracy, Interoperability and Security.
            </li>
            <li>
              Reliability: A set of attributes that bear on the capability of
              software to maintain its level of performance under stated
              conditions for a stated period of time. The following
              sub-characteristics were defined. The following
              sub-characteristics were defined:  Maturity, Fault Tolerance
              and Recoverability.
            </li>
            <li>
              Usability: A set of attributes that bear on the effort needed
              for use.  The following sub-characteristics were defined:
              Understandability, Learnability and Operability.
            </li>
            <li>
              Efficiency: A set of attributes that bear on the relationship
              between the level of performance of the software and the amount
              of resources used. The following sub-characteristics were
              defined: Time Behaviour and Resource Behaviour.
            </li>
            <li>
              Maintenability: A set of attributes that bear on the effort
              needed to make specified modifications. The following
              sub-characteristics were defined: Analyzability, Changeability,
              Stability and Testability.
            </li>
            <li>
              Portability: A set of attributes that bear on the ability of
              software to be transferred from one environment to another. The
              following sub-characteristics were defined: Adaptability,
              Installability, Conformance and Replaceability.
            </li>
          </ul>
          <figure>
            <img src='images/unit1-fig3-iso9126.png'>
            <figcaption>ISO 9126 Quality Characteristics</figcaption>
          </figure>
          <p>
            Quality in use is defined by ISO as "the extent to which a
            product used by specified users meets their needs to achieve
            specified goals with effectiveness, productivity, and
            satisfaction in specified contexts of use". The quality in
            use hence depends on the context in which the product is used and
            its intrinsic quality.
          </p>
        </section>
        <section>
          <h3>Summary</h3>
          <p>
            There is no a single definition of quality. However, the
            importance of Software Quality is continuously increasing. The
            concepts of external and internal quality are commonly used
            across the software industry, but despite that, the properties 
            used to measure the quality diverge across different 
            methodologies, standards or companies. 
          </p>
        </section>
      </section>
      <section>
        <h2> Key Definitions </h2>
        <p>
          Despite the availability of different quality definitions,
          characteristics and entities, a common understanding is that high
          quality is usually linked to products with low number of defects.
          Therefore, it is assumed that a quality problem is due to the
          impact of a defect.
        </p>
        <p>
          But in order to identify which is high quality, defining what a
          defect is needed. In general, there are three concepts used in
          software quality to refer to defects:
        </p>
        <ul>
          <li>
            <b>Failure:</b> Any deviation of the observed behaviour from the
            specified behaviour.
          </li>
          <li>
            <b>Error:</b> System state where any further processing by the system
            will lead to a failure.
          </li>
          <li>
            <b>Fault:</b> Algorithmic cause of an error. (Also known as bug or
            defect).
          </li>
        </ul>
        A real life example of all these concepts is described in Example 2.
        <div class="example">
          <pre class="example">
            Peter is driving his car towards Oxford. While he is driving, the road 
            diverts into two different directions:
              1.  Left road to Oxford
              2.  Right road to Cambridge

            By mistake, Peter takes the road to Cambridge. That is a fault that is 
            committed by Peter.

            Suddenly, Peter is in an error situation or state: Peter is heading 
            Cambridge and not Oxford.

            If Peter goes on and arrives to Cambridge, that would be a failure: 
            Peter was planning to get to Oxford but he has arrived to Cambridge instead.

            If Peter realizes of the error situation while he is driving Cambridge, 
            returns to the junction and takes the right road to Oxford no failure 
            would happen as Peter recovers from the error condition.
          </pre>
        </div>
        <div class="example">
          <pre class="example highlight prettyprint">
            public static int numZero (int[] x) { 
              // effects: if x == null throw NullPointerException
              // else return the number of occurrences of 0 in x 
              int count = 0; 
              for (int i = 1; i < x.length; i ++) {
                if (x[i] == 0) { 
                 count ++;
                }
              }
              return count;
            }
          </pre>
        </div>
        <p>
          The fault in the code above is that it starts looking for zeroes at
          index 1 instead of index 0. For example, numZero([2, 7, 0])
          correctly evaluates to 1, while numZero([0, 7, 2]) incorrectly
          evaluates to 0. In both cases the fault is present and is executed.
          Although the code is in both cases in an error situation, only in
          the second case there is a failure: the result is different from
          the expected one. In the first case, the error condition (the for
          starts in 1) do not propagates to the output. 
        </p>
        <p>
          Some early conclusions can be already identified:
        </p>
        <ul>
          <li>
            For a given fault, not all the inputs to the software will
            "trigger" the fault to create a failure. In some
            situations, the failure will not appear to the end-user, despite
            the fault is in the code.
          </li>
          <li>
            Identifying a fault given an observed failure is, in some
            situations, very difficult, as multiple entities within the code
            (objects, methods, attributes) may be involved and the fault may
            occur in any of them.
          </li>
        </ul>
      </section>
      <section>
        <h2> Software Quality Assurance</h2>
        <section>
          <h3>Introduction to SQA</h3>
          <p>
            Software Quality Assurance (SQA) is the set of methods used to
            improve internal and external qualities. SQA aims at preventing,
            identifying and removing defects throughout the development cycle
            as early as possible, as such reducing test and maintenance
            costs.
          </p>
          <p>
            SQA consists of a systematic, planned set of actions necessary to
            provide adequate confidence that the software development process
            or the maintenance process of a software system product conforms
            to established functional technical requirements as well as with
            the managerial requirements of keeping the schedule and operating
            within the budgetary confines.
          </p>
          <p>
            The ultimate target of the SQA activities is that few, if any,
            defects remain in the software system when it is delivered to its
            customers or released to the market. As it is virtually
            impossible to remove all defects, another aim of QA is to
            minimize the disruptions and damages caused by these remaining
            defects.
          <p>
            The SQA methodology will also depend on the software development
            methodology used, as they are inherently couple. For instance,
            different software development models will focus the test effort
            at different points in the development process. Newer development
            models, such as Agile, often employ test driven development and
            place an increased portion of the testing in the hands of the
            developer, before it reaches a formal team of testers. In a more
            traditional model, most of the test execution occurs after the
            requirements have been defined and the coding process has been
            completed.
          </p>
          <p>
            An example of an SQA methodology is available at
            [[IEEE-QA-TEMPLATE]].
          </p>
          <p>
            SQA activities are not only carried out by the Software Quality
            group, the software engineer group is responsible for putting in
            place the SQA methodology defined, which may include different
            activities such as testing, inspection, reviews...
          </p>
        </section>
        <section>
          <h3>SQA Activities</h3>
          <section>
            <h4>Classification SQA Activities</h4>
            <p>
              The activities that are carried out as part of the SQA process
              can be divided in three different categories.
            </p>
            <ol>
              <li>
                Defect Prevention: Defect prevention consists on preventing
                certain types of faults from being injected into the
                software. As explained in previous section, a fault is the
                missing or incorrect human actions that lead to error
                situations in the software. There are two generic ways to
                prevent defects:
                <ul>
                  <li> 
                    Eliminating certain fault sources such as ambiguities or
                    human misconceptions
                  </li>
                  <li>
                    Fault prevention or blocking: Breaking the causal relation
                    between error sources and faults through the use of certain
                    tools and technologies.
                  </li>
                </ul>
              </li>
              <li>
                Defect reduction: Consists in removing the faults from the
                software through fault detection and removal. These QA
                alternatives detect and remove certain faults once they have
                been injected into the software systems. The most traditional
                QA activities fall into this category such as:
                <ul>
                  <li>
                    Inspection: directly detects and removes faults from the
                    software code, design, etc.
                  </li>
                  <li>
                    Testing: removes faults based on related failure
                    observations during program execution.
                  </li>
                </ul>
              </li>
              <li>
                Defect containment:  Consists in minimizing the impact of the
                software faults. The most important techniques in this area
                are:
                <ul>
                  <li>
                    Fault-tolerance techniques: Try to break the causal
                    relationship between faults and failures. E.g. ensuring
                    that error conditions do not lead to a software failure.
                  </li>
                  <li>
                    Containment measures: Once the error has occurred, if there
                    is no way to prevent the failure, ideally, it should be
                    possible to perform some actions to minimize the impact and
                    consequences of the failure.
                  </li>
                </ul>
              </li>
            </ol>
            <p>
              The following sections explain in detail these SQA activities.
            </p>
          </section>
          <section>
            <h4>Fault Prevention</h4>
            <p>
              The main goal of these activities is reducing the chance for
              defect injections and the subsequent cost to deal with these
              injected defects.
            </p>
            <p>
              Most of the defect prevention activities assume that there are
              known error sources or missing/incorrect actions that result in
              fault injections, as follows:
            </p>
            <ul>
              <li>
                If human misconceptions: Lack of education.
              </li>
              <li>
                If imprecise designs & implementations: Lack of formal 
                methods.
              </li>
              <li>
                If non-conformance to standards: No standard enforcement.
              </li>
              <li>
                If lack of tools and techniques: No technique or tool 
                adoption.
              </li>
            </ul>
            <section>
              <h5>Education and Training</h5>
              <p>
                People is the most important factor that determines the
                quality and, ultimately, the success or failure of most
                software projects. Hence, it is important that people
                involved in the software planning, design and development
                have the right capabilities for doing their jobs. The
                education and training effort for error source elimination
                should focus on the following areas:
              </p>
              <ul>
                <li>
                  Product and domain specific knowledge.
                </li>
                <li>
                  Software development knowledge and expertise.
                </li>
                <li>
                  Knowledge about Development methodology, technology, and 
                  tools.
                </li>
                <li> 
                  Development process knowledge.
                </li>
              </ul>
            </section>
            <section>
              <h5>Formal Methods</h5>
              <p> 
                Formal methods provide a way to eliminate certain error
                sources and to verify the absence of related faults. Formal
                development methods, or formal methods in short, include
                formal specification and formal verification.
              </p>
              <ul>
                <li>
                  Formal specification is concerned with producing an
                  unambiguous set of product specifications. An unclear
                  specification implies that the software target and
                  behaviour may depend on the interpretation of the
                  developer, due to that the likelihood of defects in errors
                  is higher. 
                </li>
                <li>
                  Formal verification checks the conformance of software
                  design or code against these formal specifications, thus
                  ensuring that the software is fault-free with respect to
                  its formal specifications.
                </li>
              </ul>
            </section>
          </section>
          <section>
            <h4>Fault Removal</h4>
            <p>
              Even if the best software developers in the world are involved
              in a software project, and even if they follow the formal
              methods described in the previous section, some faults will be
              injected in the software code. Due to that, defect prevention
              needs to be complemented with other techniques focused on
              removing as many of the injected faults as possible under
              project constraints.
            </p>
            <p>
              Fault distribution is highly uneven for most software products,
              regardless of their size. Much empirical evidence has
              accumulated over the years to support the so-called 80:20 rule,
              which states that 20% of the software components are
              responsible for 80% of the problems (Pareto Law). There is a
              great need for risk identification techniques to detect the
              areas in which the fault removal activities should be focused.
            </p>
            <p>
              There are two key activities that deal with fault removal: Code
              Inspection and Testing.
            </p>
            <section>
              <h5>Inspections</h5>
              <p>
                The software inspections were first introduced by Michael E.
                Fagan in 1970s, when he was a software development manager at
                IBM. The inspections are a means of verifying intellectual
                products by manually examining the developing product, a
                piece at a time, by small groups of peers to ensure that it
                is correct and conforms to product specifications and
                requirements. Inspections may be done in the software code
                itself and also in other related items such as design or
                requirements documents.
              </p>
              <p>
                Code inspections should check for technical accuracy and
                completeness of the code, verify that it implements the
                planned design, and ensure good coding practices ?and
                standards are used.  Code inspections should be done after
                the code has been compiled and all syntax errors removed, but
                before it has been unit tested. 
              </p>
              <p>
                There are different kind of inspections depending on factors
                such as the formality (formal vs informal) the size of the
                team (peer review, team review), whether it is guided or
                not; The type of inspection to be done depends on the
                software to be reviewed, the team involved and the target of
                the review.
              </p>
              <p>
                Regardless of the inspection type used, there are clear
                benefits when inspections are used. For instance, according
                to Bell-Northen Researh, the cost of detecting a defect is
                much lower in case of inspections (1 hour per defect) than in
                the case of testing (2-4 hours per defect).
              </p>
              <p>
                More information about inspections can be found at
                [[INSPECTIONS-AND-REVIEWS]] and [[TRUTHS-PEER-REVIEWS]] and
                in the last chapter.
              </p>
            </section>
            <section>
              <h5>Testing</h5>
              <p>
                Testing is the execution of software and the observation of
                the program behaviour and outcome. As in the case of the
                software inspections, there are different kind of testing,
                usually applied in different phases of the software
                development process.
              </p>
              <p>
                Some of the most typical testing types are:
              </p>
              <ul>
                <li>
                  Unit Testing: individual units of source code are tested to
                  determine if they are fit for use. A unit is the smallest
                  testable part of an application.
                </li>
                <li>
                  Module Testing: A complete module is tested to determine if
                  it fulfils its requirements.
                </li>
                <li>
                  Integration Testing: Any type of software testing that
                  seeks to verify the interfaces between modules against a
                  software design.
                </li>
                <li>
                  System Testing: It tests a completely integrated system to
                  verify that it meets its requirements
                </li>
                <li>
                  Acceptance Testing: Testing performed often in production
                  or pre-production environment to check that the software is
                  ready for being delivered and deployed.
                </li>
              </ul>
              <p>
                A concept tight related with testing (although applicable in
                other areas such as reviews) is the handling of the defects.
                In particular it is very important that the defects detected
                are properly recorded (defect logging) with all the relevant
                information as in many situations finding the error related
                with a fault is not trivial. It is also very important that
                the issues detected are monitored so that everybody knows
                what is the status of every defect after the initial
                discovery (defect tracking).
              </p>
            </section>
          </section>
          <section>
            <h4>Defect Containment</h4>
            <p>
              The defect reduction activities can only reduce the number of
              faults to a fairly low level, but not completely eliminate
              them. For instance, in many situations, the combination of
              possible situations is so big, that it is impossible to test
              all those situations, especially those linked to rare
              conditions or unusual dynamic scenarios.
            </p>
            <p>
              Depending on the purpose of the software, these remaining
              faults, and the failure risk due to them may be still
              inadequate, so some additional QA techniques are needed:
            </p>
            <ul>
              <li>Software fault tolerance.</li>
              <li>Failure containment.</li>
            </ul>
            <p>
              For instance, the software used in the flight control systems
              is one example of software with very extreme requirements about
              failures. The report [[CHALLENGES-FAULT-TOLERANT-SYSTEMS]] 
              provides more details about the challenges that this kind of 
              systems pose to software developers.
            </p>
            <section>
              <h5>Software Fault Tolerance</h5>
              <p>
                Software fault tolerance ideas originate from fault tolerance
                designs in traditional hardware systems that require higher
                levels of reliability, availability, or dependability.
              </p>
              <p>
                All fault tolerance systems must be based on the provision of
                useful redundancy that allows to switch between components
                when one of they fail (due to software or hardware faults).
                That implies that there has to be some extra components,
                which ideally should have a different design to avoid the
                same error to happen twice. Based on how those redundant
                components structured are used (e.g. when to switch from one
                to another) there are different kind of systems:
              </p>
              <ol type="A">
                <li>
                  Recovery blocks: Use repeated executions (or redundancy over
                  time) as the basic mechanism for fault tolerance. The
                  software includes a set of "recovery points" in which the
                  status is recorded so that they could be used as fallbacks in
                  case something goes wrong. When a piece of code is executed,
                  a "test acceptance" is internally executed, if the result is
                  OK, a new "recovery point" is set-up, if the result is not
                  acceptable, then the software returns to the previous
                  "recovery point" and an alternative to the faulty code is
                  enacted. This process continues until the "acceptance test"
                  is passed or no more alternatives are available, which leads
                  to a failure. Some key characteristics about this scheme that
                  is depicted in <a href='#fig-recovery-blocks'></a>: 
                  <ul type="a">
                    <li>
                      It is a backward error recovery technique: when an
                      error occurs, appropriate actions are taken to react
                      but no preventing action is taken.
                    </li>
                    <li>
                      It is a "serial technique" in which the same
                      functionality (the recovery block) is never executed in
                      parallel.
                    </li>
                    <li>
                      The "acceptance test" algorithm is the critical
                      part to success as well as the availability of recovery
                      blocks designed in different ways to the original code.
                    </li>
                  </ul>
                  <figure>
                    <img src='images/unit1-fig4-recoveryblocks.png' height='400px' width='500px'>
                    <figcaption>Recovery Blocks</figcaption>
                  </figure>
                </li>
                <li>
                  <p>
                    NVP (N-version programming):
                  </p>
                  <p>
                    This technique uses parallel redundancy, where N copies,
                    each of a different version, of codes fulfilling the same
                    functionality are running in parallel with the same
                    inputs. When all of those N-copies have completed the
                    operation, an adjudication process (decision unit) takes
                    place to determine (based in a more or less complex vote)
                    the output.
                  </p>
                  <p>
                    Some key characteristics about this scheme that is
                    depicted in 
                    <a href='#fig-n-version-programming'></a>
                  </p>
                  <ul type="a">
                    <li> 
                      It is a forward error recovery technique: preventive
                      actions are taken. Even if no error occurs the same
                      functionality is executed multiple times.
                    </li>
                    <li>
                      It is a "parallel technique" in which the same
                      functionality is always executed in parallel by
                      different versions of the same functionality.
                    </li>
                    <li>
                      The "decision unit" algorithm is the critical part
                      to success as well as the availability of different
                      versions of the same code designed in different ways.
                    </li>
                  </ul>
                  <figure>
                    <img src='images/unit1-fig5-Nversion.png' height='400px' width='500px'>
                    <figcaption>N version programming</figcaption>
                  </figure>
                  <p>
                    Obviously a wide range of different variants of those
                    systems have been proposed based in multiple combinations
                    of them [[COST-EFFECTIVE-FAULT-TOLERANCE]] and multiple
                    comparisons between the performance are also available
                    [[PERFORMANCE-RB-NVP-SCOP]].
                  </p>
                </li>
              </ol>
              <p>
                What do you think are the key advantages and disadvantages
                of the two fault tolerance techniques described (Recovery
                Blocks & N-Version)? Exercise 3: Recovery Blocks vs.
                N-Version
              </p>
            </section>
            <section>
              <h5>Failure Containment</h5>
              <p>
                There is software that is used in safety critical systems,
                that have severe consequences in case a failure occurs. In
                those situations it is very important to avoid some of the
                potential accidents or at lt
              </p>
              <p>
                Various specific techniques are used for this kind of
                systems, most of them based on the analysis of the potential
                hazards linked to the failures: 
              </p>
              <ul>
                <li>
                  Hazard Elimination through substitution, simplification,
                  decoupling, elimination of specific human errors and
                  reduction of hazardous materials or conditions. These
                  techniques reduce certain defect injections or substitute
                  non-hazardous ones for hazardous ones. The general approach
                  is similar to the defect prevention and defect reduction
                  techniques surveyed earlier, but with a focus on those
                  problems involved in hazardous situations.
                </li>
                <li>
                  Hazard Reduction through design for controllability (for
                  example, automatic pressure release in boilers), us of
                  locking devices (for example, hardware/software
                  interlocks), and failure minimization using safety margins
                  and redundancy. These techniques are similar to fault
                  tolerance, where local failures are contained without
                  leading to system failures.
                </li>
                <li>
                  Hazard control through reducing exposure, isolation and
                  containments (for example barriers between the system and
                  the environment), protection systems (active protection
                  activated in case of hazard), and fail-safe design (passive
                  protection, fail in a safe state without causing further
                  damages). These techniques reduce the severity of failures,
                  therefore weakening the link between failures and
                  accidents.
                </li>
                <li>
                  Damage control through escape routes, safe abandonment of
                  products and materials, and devices for limiting physical
                  damages to equipment or people. These techniques reduce the
                  severity of accidents, thus limiting the damages cause by
                  these accidents and related software failures.
                </li>
              </ul>
              <p>
                Notice that both hazard control and damage control above are
                post-failure activities that attempt to "contain" the
                failures so that they will not lead to accidents or the
                accident damage can be controlled or minimized. All these
                techniques are usually very expensive and process/technology
                intensive, hence they should be only applied when safety
                matters and deal with rare conditions related to accidents.
              </p>
            </section>
          </section>
        </section>
      </section>
      <section>
        <h2>Software Quality Engineering</h2>
        <p>
          Whereas Quality Assurance defines a set of methods to improve
          Software Quality, it does not define aspects that are key in order
          to ensure good quality software is delivered such as:
        </p>
        <ul>
          <li>
            What is the Quality Target? I.e. when to stop and deliver the
            software
          </li>
          <li>
            How can that quality target be checked.
          </li>
          <li>
            Are the right QA tasks being done at the right time?
          </li>
          <li>
            Are the QA tasks being executed right?
          </li>
        </ul>
        <p>
          In order to address these questions, the QA activities should be
          considered not in an isolated manner, but as part of a full
          engineering problem. Software Quality Engineering is the discipline
          that defines the processes to ensure high quality products. QA
          activities are only a part of that process, which requires further
          activities such as Quality Planning, Goal Setting or Quality
          Assessment. The <a href='#fig-sqe-cycle'></a>provides an overview 
          of the typical SQE: 
        </p>
        <figure>
          <img src='images/unit1-fig6-sqe.png'>
          <figcaption>SQE Cycle</figcaption>
        </figure>
        <section>
          <h3>Pre-QA Activities: Quality Planning</h3>
          <p>
            Before doing any QA activity, it is important to consider some
            aspects such as the target quality, the most appropriate QA
            activities to be done and when should be done, how are the
            quality going to be measured. All those activities are usually
            called Pre-QA or Quality Planning Activities.
          </p>
          <p>
            The first activity that should be done in SQE is defining what
            are the specific quality goals for the software to be delivered.
            In order to do so, it is important to understand what are the
            expectations of the software end-user/customer. Obviously, it is
            also key to recognize that the budget is limited and that the
            quality target should be financially doable. The following
            activities are key to identify the target quality of the
            software:
          </p>
          <ol>
            <li>
              Identify quality views and attributes meaningful to target
              customers and users. Which aspects will be key for them to
              perceive the software as high quality one? This may depend a lot
              on the type of product and on the target customers. 
            </li>
            <li>
              Select direct quality measures that can be used to measure
              those quality attributes that are key for the customers.
            </li>
            <li> 
              Quantify these quality measures to set quality goals while
              considering the market environment and the cost of achieving
              different quality goals.
            </li>
          </ol>
          <p>
            Once that the quality goals are clear, the QA strategy should be
            defined. Two key decisions should be made during this stage:
          </p>
          <ol>
            <li>
              Which QA activities are the most adequate ones to meet the
              customer quality expectations. For doing this, it is important
              to translate the quality views, attributes and goals into the
              QA activities to be performed. It is also very important to
              determine when every QA activity is going to be executed as
              part of the full Software Development Process. 
            </li>
            <li>
              The external quality measures should be mapped into internal
              indirect ones via selected quality models. Good models are
              required in order to predict external quality based on internal
              indicators. It is also very important to identify how the
              results of this measure are going to be collected and used
              (e.g. what happens if the quality is not good enough or how the
              feedback is going to be used).
            </li>
          </ol>
        </section>
        <section>
          <h3>In-QA Activities</h3>
          <p>
            These activities have been described in section 1.4.2 and
            basically consist in executing the QA activities planned and
            handling the defects discovered as a result of them. 
          </p>
        </section>
        <section>
          <h3>Post-QA Activities</h3>
          <p>
            These activities consist in measuring the quality of the software
            (after the QA activities), assess the quality of the software
            product and the definition of the decisions and actions need to
            improve its quality.
          </p>
          <p>
            All these activities are usually carried out after normal QA
            activities have started but as part of these "normal" QA
            activities.  Their goal is to provide feedback so that decisions
            can be made and improvements can be suggested. The key activities
            include:
          </p>
          <ul>
            <li>
              Measurement: Besides the direct measure of tracking the defects
              during the in-QA activities, various other measurements are
              needed in order to the track the QA activities and for project
              management purposes. The data resulting from this analysis is
              important to manage software project and quality.
            </li>
            <li>
              Analysis and Modelling: These activities analyse measurement
              data from software projects and fit them to analytical models
              that provide quantitative assessment of selected quality
              characteristics and sub-characteristics. This is key to obtain
              an objective assessment of the current product quality, predict
              future quality or identify problematic areas.
            </li>
            <li>
              Providing feedback and identifying improvement potentials: The
              results of the previous activities can lead to some suggestions
              to improve the process followed with the software being
              assessed (e.g.  more testing resources are needed, test cases
              are not sufficient...) or the general SQE methodology.
            </li>
            <li>
              Follow-up Activities: Besides immediate actions, some actions
              resulting from the analysis may require a longer time. For
              instance, if major changes are suggested to change the SQE
              process, they cannot be usually implemented while the current
              process has not finished.
            </li>
          </ul>
        </section>
      </section>
      <section>
        <h2>Quality Improvement Process (QIP)</h2>
        <p>
          The overall framework for quality improvement is called QIP, and it
          includes three interconnected steps:
        </p>
        <ul>
          <li>
            Understand the baseline so that improvement opportunities can be
            identified and clear, measurable goals can be set.
          </li>
          <li>
            Introduce process changes through experiments, pilot projects,
            assess their impact, and fine tune these process changes.
          </li>
          <li>
            Package baseline data, experiment results, local experience, and
            updated process as the way to infuse the findings of the
            improvement program into the development organization
          </li>
        </ul>
        <a href='#fig-qip-flow'></a>describes graphically the flow of those
        steps related to the SQE process.
        <figure>
          <img src='images/unit1-fig7-qip.png'>
          <figcaption>QIP Flow</figcaption>
        </figure>
        <section>
          <h3>The Deming Quality Cycle</h3>
          <p>
            W. Edwards Deming in the 1950's proposed that business processes
            should be analysed and measured to identify sources of variations
            that cause products to deviate from customer requirements. He
            recommended that business processes be placed in a continuous
            feedback loop so that managers can identify and change the parts
            of the process that need improvements. As a teacher, Deming
            created a (rather oversimplified) diagram to illustrate this
            continuous process, commonly known as the PDCA cycle for Plan,
            Do, Check, Act:
          </p>
          <ul>
            <li>
              Plan Quadrant: one defines the objectives and determines the
              conditions and methods required to achieve them.
            </li>
            <li>
              Do Quadrant: the conditions are created and the necessary
              training to execute the plan is performed (new procedures). The
              work is then performed according to these procedures.
            </li>
            <li>
              Check Quadrant: One must check to determine whether work is
              progressing according to the plan and whether the expected
              results are obtained.
            </li>
            <li>
              Action Quadrant: If the checkup reveals that the work is not
              being performed according to plan or results are not what was
              anticipated, measures must be devised for appropriate action.
              </li>
          </ul>
          <p>
            Deming's PDCA cycle can be illustrated as in 
            <a href='#fig-pdca-circle'></a>:
          </p>
          <figure>
            <img src='images/unit1-fig8-pdca.png' height='300px' width='300px'>
            <figcaption>PDCA Circle</figcaption>
          </figure>
          <p>
            By going around the PDCA circle, the working methods are
            continuously improved as well as the results obtained. However,
            it is important to take care avoid a situation called "spiral of
            death" It happens when an organization goes around and around
            the quadrants, never actually bringing a system into production.
          </p>
        </section>
      </section>
      <section>
        <h2>QE in Software Development Process</h2>
        <p>
          The quality engineering process cannot be considered in an isolated
          manner, but as part for the overall software engineering process.
          For instance, most of the SQE Activities should be included as part
          of the Software Development activities (<a
          href='#fig-sqe-and-software-development'></a>):
        </p>
        <ul>
          <li>
            Quality Planning should be part of product planning.
          </li>
          <li>
            In-QA activities should be part of the development activities.
          </li>
          <li>
            The Quality Analysis/Feedback should be part of the project
            management responsibilities.
          </li>
        </ul>
        <figure>
          <img src='images/unit1-fig9-sqeswd.png'>
          <figcaption>SQE and Software Development</figcaption>
        </figure>
        <p>
          However, it should be considered that SQE activities have different
          timing requirements, activities and focus. For instance, <a
          href='#fig-focus-of-sqe-activities-during-the-development-process'></a> 
          represents the typical effort spent in the different quality
          activities during the software development time.
        </p>  
        <figure>
          <img src='images/unit1-fig10-sqefocus.png'>
          <figcaption>
            Focus of SQE Activities during the development process
          </figcaption>
        </figure>
        <p>
          Focusing on the QA activities, in a typical waterfall development
          model, the <a
          href='#fig-focus-of-qa-activities-during-the-development-process'>
          </a> provides an estimate of the key QA activities done during each of
          the project phases:
        </p>
        <figure>
          <img src='images/unit1-fig11-qafocus.png' height='400px' width='400px'>
          <figcaption>
          Focus of QA Activities during the development process
          </figcaption>
        </figure>
        <p>
          Another important aspect to be considered is that some of the QA
          activities cannot be done until it is already too late. For
          example, for safety critical systems, post-accident measurements
          provide a direct measure of safety, but due to the damage linked to
          those accidents, they should be avoided by all means. In order to
          take early measures, appropriate models that link some of the
          quality measures during the development process with the end
          product quality are needed. Last but not least, it should be
          stressed that there is an increasing cost of fixing problems late
          instead of doing early, because a hidden problem may lead to other
          related problems, and the longer it stays in the system, the
          discovery is more difficult.
        </p>
      </section>
      <section>
        <h2>The cost of quality</h2>
        <p>
          In section 1.1, some of the implications of bad quality software
          have been introduced. The cost of poor quality (COPQ) is not the
          only cost that Software Quality Engineering should take into
          account. The cost of having good quality (COGQ) that may be linked
          to SQA activities (e.g testing or code inspections) should not be
          underestimated and considered when the total quality cost is
          assessed. 
        </p>
        <p>
          As in the case of the external and internal quality, the different
          costs linked to quality have been represented by some authors as an
          iceberg, in which some of the costs are easy to be identified (e.g.
          testing costs, customer returns...) while some others are not
          always taken account (e.g. unused capacity, excessive IT costs...).
          In [[COST-OF-QUALITY]] there is  a detailed analysis of this
          approach for identifying quality costs.
        </p>
        <figure>
          <img src='images/unit1-fig12-badgoodcost.png' height='400px' width='500px'>
          <figcaption>Bad and Good Software Quality Cost</figcaption>
        </figure>
      </section>
    </section>
    <section>
      <h1> Software Quality Metrics </h1>
      <p>
        "Quality metrics let you know when to laugh and when to cry", Tom Gilb
      </p>
      <p>
        "If you can't measure it, you can0t manage it", Deming
      </p>
      <p>
        "Count what is countable, measure what is measurable. What is not 
        measurable, make measurable", Galileo
      </p>
      <p>
        These are just some sample sentences about the importance of measuring
        in general. Obviously, the capability of quantifying characteristics of
        a product are extremely helpful to manage that product. However, it's 
        also important to stress that is essential to understand the attributes
        that are being measured so the metric doesn't end-up being a number but
        a proper indicator with a very clear meaning. Additionally, we studied
        in Unit 1 that Quality is a extremely subjective thing, so we should 
        not assume that every aspect related with product quality can be 
        quantified, or at least, quantified easily. Albert Einstein put it 
        very nicely when he said his famous sentence: "Not everything that can 
        be counted counts, and not everything that counts can be counted.".
      </p>
      <p>
        Also, we should bear in mind that the act of measuring some software
        attribute, is not intended to improve that metric but firstly to 
        understand its impact and its validity as an indicator for some software
        characteristic. A typical mistake is trying to improve any metric you
        are calculating in your projects. Doing this for the sake of it is a
        mistake, as Goodhart explained in his law: <b>"When a measure becomes a
        target, it ceases to be a good measure". </b> Imagine you are working
        in a development team and you are told that you need to increase the
        number of defects found during the development cycles. What is likely
        to happen is that the team is going to start reporting anything they
        could suspect is a bug, it doesn't matter how small, difficult to
        detect, or difficult to reproduce it is. At the end of the day, the
        team has been asked to find more bugs, and that is what they are
        going to do!
      </p>
      <section>
        <h2>Introduction</h2>
        <p>
          What does this all mean? It means that we shuld try to get metrics,
          but more importantly, we need to understand how those metrics affect
          the quality of the product, and how they could become indicators of
          certain software attributes.
        </p>
        <p>
          If we do that, we could use those metrics to improve our process and
          increase product quality over releases, predict potential issues (e.g.
          last time we got those indicators, the product was a disaster on the
          field), re-use successful experiences (if a product worked extremely
          well, check it's metrics and what did it make differente), etc. In 
          summary we should use metrics to understand first and imrpove 
          afterwards.
        </p>
        <p>
          But, what is relationship between the terms measurement, metric and
          indicator?
        </p>
        <ul>
          <li>
            Measurement is the process to get a quantitative indication of the
            exact amount (dimension, capacity, etc.) of some attributes. The
            output of measuremnt is a measure. Examples: 120.000 Lines of code 
            is a mesasurement of the size of the software, 120 defects could be
            a measure of the number of defect that I have found, etc.
          </li>
          <li>
            Metric: is a quantitative measure of the degree to which a system, 
            component or a process possesses a given attribute. I.e. the 
            difference between a measure and a metric is that the metric has a
            semantic meaning linked to it. E.g. 120 defects / 20.000 LOC means
            that the Defect Density is 1 defect/KLOC.
          </li>
          <li>
            Indicator: Once you understand the meaning of a metric, you can use
            it as indiator to evaluate a product or retrieve insights from it. 
            For instance, we could use the Defect Density to get insights about
            how my defect removal techniques (Testing or Code Reviews) are 
            working.
          </li>
        </ul>
        <table>
          <caption>Measurement, Metrics, Indicator</caption>
          <thead>
            <tr>
              <th>Action </th>
              <th>Concept</th>
              <th>Examples</th>
            </tr>￼
          </thead>
          <tbody>
            <tr>
              <td> Collect (Data) </td>
              <td>
                <ul>
                  <li>
                    <b>Measure:</b> Quantitative indication of the exact amount
                    (dimension, capacity, etc.) of some attributes
                  </li>
                  <li>
                    <b>Measurement:</b> The act of determining a measure
                  </li>
                </ul>
              </td>
              <td> 
                <code><ul>
                    <li>120 detected bugs</li>
                    <li>12 months of project duration</li>
                    <li>10 engineers working on the project</li>
                    <li>100.000 Lines of Code (LOC)</li>
                </ul></code>
              </td>
            </tr>￼
            <tr>
              <td> Calculate (Metrics) </td>
              <td>
                <ul><li>
                <b>Metric:</b> is a quantitative measure of the degree to which a
                system, component or a process possesses a given attribute
                </li></ul>
              </td>
              <td> 
                <code><ul>
                    <li>2 bugs found per engineer-month</li>
                    <li>1.2 bugs per KLOC </li>
                </ul></code>
              </td>
            </tr>
            <tr>
              <td> Evaluate (Metrics) </td>
              <td>
                <ul><li>
                <b>Indicator:</b> a metric that provides insights into the product,
                process, project.
                </li></ul>
              </td>
              <td> 
                <code><ul>
                 <li>Bugs found per engineer-month might be an indicator
                   of the test process efficiency</li>
                 <li>bugs/KLOC an indicator of code quality, etc. </li>
                </ul></code>
              </td>
            </tr>￼
          </tbody>
        </table>
        <p>
          There are three main kind of metrics related to software:
        </p>
        <ul>
          <li>
            Process Metrics: They are related to the <i>Life Cycle</i> of 
            the Software Development Process. Hence, they are typically 
            used to improve software processes.Some examples of these type 
            of metrics are: effectiveness of defect removal during 
            development, the pattern of testing defect arrival, the response
            time of the fix process, etc.
          </li>
          <li>
            Project Metrics: Describe the project characteristics and
            execution: number of software developers, cost, schedule, and
            productivity, etc.
          </li>
          <li>
            Product Metrics: describe the characteristics of the product at 
            a given point of time (a snaphot of the product) without paying
            attention to the process that led to that product. Some examples 
            are: complexity, size, defect density, etc.
          </li>
        </ul>
        <p>
          Example of measurements:
        </p>
        <pre>
          * 120 defects detected during 6 months by 2 engineers
          * Defects defected every month: 10, 10, 20, 20, 25, 35
          * Defects remaining in the final product: 40 
          * Size of the Product: 40.000 Lines of Code
        </pre>
        <p>
          Metrics and Indicator Examples:
        </p>
        <pre>
          * Process Metric: Defect Arrival Pattern per month: 10, 10, 20, 20, 25, 35 -> Indicator of Maturity
          * Project Metric: 40 KLOC / 2 / 6 = KLOC per eng-month   -> Indicator of Productivy
          * Product Metric: 40 defects / 40 KLOC = 1 defect / KLOC -> Indicator of Quality
        </pre>
        <p>
          Software quality metrics are a subset of software metrics that
          focus on the quality aspects of the product, process, and project.
          Software quality metrics can be divided further into:
        </p>
        <ul>
          <li>
            End-product quality metrics: Metrics related to the software
            product once it has been finalized and delivered. The following
            type of metrics are in this group:
            <ol>
              <li>
                Intrinsic Product Quality Metrics: These are the metrics that
                are related with the quality of the product itself, without
                the need to involve customers.
              </li>
              <li>
                Customer Satisfaction Metrics: These are the metrics that are
                related to the view customers have of the product quality.
              </li>
            </ol>
          </li>
          <li>
            In-process quality metrics: Metrics related to the software while
            it is under development. As the goal of a software development
            team is use the methodologies that provide the best possible
            quality, it is important to pay attention to those metrics. In
            general, they are less formally defined that the end-product
            metrics.
          </li>
        </ul>
        <p>
          In general, the quality of a developed product (end-product
          metrics) is influenced by the quality of the production process
          (in-process metrics). Identifying the link between those two type
          of metrics is essential for software development as the end-product
          metrics, most of the times, can be only discovered when it is too
          late (i.e. the product is alreay in the market). However, the link
          between both type of metrics is hard and complex as in the most of
          the times its relationship is poorly understood.
        </p>
        <p>
          The link model between a process and a product for manufacturer
          goods is in most of the cases simple. However, for software, this
          model is in general more complex because the influence of the humans
          involved in software development is way higher than in goods
          manufacturing and the degree of automation is smaller in software
          development that in manufacturing.
        </p>
        <p>
          As engineers, our target is:
        </p>
        <ul>
          <li>
            Applying all the possible techniques during the development process
            to reduce the number of defects (prevention, removal) and implement
            techniques to minimise their consequences.
          </li>
          <li>
            Measure in-process metrics to anticipate possible issues when 
            launching the product to the customers. If we are doubtful to 
            release a new version because we have observed "anomalous" 
            in-process metrics, we can use techinques such as controlled 
            roll-out, A/B Testing, Beta Users, etc. to do the launch in a 
            smaller scale.
          </li>
          <li>
            Measure end-product metrics when it's used by the customers and 
            correlate them with the in-process metrics so we can use that 
            correlation for predict the behaviour of next releaes or future
            products.
          </li>
        </ul>
        </p>
        <p>
          The ultimate goal of software quality engineering is to investigate
          the relationships among in-process metrics, project
          characteristics, and end-product quality, and based on these
          findings to engineer improvements in both process and product
          quality.
        </p>
      </section>
      <section>
        <h2>Product quality metrics</h2>
        <section>
          <h3>Intrinsic Product Quality Metrics</h3>
          <section>
            <h4>Reliability, Error Rate and Mean Time To Failure</h4>
            <p>
              Software reliability is a measure of how often the software
              encounters an error that leads to a failure. From a formal
              point view, Reliability can be defined as the probability of
              not failing during a specified length of time:
            </p>
            <p style="color: red">
              R(n) (where n is the number of time units) 
            </p>
            <p>
              The probability of failing in a specified length of time is 1
              minus the reliability for that length of time and it's usually
              denoted by a capital F letter:
            </p>
            <p style="color: red">
              F(n) = 1 - R(n)
            </p>
            <p>
              If time is measured in days, R(1) is the probability of the
              software system having zero failures during one day (i.e. the
              probability of not failing in 1 day)
            </p>
            <p>
              A couple of metrics related with the software reliability are 
              the "Error Rate" and the "Mean Time To Failure" (MTTF). The 
              <b>MTTF</b> can be defined as the average time that occurs
              between two system failures. Error Rate is the average number of
              failures suffered by the system during a given amount of time.
              Both metrics are related with the following formula:
            </p>
            <math xmlns="http://www.w3.org/1998/Math/MathML" styel="color: red">
              <mrow><mi><mtext>Error Rate</mtext></mi> <mo>=</mo>
              <mfrac><mrow><mtext>1</mtext></mrow><mrow><mtext>MTTF</mtext></mrow></mfrac>
              </mrow>
            </math>
            <p>
              The relationship between the error rate and the reliability 
              depends on the statistical distribution of the errors, not only
              on the error rate.
            </p>
            <p>
              For instance, the following table shows the errors that occured
              per day in two different systems during one week.
            </p>
            <table>
              <caption>Example of same error rate with different distribution</caption>
              <thead>
                <tr>
                  <th colspan=8>Defects per Day</th>
                </tr>￼
                <tr>
                  <th></th>
                  <th>DAY 1</th>
                  <th>DAY 2</th>
                  <th>DAY 3</th>
                  <th>DAY 4</th>
                  <th>DAY 5</th>
                  <th>DAY 6</th>
                  <th>DAY 7</th>
                </tr>￼
              </thead>
              <tbody>
                <tr>
                  <td><b>Project A</b></td>
                  <td>1</td>
                  <td>2</td>
                  <td>3</td>
                  <td>4</td>
                  <td>5</td>
                  <td>6</td>
                  <td>7</td>
                </tr>
                <tr>
                  <td><b>Project B</b></td>
                  <td>7</td>
                  <td>6</td>
                  <td>5</td>
                  <td>4</td>
                  <td>3</td>
                  <td>2</td>
                  <td>1</td>
                </tr>
              </tbody>
            </table>
            <p>
              It can be seen that both systems suffered the same amount of 
              errors during the week 28 and hence the error rate for both 
              systems is the same: 28/7 = 4 Errors/Day. However, the 
              reliability of the system for the first day is very different.
            </p>
            <p>
              Unless detailed statistics/models are available, the best estimate
              of the short-term future behavior is the current behavior. For 
              instance, if a system suffers 24 failures during one day, the best
              estimate for the next day is that 24 failures will occur (24 
              errors/day) that correspond to a 1hour MTTF. That means that by 
              default, we could assume that most of the system failures follow 
              an exponential distribution, and hence the following formula could
              be used to calculate their reliability:
            </p>
              <math xmlns="http://www.w3.org/1998/Math/MathML" styel="color: red">
                <mrow><mi><mtext>R(t)</mtext></mi><mo> = </mo><msup> <mi>e</mi> <mn>&minus;&lambda;t</mn> </msup></mrow>
              </math>
            <p>
              Where &lambda; is the Error Rate and t is the amount of time for 
              which the system reliability is calculated. A key concept of 
              exponential distributions is that the error rate is constant and
              hence it doesn't change with the time. The following tables show
              the Probability and Cumulative Density Functions of exponential
              curves with different values of &lambda;
            </p>
            <figure>
              <img src='images/unit2-fig20-exp-pdf.png' height='400px' width='500px'>
              <figcaption>Probabily Density Function for Exponential Distributions</figcaption>
            </figure>
            <figure>
              <img src='images/unit2-fig21-exp-cdf.png' height='400px' width='500px'>
              <figcaption>Cumulative Density Function for Exponential Distributions</figcaption>
            </figure>
            <p>
              However, this concept of systems having a constant error rate
              that doesn't change over time, it's not very common in the real
              world. For instance, in Hardware components, the error rate
              evolves with the time in different ways:
              <ul>
                <li>
                  During the early life of the product the error rate starts at
                  high value, but this error rate tends to go down gradually
                  over time.
                </li>
                <li>
                  After the early life of the product passes (the duration of
                  the early life depends on the product) the error rate tends
                  to be stable, during a time called <b> Useful Time </b>.
                </li>
                <li>
                  After the useful time is over, the <b>wearout period</b>
                  starts. During that period, the error rate keeps growing
                  until the hardware component suffers a failure.
                </li>
              </ul>
              <figure>
                <img src='images/unit2-fig22-bath.png' height='350px' width='500px'>
                <figcaption>Error Rate evolution in hardware products</figcaption>
              </figure>
              A canonical example of this behaviour are lightbulbs. In order to
              minimize the number of failures users suffer with lightbulbs,
              right after being produced, the manufacturers keep them switched
              on during a time similar to the early life of the product. By
              doing this, they guarantee that the lightbulbs that go to the
              customer hands, got there at a time when the error rate is as low
              as possible. Any component with early failures is discarded
              before it can get to the end-users.
            </p>
            <p>
              You can find a lot of information about reliability and how maths
              is used for calculating it at [[RELIABILITY-MATHS]]
            <p>
            <p>
              Although exponential distribution may be a good compromise 
              that could be applied to any software system, there are other 
              distributions that may describe in a more accurate way the 
              idea of having a non constant error rate. For instance, the 
              Weibull distribution is frequently used in reliability 
              analysis [[WEIBULL-BASICS]].
            </p>
            <p>
              In a Weibull distribution, the error rate can change with the
              time. The Reliability function is: 
            </p>
              <math xmlns="http://www.w3.org/1998/Math/MathML" styel="color: red">
                <mrow>
                  <mi>R(t)</mi>
                  <mo> = </mo>
                  <msup> 
                    <mi>e</mi>
                    <mn>
                      <!--<mfrac>
                        <mrow>&minus;t</mrow>
                        <mrow>&eta;</mrow>
                      </mfrac>-->
                      (&minus;t<mo>/</mo>&eta;)
                      <mn>
                        &beta; 
                      </mn>
                    </mn>
                  </msup>
                </mrow>
              </math>
            <p>
              That depends on two variables, &eta; and &beta; that define the 
              shape of the distribution function. For a fixed value of &eta;, 
              the failure rate could be constant (&beta; = 1), descending 
              (&beta; < 1) or ascending (&beta; > 1). Please note that combining
              those three options we could describe the Hardware component 
              failure rate phases.
            </p>
            <figure>
              <img src='images/unit2-fig23-weibull-rate.png' height='400px' width='500px'>
              <figcaption>Weibull Error Rate depending on &beta;</figcaption>
            </figure>
            <p>
              If we take into account Software Upgrades, there are some
              interesting analysis about how a sawteeth pattern is observed
              [[SOFTWARE-RELIABILITY]]. Again, such a curve could be defined by
              combining different Weibull distributions.
            </p>
            <figure>
              <img src='images/unit2-fig24-sawteeth-rate.png' height='350px' width='500px'>
              <figcaption>Sawteeth distribution of error rate</figcaption>
            </figure>
          </section>
          <section>
            <h4>Defect Density</h4>
            <p>
              Defect Density is the number of confirmed defects detected in
              software/component during a defined period of
              development/operation divided by the size of the
              software/component.
            </p>
            <p>
              <math xmlns="http://www.w3.org/1998/Math/MathML" styel="color: red">
                <mrow><mi><mtext>Defect Density</mtext></mi> <mo>=</mo>
                <mfrac><mrow><mtext>Number of Confirmed Defects</mtext></mrow><mrow><mtext>Software Size</mtext></mrow></mfrac>
                </mrow>
              </math>
            </p>
            <p>
              The "defects" are usually counted as confirmed and agreed
              defects (not just reported). For instance, dropped defects are
              not counted.
            </p>
            <p>
              The "period" or metrics time frame, might be for one of the
              following:
            </p>
            <ul>
              <li>
                for a duration (say, the first month, the quarter, or the
                year).
              </li>
              <li>
                for each phase of the software life cycle.
              </li>
              <li>
                for the whole of the software life cycle, usually known as
                Life Of Product (LOF) and may comprise time after the
                software product's release to the market.
              </li>
            </ul>
            <p>
              The "opportunities for error" (OFE) or sofware "size" is
              measured in one of the following:
            </p>
            <ul>
              <li>
                Source Lines of Code that are usually counted as thousands of
                Lines Of Code (KLOC)
              </li>
              <li>
                Function Points (FP)
              </li>
            </ul>
            <p>
              In the following chapters both ways of measuring OFE will be
              studied separately.
            </p>
            <section>
              <h5>Lines of Code</h5>
              <p>
                Counting the lines of code (LOC) is way more complex that what
                it could be initially considered. The major problem for couting
                lines of code comes from the ambiguity of the operational
                definition, the actual counting. In the early days of Assembler
                programming, in which one physical line was the same as one
                instruction, the LOC definition was clear.  With the
                availability of high-level languages the one-to- one
                correspondence broke down. Differences between physical lines
                and instruction statements (or logical lines of code) and
                differences among languages contribute to the huge variations
                in counting LOCs.  Even within the same language, the methods
                and algorithms used by different counting tools can cause
                significant differences in the final counts. Multiple
                variations were already described by Jones in 1986 such as:
              </p>
              <ul>
                <li>
                  Count only executable lines.
                </li>
                <li>
                  Count executable lines plus data definitions.
                </li>
                <li>
                  Count executable lines, data definitions, and comments.
                </li>
                <li>
                  Count executable lines, data definitions, comments, and job
                  control language.
                </li>
                <li>
                  Count lines as physical lines on an input screen.
                </li>
                <li>
                  Count lines as terminated by logical delimiters.
                </li>
              </ul>
              <p>
                For instance, next example includes two approaches for coding
                the same functionality. As the functionality is the same, and
                it is writing in the same manner, the opportunities for error
                should be the same, however, the lines of code differ. For
                instance, if we count all the lines (job control language,
                comments...), in the first case only one line of code is used
                whereas in the second case 5 lines of code have been used.
              </p>
              <div class="example">
                <pre class="example highlight prettyprint">
                  for (i=0; i<100; ++i) printf("I love compact coding"); /* what is the number of lines of code in this case? */

                  /* How many lines of code is this? */
                  for (i=0; i<100; ++i)
                  {
                    printf("I am the most productive developer"); 
                  }
                  /* end of for */
                </pre>
              </div>
              <p>
                Some authors have considered LOC not only a less useful way to
                measure software size but also a harmful thing for sofware
                economics and productivity. For instance, the paper written by
                Capers Jones called "A Short History of Lines of Code (LOC)
                Metrics" [[LOC-HISTORY]] offers a very interesting historical
                view about the evolution of Software Programming Languages and
                LOC metrics.
              </p>
              <p>
                Regardless of the LOC measurements used, when a software
                product is released to the market for the first time, and when
                a certain way to measure lines of code is specified, it is
                relatively easy to state its quality level (projected or
                actual). However, when enhancements are made and subsequent
                versions of the product are released, the measurement is more
                complicated. In order to have a good insight on the product
                quality is important to follow a two-fold approach:
              </p>
              <ul>
                <li>
                  Measure the quality of the entire product.
                </li>
                <li>
                  Measure the quality of the new/changed parts of the product.
                </li>
              </ul>
              <p>
                The first measure may improve over releases due to aging and
                defect removal, but that improvement in the overall defect
                rate may hide problems on the developement/quality process
                (e.g. new code contains a higher defect density that the
                "old" code which indicates a problem in the process). In
                order to be able to calculate defect rate for the new and
                changed code, the following must be available:
              </p>
              <ul>
                <li>
                  LOC count: The entire software product as well as the new
                  and changed code of the release must be available.
                </li>
                <li>
                  Defect tracking: Defects must be tracked to the release
                  origin, i.e. the portion of the code that contains the
                  defects and at what release the portion was added, changed,
                  or enhanced. When calculating the defect rate of the entire
                  product, all defects are used; when calculating the defect
                  rate for the new and changed code, only defects of the
                  release origin of the new and changed code are included.
                </li>
              </ul>
              <p>
                These tasks are enabled by the practice of change flagging.
                Specifically, when a new function is added or an enhancement
                is made to an existing function, the new and changed lines of
                code are flagged. The change-flagging practice is also
                important to the developers who deal with problem
                determination and maintenance. When a defect is reported and
                the fault zone determined, the developer can determine in
                which function or enhancement pertaining to what requirements
                at what release origin the defect was injected. The following
                is an example on how the overall defect rate and the defect
                rate for new code is mesaured at IBM according to the book
                "Metrics and models in software quality engineering" by
                Stephen H. Kan.
              </p>
              <div class="example">
                <p>
                  In the first version of a software product 30 Defects
                  were reported by end-users and the software size was 30KLOC.
                  After fixing all the discovered bugs, the team works in a 
                  new version that includes 10 new KLOC. End-users report 10
                  additional defects in this new version that were injected
                  in the new 10KLOC.
                </p>
                <p>
                  The Defect Density of the first version was 1 defect/KLOC. 
                </p>
                <p>
                  If we calculate the Defect Density of the second version in
                  the same way, it would be: DD = 10/40 = 0.25 defects/KLOC. If
                  we compare this value, we could conclude that the second
                  version was way better that the first one.
                </p>
                <p>
                  But, this could be misleading as indeed, the second version
                  could include only defects in the new 10KLOC, not in the old
                  ones. We could calculate the same metric but just counting
                  the new Lines of Code. If we do so, the result would be: 
                  DD = 10/10 = 1 defect/KLOC which is the same than for the
                  first release.
                </p>
                <p>
                  We could conclude that for end-users, the second version is
                  going to be a significant improvement as the number of defects
                  are going to perceive is smaller both in absolute and relative
                  terms. However, the team has been doing a similar job in 
                  terms of defects remaining after releasing the product (defect
                  injection and detection).
                </p>
              </div>
              <p>
                It is important to think how useful is this metric from two
                points of view:
              </p>
              <ul>
                <li>
                  Drive Quality Improvement: Very important for the development
                  team.
                </li>
                <li>
                  Meet customer expectations.
                </li>
              </ul>
              <p>
                From the customer's point of view, the defect rate is not as
                relevant as the total number of defects that might affect
                their business. Therefore, a good defect rate target should
                lead to a release-to-release reduction in the total number of
                defects, regardless of size. I.e. Not only the defect rate
                should be reduced but also the total number of defects. If a
                new release is larger than its predecessors, it means the
                defect rate goal for the new and changed code has to be
                significantly better than that of the previous release in
                order to reduce the total number of defects.
              </p>
              <p>
                In the example above, from the initial release to the second 
                release the defect rate didn't improve. However, customers 
                experienced a 66% reduction [(30 - 10)/30] in the number of 
                defects because the second release is smaller.
              </p>
            </section>
            <section>
              <h5>Function Points</h5>
              <p>
                As explained in the previous chapter, measuring the
                opportunities for error through the lines of code has some
                problems. Counting lines of code is but one way to measure
                size. Another alternative is the using the function point. In
                recent years the function point has been gaining acceptance in
                application development in terms of both productivity (e.g.,
                function points per person-year) and quality (e.g., defects per
                function point)
              </p>
              <p>
                A function can be defined as a collection of executable
                statements that performs a certain task, together with
                declarations of the formal parameters and local variables
                manipulated by those statements.  The ultimate measure of
                software productivity is the number of functions a development
                team can produce given a certain amount of resource, regardless
                of the size of the software in lines of code.  The defect rate
                metric, ideally, is indexed to the number of functions a
                software provides. If defects per unit of functions is low,
                then the software should have better quality even though the
                defects per KLOC value could be higher — when the functions
                were implemented by fewer lines of code. Although this approach
                seems very powerful and promising, from a practical point of
                view it is very difficult to be used.
              </p>
              <p>
                The function point metric was originated by Albrecht and his
                colleagues at IBM in the mid-1970s. The name could be a bit
                misleading as the technique itself does not count the
                functions. Instead it tries to measures some aspects that
                determine the software complexity withouth taking into the
                differences between programming languages and development
                styles that change the LOC metric. In order to do so, it takes
                into account five major components that comprise a software
                product:
              </p>
              <ul>
                <li>
                  <b>External Inputs (EIs):</b> Elementary process in which
                  data crosses the boundary from outside to inside. This data
                  may come from a data input screen or another application.
                  The data may be used to maintain one or more internal logical
                  files. The data can be either control information or business
                  information. If the data is control information it does not
                  have to update an internal logical file.
                </li>
                <li>
                  <b>External Outputs (EOs):</b> Elementary process in which
                  derived data passes across the boundary from inside to
                  outside. Additionally, an EO may update an ILF. The data
                  creates reports or output files sent to other applications.
                  These reports and files are created from one or more internal
                  logical files and external interface file.
                </li>
                <li>
                  <b>External Inquiries (EQs):</b> Elementary process with both
                  input and output components that result in data retrieval
                  from one or more internal logical files and external
                  interface files. The input process does not update any
                  Internal Logical Files, and the output side does not contain
                  derived data.
                </li>
                <li>
                  <b>Internal Logical Files (ILFs):</b> A user identifiable
                  group of logically related data that resides entirely within
                  the applications boundary and is maintained through external
                  inputs.
                </li>
                <li>
                  <b>External Interface Files (EIFs):</b> A user identifiable
                  group of logically related data that is used for reference
                  purposes only. The data resides entirely outside the
                  application and is maintained by another application. The
                  external interface file is an internal logical file for
                  another application.
                </li>
              </ul>
              <p>
                Following figure provides a graphical example on how all these
                components work together and how they interact with the
                end-users.
              </p>
              <figure>
                <img src='images/unit2-fig0-fp.png'>
                <figcaption>Function Points Overview</figcaption>
              </figure>
              <p>
                Apart from being technology independent, this way of
                identifying the key software functions is very interesting as
                it is focused on the end-user point view: most of the
                components are thought from the user’s perspective (not the
                developers one), hence it works well with use cases.
              </p>
              <p>
                The number of function points is obtained by the addition of
                the number of occurrences of those components (each of them
                weighted by a different factor) multiplied by an adjustment
                factor chosen based on the software characteristics:
              </p>
              <p>
                FP = FC x VAF 
              </p>
              <p>
                Where:
              </p>
              <ul>
                <li>
                  FP: Is the Function Points
                </li>
                <li>
                  FC: Is the weighted function count
                </li>
                <li>
                  VAF: Is the adjustment factor that depends on software
                  characteristics
                </li>
              </ul>
              <p>
                In order to calculate the Function Points, every component is
                classified in three categories according to its complexity
                (low/medium/high). A different weight factor is assigned to
                every component type and category. The following weights are
                defined for every component and complexity:
              </p>
              <ul>
                <li>
                  Number of external inputs: 3-4-6.
                </li>
                <li>
                  Number of external outputs: 4-5-7. 
                </li>
                <li>
                  Number of external inquiries: 3-4-6.
                </li>
                <li>
                  Number of logical internal files: 7-10-15.
                </li>
                <li>
                  Number of external interface files: 5-7-10.
                </li>
              </ul>
              <p>
                When the number of components (classified by complexity) is
                available, given the previous weighting factors, the Function
                Counts (FCs) can be calculated based on the following formula:
              </p>
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                <mi>FC</mi>
                <mo>=</mo>
                </mrow>
                <mrow>
                <munderover>
                <mo>&Sum;</mo>
                <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mn>1,</mn>
                <mi>n</mi>
                <mo>=</mo>
                <mn>1,</mn>
                </mrow>
                <mrow>
                <mn>3,5</mn>
                </mrow>
                </munderover>
                <mrow>
                <msub>
                <mrow>
                <mi>w</mi>
                </mrow>
                <mrow>
                <mn>ij</mn>
                </mrow>
                </msub>
                <msub>
                <mrow>
                <mi>x</mi>
                </mrow>
                <mrow>
                <mn>ij</mn>
                </mrow>
                </msub>
                </mrow>
                </mrow>
              </math>
              <p>
                Where wij are the weighting factors and xij the number of
                ocurrences of each component in the software. i denotes
                complexity and j denotes the component type.  The following
                table shows graphically how can this function be calculated
                easily.
              </p>
              <table>
                <caption>FC Calculation</caption>
                <thead>
                  <tr>
                    <th>Type </th>
                    <th>Low Complexity </th>
                    <th>Mid Complexity </th>
                    <th>High Complexity </th>
                    <th>Total </th>
                  </tr>￼
                </thead>
                <tbody>
                  <tr>
                    <td>EI </td>
                    <td>_ x 3 + </td>
                    <td>_ x 4 + </td>
                    <td>_ x 6 + </td>
                    <td>= </td>
                  </tr>￼
                  <tr>
                    <td>EO </td>
                    <td>_ x 4 + </td>
                    <td>_ x 5 + </td>
                    <td>_ x 7 + </td>
                    <td>= </td>
                  </tr>
                  <tr>
                    <td>EQ </td>
                    <td>_ x 3 + </td>
                    <td>_ x 4 + </td>
                    <td>_ x 6 + </td>
                    <td>= </td>
                  </tr>
                  <tr>
                    <td>ILF </td>
                    <td>_ x 7 + </td>
                    <td>_ x 10 + </td>
                    <td>_ x 15 + </td>
                    <td>= </td>
                  </tr>
                  <tr>
                    <td>EIF </td>
                    <td>_ x 5 + </td>
                    <td>_ x 7 + </td>
                    <td>_ x 10 + </td>
                    <td>= </td>
                  </tr>
                </tbody>
              </table>
              <p>
                The complexity classification of each component is based on a
                set of standards that define complexity in terms of objective
                guidelines. For instance, for the external output component, if
                the number of data element types is 20 or more and the number
                of file types referenced is 2 or more, then complexity is high.
                If the number of data element types is 5 or fewer and the
                number of file types referenced is 2 or 3, then complexity is
                low. The following tables provide the standard categorization
                where:
              </p>
              <ul>
                <li>
                  DETs are equivalent to non-repeated fields or attributes.
                </li>
                <li>
                  RETs are equivalent to mandatory or optional sub-groups.
                </li>
                <li>
                  FTRs are equivalent to ILFs or EIFs referenced by that
                  transaction.
                </li>
              </ul>
              <p>
                <table>
                  <caption> ILF and EIF Complexity Matrix </caption>
                  <thead>
                    <tr>
                      <th> RETs</th><th> 1-19 DETs</th><th> 20-50 DETs</th><th> 51+ DETs</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>1</td> <td>Low</td> <td>Low</td> <td>Medium</td>
                    </tr>
                    <tr>
                      <td>2-5</td> <td>Low</td> <td>Medium</td> <td>High</td>
                    </tr>
                    <tr>
                      <td>6+</td> <td>Medium</td> <td>High</td> <td>High</td>
                    </tr>
                  </tbody>
                </table>
              </p>
              <p>
                <table>
                  <caption> EI Complexity Matrix </caption>
                  <thead>
                    <tr>
                      <th> FTRs</th><th> 1-4 DETs</th><th> 5-15 DETs</th><th> 16+ DETs</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>0-1</td> <td>Low</td> <td>Low</td> <td>Medium</td>
                    </tr>
                    <tr>
                      <td>2</td> <td>Low</td> <td>Medium</td> <td>High</td>
                    </tr>
                    <tr>
                      <td>3+</td> <td>Medium</td> <td>High</td> <td>High</td>
                    </tr>
                  </tbody>
                </table>
              </p>
              <p>
                <table>
                  <caption> EO and EQ Complexity Matrix </caption>
                  <thead>
                    <tr>
                      <th> FTRss</th><th> 1-5 DETs</th><th> 6-19 DETs</th><th> 20+ DETs</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>0-1</td> <td>Low</td> <td>Low</td> <td>Medium</td>
                    </tr>
                    <tr>
                      <td>2-3</td> <td>Low</td> <td>Medium</td> <td>High</td>
                    </tr>
                    <tr>
                      <td>4+</td> <td>Medium</td> <td>High</td> <td>High</td>
                    </tr>
                  </tbody>
                </table>
              </p>
              <p>
                In order to calculate the Value Adjustment Factor (VAF), 14
                characteristics of the software system must be scored (in a
                scale from 0 to 5) in terms of their effect on the software.
                The list of characteristics is:
              </p>
              <ol>
                <li>
                  <b>Data communications:</b> How many communication facilities
                  are there to aid in the transfer or exchange of information
                  with the application or system?
                </li>
                <li>
                  <b>Distributed data processing: </b> How are distributed data
                  and processing functions handled?
                </li>
                <li>
                  <b>Performance: </b> Did the user require response time or
                  throughput?
                </li>
                <li>
                  <b>Heavily used configuration: </b> How heavily used is the
                  current hardware platform where the application will be
                  executed?
                </li>
                <li>
                  <b>Transaction rate: </b> How frequently are transactions
                  executed daily, weekly, monthly, etc.?
                </li>
                <li>
                  <b>On-Line data entry: </b> What percentage of the
                  information is entered On-Line?
                </li>
                <li>
                  <b>End-user efficiency: </b> Was the application designed for
                  end-user efficiency?
                </li>
                <li>
                  <b>On-Line update: </b> How many ILF’s are updated by On-Line
                  transaction?
                </li>
                <li>
                  <b>Complex processing: </b> Does the application have
                  extensive logical or mathematical processing?
                </li>
                <li>
                  <b>Reusability: </b> Was the application developed to meet
                  one or many user’s needs?
                </li>
                <li>
                  <b>Installation ease: </b> How difficult is conversion and installation?
                </li>
                <li>
                  <b>Operational ease: </b> How effective and/or automated are
                  start-up, back-up, and recovery procedures?
                </li>
                <li>
                  <b>Multiple sites: </b> Was the application specifically
                  designed, developed, and supported to be installed at
                  multiple sites for multiple organizations?
                </li>
                <li>
                  <b>Facilitate change: </b> Was the application specifically
                  designed, developed, and supported to facilitate change?
                </li>
              </ol>
              <p>
                Once all these characteristics are assessed, they are summed,
                based on the following formula, to arrive at the value
                adjustment factor (VAF):
              </p>
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                <mi>VAF</mi>
                <mo>=</mo>
                <mi>0.65</mi>
                <mo>+</mo>
                <mi>0.01</mi>
                </mrow>
                <mrow>
                <munderover>
                <mo>&Sum;</mo>
                <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
                </mrow>
                <mrow>
                <mn>14</mn>
                </mrow>
                </munderover>
                <mrow>
                <msub>
                <mrow>
                <mi>c</mi>
                </mrow>
                <mrow>
                <mn>i</mn>
                </mrow>
                </msub>
                </mrow>
                </mrow>
              </math>
              <p>
                Where ci is the score for general system characteristic i.
              </p>
              <p>
                Over the years the function point metric has gained acceptance
                as a key productivity measure from a practical point of view.
                However, the meaning of function point and the derivation
                algorithm and its rationale may need more research and more
                theoretical groundwork. Furthemore, function point counting can
                be time-consuming and expensive, and accurate counting requires
                certified function point specialists.
              </p>
            </section>
          </section>
        </section> <!-- intrinsic -->
        <section>
          <h3>Customer satisfaction metrics</h3>
          <section>
            <h4>Customer Problem Metrics</h4>
            <p>
              Another product quality metric vastly used in the software
              industry measures the problems customers encounter when using
              the product.
            </p>
            <p>
              For the defect denstity metric (section 1.2.1.2), the numerator
              was the number of valid defects. However, from the
              customers’ standpoint, all problems they encounter
              while using the software product, not just the valid defects,
              are problems with the software. Software problems suffered by
              end- users that are not valid defects may be:
            </p>
            <ul>
              <li>
                Usability problems.
              </li>
              <li>
                Unclear documentation or information.
              </li>
              <li>
                Duplicates of valid defects (defects that were reported by
                other customers and fixes were available but the current
                customers did not know of them).
              </li>
              <li>
                User errors.
              </li>
            </ul>
            <p>
              These so-called non-defect-oriented problems, together with the
              defect problems, constitute the total problem space of the
              software from the customers’ perspective.
            </p>
            <p>
              The problems metric is usually expressed in terms of problems
              per user month (PUM):
            </p>

            <math xmlns="http://www.w3.org/1998/Math/MathML" styel="color: red">
              <mrow>
                <mi>PUM</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow><mtext>
                    Total number of problems that customers reported during a period of time
                  </mtext></mrow>
                  <mrow><mtext>
                    Total number of license months of the software during that period
                  </mtext></mrow>
                </mfrac>
              </mrow>
            </math>
            <p>
              Where the total number of license-months is the number of months
              all the users have been using the software and may be calculated
              multipling the Number of install licenses of the software by the
              Number of months in the calculation period.
            </p>
            <p>
              PUM is usually calculated for each month after the software is
              released to the market, and also for monthly averages by year.
              Note that the denominator is the number of license-months instead
              of thousand lines of code or function point, and the numerator is
              all problems customers encountered. Basically, whereas the defect
              density focuses in the number of real problems with regards to
              the software complexity, this metric relates detected problems to
              software usage.
            </p>
            <p>
              There are different approaches to minimize PUM:
            </p>
            <ul>
              <li>
                Improve the development process and reduce the product
                defects.
              </li>
              <li>
                Reduce the non-defect-oriented problems by improving all
                aspects of the products (such as usability, documentation),
                customer education, and support.
              </li>
              <li>
                Increase the sale (the number of installed licenses) of the
                product.
              </li>
            </ul>
            <p>
              The first two approaches reduce the numerator of the PUM metric,
              and the third increases the denominator. The result of any of
              these actions will be that the PUM metric has a lower value. All
              three approaches make good sense for quality improvement and
              business goals for any organization. The PUM metric, therefore,
              is a good metric. The only minor drawback is that when the
              business is in excellent condition and the number of software
              licenses is rapidly increasing, the PUM metric will look
              extraordinarily good (low value) and, hence, the need to continue
              to reduce the number of customers’ problems (the numerator of the
              metric) may be under- mined. Therefore, the total number of
              customer problems should also be monitored and aggressive
              year-to-year or release-to-release improvement goals set as the
              number of installed licenses increases.  However, unlike valid
              code defects, customer problems are not totally under the control
              of the software development organization. Therefore, it may not
              be feasible to set a PUM goal that the total customer problems
              cannot increase from release to release, especially when the
              sales of the software are increasing.
            </p>
            <p>
              The key points of the defect rate metric and the customer
              problems metric are briefly summarized in the following table.
              The two metrics represent two perspectives of product quality.
              For each metric the numerator and denominator match each other
              well: Defects relate to source instructions or the number of
              function points, and problems relate to usage of the product. If
              the numerator and denominator are mixed up, poor metrics will
              result. Such metrics could be counterproductive to an
              organization’s quality improvement effort because they will cause
              confusion and wasted resources.
            </p>
            <table>
              <caption>DDR vs PUM</caption>
              <thead>
                <tr>
                  <th></th>
                  <th>Defect Density Rate</th>
                  <th>PUM</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Numerator</td>
                  <td>Valid and Unique defects</td>
                  <td>All customer problems</td>
                </tr>
                <tr>
                  <td>Denominator</td>
                  <td>Size of Product</td>
                  <td>Usage of Product</td>
                </tr>
                <tr>
                  <td>Measurement</td>
                  <td>Producer Perspective</td>
                  <td>Consumer Perspective</td>
                </tr>
                <tr>
                  <td>Scope</td>
                  <td>Intrinsic Product Quality</td>
                  <td>Intrinsic Product Quality + Other</td>
                </tr>
              </tbody>
            </table>
            <p>
              The customer problems metric can be regarded as an intermediate
              measurement between defects measurement and customer
              satisfaction. To reduce customer problems, one has to reduce
              the functional defects in the products and, in addition,
              improve other factors (usability, documentation, problem
              rediscovery, etc.)
            </p>
          </section>
          <section>
            <h4>Customer Satisfaction Metrics</h4>
            <p>
              Customer satisfaction is often measured by customer survey data
              in which the users are asked to qualify the software or
              characteristics of the software through a scale.
            </p>
            <p>
              Based on the survey result data, several metrics with slight
              variations can be constructed and used, depending on the
              purpose of analysis. For example:
            </p>
            <ul>
              <li>
                Percent of completely satisfied customers.
              </li>
              <li>
                Percent of satisfied customers (satisfied and completely
                satisfied)
              </li>
              <li>
                Percent of dissatisfied customers (dissatisfied and
                completely dissatisfied)
              </li>
              <li>
                Percent of nonsatisfied (neutral, dissatisfied, and
                completely dissatisfied)
              </li>
            </ul>
            <p>
              In addition to forming percentages for various satisfaction or
              dissatisfaction categories, the net satisfaction index (NSI) is
              also used to facilitate comparisons across product. NSI ranges
              from 0% (all customers are completely dissatisfied) to 100% (all
              customers are completely satisfied). If all customers are
              satisfied (but not completely satisfied), NSI will have a value
              of 75%. This weighting approach, however, may be masking the
              satisfaction profile of one’s customer set. For example, if half
              of the customers are completely satisfied and half are neutral,
              NSI’s value is also 75%, which is equivalent to the scenario that
              all customers are satisfied. If satisfaction is a good indicator
              of product loyalty, then half completely satisfied and half
              neutral is certainly less positive than all satisfied.
            </p>
          </section>
        </section> <!-- customer satisfaction -->
      </section> <!-- product quality-->
      <section>
        <h2>In process Quality Metrics </h2>
        <section>
          <h3>Defect Density After a Development Cycle</h3>
          <p>
            Defect rate during a development cycle is usually positively 
            correlated with the defect rate in the next phases. For instance,
            the defect rate after integration testing is usually positively
            correlated with the defect rate in the field. Higher defect rates
            found during a phase is an indicator that the software has
            experienced higher error injection during that phase, unless the 
            higher testing defect rate is due to an extraordinary testing 
            effort (for example, additional testing or a new testing approach 
            that was deemed more effective in detecting defects). The
            rationale for the positive correlation is simple: Software defect
            density never follows the uniform distribution. If a piece of code
            or a product has higher testing defects, it is a result of more
            effective testing or it is because of higher latent defects in the
            code. Myers suggested a counterintuitive principle that the more
            defects found during testing, the more defects will be found later.
          </p>
          <p>
            This simple metric of defects per KLOC or function point is
            especially useful to monitor subsequent releases of a product in
            the same development organization. The development team or the
            project manager can use the following scenarios to judge the
            release quality:
          </p>
          <ol>
            <li>
              If the defect rate during testing is the same or lower than that
              of the previous release (or a similar product), then ask: Does
              the testing for the current release deteriorate?
              <ul>
                <li>
                  If the answer is no, the quality perspective is positive.
                </li>
                <li>
                  If the answer is yes, you need to do extra testing (e.g.,
                  add test cases to increase coverage, blitz test, customer
                  testing, stress testing, etc.).
                </li>
              </ul>
            </li>
            <li>
              If the defect rate during testing is substantially higher
              than that of the previous release (or a similar product),
              then ask: Did we plan for and actually improve testing
              effectiveness?
              <ul>
                <li>
                  If the answer is no, the quality perspective is negative.
                  Ironically, the only remedial approach that can be taken
                  at this stage of the life cycle is to do more testing,
                  which will yield even higher defect rates.
                </li>
                <li>
                  If the answer is yes, then the quality perspective is the
                  same or positive.
                </li>
              </ul>
            </li>
          </ol>
          <p>
            This concept is shown graphically in the next diagram:
          </p>
          <figure>
            <img src='images/unit2-fig25-defectcuadrant.png' height='300px' width='400px'>
            <figcaption>Understanding Evolution of Defect Density</figcaption>
          </figure>
        </section>
        <section>
          <h3>Defect Arrival Pattern </h3>
          <p>
            Overall defect density during testing is a summary indicator. The
            pattern of defect arrivals (or for that matter, times between
            failures) gives more information. Even with the same overall defect
            rate during testing, different patterns of defect arrivals indicate
            different quality levels in the field.
          </p>
          <p>
            Next figure shows two contrasting patterns for both the defect
            arrival rate and the cumulative defect rate. Data were plotted from
            44 weeks before code-freeze until the week prior to code-freeze.
            In both projects, the overall defect count is the same, however the
            potential forecast of the quality on the field is quite different.
            In the first project, during the last weeks, the number of defects
            reported every week is smaller and is tending to zero.
            The second project, represented by the charts on the right side,
            follows the opposite pattern. This obviously indicates that testing
            started late, the test suite was not sufficient, and that the 
            testing ended prematurely. It's extremely likely that this project,
            if released as it is, would lead to even more defects in the field.
          </p>
          <figure>
            <img src='images/unit2-fig1.png'>
            <figcaption>Two Contrasting Arrival Patterns during Testing</figcaption>
          </figure>
          <p>
            The objective is always to look for defect arrivals that stabilize
            at a very low level, or times between failures that are far apart,
            before ending the testing effort and releasing the software to the
            field. Such declining patterns of defect arrival during testing are
            indeed the basic assumption of many software reliability models.
            The time unit for observing the arrival pattern is usually weeks
            and occasionally months. For reliability models that require
            execution time data, the time interval is in units of CPU time.
          </p>
          <p>
            When we talk about the defect arrival pattern, there are actually 
            three slightly different metrics, which should be looked at 
            simultaneously:
          </p>
          <ul>
            <li>
              The defect arrivals (defects reported) during the testing phase
              by time interval (e.g. week). These are the raw number of 
              arrivals, not all of which are valid defects.
            </li>
            <li>
              The pattern of valid defect arrivals when problem determination 
              is done on the reported problems. This is the true defect pattern.
            </li>
            <li>
              The pattern of defect backlog overtime. This metric is needed
              because development organizations cannot investigate and fix
              all reported problems immediately. This metric is a workload
              statement as well as a quality statement. If the defect backlog
              is large at the end of the development cycle and a lot of fixes
              have yet to be integrated into the system, the stability of the
              system (hence its quality) will be affected. Retesting
              (regression test) is needed to ensure that targeted product
              quality levels are reached.
            </li>
          </ul>
        </section>
        <section>
          <h3>Defect Removal Metrics </h3>
          <p>
            We have just introduced an interesting concept. Detecting a defect
            doesn't mean it is going to be automatically removed. This can 
            happen because of many different reasons:
          </p>
          <ul>
            <li>
              Lack of time: The solution of the defect requires a lot of time
              to be properly fixed (detect root, fix it, ensure no other 
              functionality breaks because of the fix, etc.)
            </li>
            <li>
              Lack of bandwidth: The team is focused on implementing features so
              no immediate attention is put on fixing that defect.
            </li>
            <li>
              Lack of importance: The defect is minor or not important enough 
              to become a priority for the team.
            </li>
            <li>
              Lack of knowdlege: The defect is well-known but the root of the 
              defect is not known yet (remember a defect is just a symptom).
            </li>
          </ul>
          <p>
            A metric intended to distinguish between defect detection and defect
            removal is the Defect Removal Pattern, that describes the evolution
            of the number of defects removed over the time. This metric can
            be calculated per unit of time or per phase of the project (e.g. 
            iteration).
          </p>
          <p>
            Some related metrics are the "average time to detect a defect" 
            (which provides an indication about how good the process is about
            detecting defects) and the "average time to fix a defect" which 
            is an indiator of how good the process is with respect to fixing
            defects once they have been detected. These metrics are key as
            we should remember that the later a defect is detected and fixed,
            the more expensive it is.
          </p>
        </section>
        <section>
          <h3>Defect Backlog / Burndown</h3>
          <p>
            A burndown chart is a graphical representation of the amount of work
            left to be done vs. the time. It's typically used in Agile 
            methodologies to check sprint evolution, measure team speed, etc.
            In those cases, the amount work is always decreasing as the tasks 
            for the sprint are identified before the sprint starts.
          </p>
          <figure>
            <img src='images/unit2-fig26-agileburndown.png'
             height='400px' width='600px'>
            <figcaption>Typical Burndown Chart</figcaption>
          </figure>
          <p>
            An equivalent concept is the Defect Backlog / Burndown. In that case
            the graphic does not represent the amount of work to be done but 
            the amount of unfixed defects. The curve can go down if no more 
            defects are found and the remaining ones are fixed, or go up if the
            number of detected defects overpace the fix rate.
          </p>
          <figure>
            <img src='images/unit2-fig27-defectburndown.png' 
             height='400px' width='600px'>
            <figcaption>Defect Burndown Chart</figcaption>
          </figure>
          <p>
            In the example above, we can see that the red line shows the number
            of cumulative defects (fixed or unfixed), the green one shows the 
            number of fixed ones, whereas the black line shows the "delta" 
            between detected and fixed defects (i.e. the size of the defect 
            backlog).
          </p>
          <p>
            Ideally, the defect backlog count should be zero before releasing
            a product. But in big product that is nearly impossible. This 
            requires product managers to play with the four key aspects of
            any software project: resources, scope, time and quality. In 
            particular, the following actions could be done:
          </p>
          <ul>
            <li>
              Increase the number of resources fixing defects: The idea is
              adding more resources to work on fixing defects. However, we 
              should be extremely cautious about this as adding resources late
              to a project can lead to additional delays.
            </li>
            <li>
              Reduce the product scope: don't let more features being added
              at a given point of time (no more features 1 month before the
              target release date). This leads to more resoures focused on
              fixing defects and fewer bugs injected by new features landing
              on the project.
            </li>
            <li>
              Postpone the release date: Ask for more time so more time can be
              spent fixing defects.
            </li>
            <li>
              Reduce the target quality by not paying attention to any bug but 
              just to the critical ones (e.g. blocker defects).
            </li>
          </ul>
          <p>
            One approach that is used sometimes to make the defect backlog 
            go to zero is using triage meetings to determine which defects
            are blockers and which ones are not. The idea is that the closer
            the release date is, the more difficult is to consider a bug
            as a blocker for the release. However, having a clear set of
            guidelines about what is a blocker and what is not, is also very
            helpful, for instance, Mozilla used <a
              href="https://wiki.mozilla.org/B2G/Triage#Blocker_Triage_Guidelines">
            a particualr one for FirefoxOS</a>
          </p>
        </section>
        <section>
          <h3>Defect Removal Effectiveness</h3>
          <p>
            Defect removal effectiveness can be defined as follows:
          </p>
          <math xmlns="http://www.w3.org/1998/Math/MathML" styel="color: red">
            <mrow>
              <mi>DRE</mi>
              <mo>=</mo>
              <mi>Defects Removed in a Development Phase</mi>
              <mo>*</mo>
              <mi>100% Defects latent in the product</mi>
            </mrow>
          </math>
          <p>
            It provides a measure of the percentage of defects removed in one
            phase with regards to the overall number of defects in the code
            when entering into that phase. As the total number of latent
            defects in the product at any given phase is not known, the
            denominator of the metric can only be approximated which is usually
            done through:
          </p>
          <math xmlns="http://www.w3.org/1998/Math/MathML" styel="color: red">
            <mi>Defects Found in a phase </mi><mo>+</mo><mi>Defects found later</mi>
            </mrow>
          </math>
          <p>
            The metric can be calculated for the entire development process,
            for the front end (before code integration), and for each phase. It
            is called early defect removal
          </p>
          <p>
            The higher the value of the metric, the more effective the
            development process and the fewer the defects escape to the next
            phase or to the field. This metric is a key concept of the defect
            removal model for software development.
          </p>
          <p>
            For instance, if during the development of a product 80 bugs were
            found and fixed, but there were still 20 defects latent that were
            found by the customers when the product hit the field, the DRE would
            be:
          </p>
          <p>
            DRE = 80 / (80 + 20) = 80%
          </p>
          <p>
            In average, 80 of every 100 defects were removed.
          </p>
          <p>
            Another view of this metric is depicted in the following Figure.
          </p>
          <figure>
            <img src='images/unit2-fig4.png'>
            <figcaption>Defect Removal and Injection</figcaption>
          </figure>
          <p>
            It shows how are defects injected, detected and repair during a
            project phase. Based on it, another way to calculate the defect
            removal efficiency can be found:
          </p>
          <math xmlns="http://www.w3.org/1998/Math/MathML" styel="color: red">
            <mrow>
              <mi>DRE</mi>
              <mo>=</mo>
              <mfrac>
                <mrow><mtext>
                  Defects Removed (at the step)
                </mtext></mrow>
                <mrow>
                  <mtext> Defects existing on step entry + Defects Injected during this step</mtext>
                </mrow>
              </mfrac>
              <mo>*</mo>
              <mi>100</mi>
            </mrow>
          </math>
          <p>
            The following table is an example in which the data about when are
            errrors injected and detected in a software project is provided.
          </p>
          <table>
            <theader>
              <tr>
                <th rowspan=2 colspan=2></th>
                <th colspan=5>Origin of the defect</th>
              </tr>
              <tr>
                <th>Iteration 1</th>
                <th>Iteration 2</th>
                <th>Iteration 3</th>
                <th>Iteration 4</th>
                <th>TOTAL REMOVED</th>
              </tr>
            </theader>
            <tbody>
              <tr>
                <td rowspan=5>Where Found?</td>
                <td>Iteration 1</td>
                <td>5</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td><b>5</b></td>
              </tr>
              <tr>
                <td>Iteration 2</td>
                <td>10</td>
                <td>15</td>
                <td>-</td>
                <td>-</td>
                <td><b>25</b></td>
              </tr>
              <tr>
                <td>Iteration 3</td>
                <td>5</td>
                <td>5</td>
                <td>10</td>
                <td>-</td>
                <td><b>20</b></td>
              </tr>
              <tr>
                <td>Iteration 4</td>
                <td>5</td>
                <td>5</td>
                <td>0</td>
                <td>5</td>
                <td><b>15</b></td>
              </tr>
              <tr>
                <td><b>Total Injected</b></td>
                <td><b>25</b></td>
                <td><b>25</b></td>
                <td><b>10</b></td>
                <td><b>5</b></td>
                <td><b>65</b></td>
              </tr>
            </tbody>
          </table>
          <p>
            With that data, the DRE could be calculated for different phases 
            of the software development process. Some examples are shown below:
          </p>
          <div class="example">
            <pre>
              During the First Iteration the total numbers of defect injected was 25.

              The meaning of the value "5" in the intersection of "Iteration 1"
              row and column is that during that phase, only 5 defects were
              removed.

              The meaning of the value 10 in the intersetion of row "Iteration
              2" and column "Iteration 1" is that during Iteration 2, 10
              defects that were originated in Iteration 1 were removed. 

              Equally, the meaning of the value 15 in the intersetion of row
              "Iteration 2" and column "Iteration 2" is that during Iteration
              2, 15 defects that were also originated in Iteration 2 were
              removed.

              We could calculate the DRE of all the different iterations quite
              easily:

              * Iteration 1: DRE = 5/25 = 20%
              * Iteration 2: DRE = (10+15) / [(25+25) - 5] = 25/45 = 55%;
              * Iteration 3: DRE = (5+5+10) / [(25+25+10) - (5+25)] = 20/30 = 66%;
              * Iteration 4: DRE = (5+5+0+5) / [(25+25+10+5) - (5+25+20)] = 15/15 = 100%;
            </pre>
          </div>
          <p>
            The following table describes for each of the software development
            process phases the most important sources of defect injection and
            removal.
          </p>
          <table>
            <tr>
              <th>Delopment Phase</th>
              <th>Defect Injection</th>
              <th>Defect Removal</th>
            </tr>
            <tr>
              <td>Requirements</td>
              <td>Requirements Gathering Process and Specification Development</td>
              <td>Requirement Analysis and Review</td>
            </tr>
            <tr>
              <td>High Level Design</td>
              <td>Design</td>
              <td>High Level Design Inspections</td>
            </tr>
            <tr>
              <td>Low Level Design</td>
              <td>Design</td>
              <td>Low Level Design Inspections</td>
            </tr>
            <tr>
              <td>Code Implementation</td>
              <td>Coding</td>
              <td>Code Inspections Testing</td>
            </tr>
            <tr>
              <td>Integration Build</td>
              <td>Integration and Build Process</td>
              <td>Build Verification Testing</td>
            </tr>
            <tr>
              <td>Unit Test</td>
              <td>Bad Fixes</td>
              <td>Testing Itself</td>
            </tr>
            <tr>
              <td>Component Test</td>
              <td>Bad Fixes</td>
              <td>Testing Itself</td>
            </tr>
            <tr>
              <td>System Test</td>
              <td>Bad Fixes</td>
              <td>Testing Itself</td>
            </tr>
          </table>
        </section>
      </section> <!-- process quality-->
      <section>
        <h2>Software Metrics VS. Quality Metrics</h2>
        <p>
          This chapter has been addresing so far metrics that are related with
          a direct measurement of the software quality. However, it is also
          critical to consider that those metrics usually have a direct
          relationship with some software characteristics that could not be
          considered as directly related with the software quality.
        </p>
        <p>
          Some intrinsic characteristics of the software that usually affect
          the software quality (either internal or external) are:
        </p>
        <ul>
          <li>
            Size: Obviously, the bigger a software is, either measured in 
            KLOCs or in Function Points, more opportunities for error there 
            will be.
          </li>
          <li>
            Control Flow Complexity: When the software has several decision 
            paths, many conditions to assess with different responses to them,
            the likelyhood of defects is also higher.
          </li>
          <li>
            Intermodule coupling: When different software modules have too 
            many dependencies, the likelyhood of defects is also higher because 
            of the cross-module error propagation.
          </li>
          <li>
            Complexity: Code that has been designed in a complex manner is 
            usually also more exposed to defects than code designed using 
            simple patterns.
          </li>
          <li>
            Understandability: If the code cannot be easily understood, 
            whenever an error is detected, it will be more difficutl to find 
            the defect and fix it.
          </li>
          <li>
            Testability: Obviously, software difficult to be tested is 
            more exposed to bugs.
          </li>
          <li>
            Size: Obviously, the bigger a software is, either measured 
            in KLOCs or in Function Points, more opportunities for error 
            there will be defects than the software that can be easily tested.
          </li>
        </ul>
        <p>
          Different metrics exist to take into account all those aspects in 
          early phases of the software development process and take 
          preventive measures. E.g. if the code is extremely complex a 
          refactoring of the software should be done in order to minimize the 
          likelyhood of defects.
        </p>
        <p>
          Some example of metrics used (in Object Oriented Programming) are:
        </p>
        <ul>
          <li>
            Average Method Size: Ideally this parameter should be under 24 
            LOC in C++ or Java.
          </li>
          <li>
            Average Number of Methods per class: Should be less than 20. 
            Bigger averages indicate too much responsibility in too few 
            classes
          </li>
          <li>
            Average Number of instance Variables per class: Should be 
            less than 6. More instance variables indicate that one class 
            is doing more than what is should.
          </li>
          <li>
            Class Hierarchy Nesting Level (Depth of Inheritance Tree, DIT): Too 
            deep inheritance tree means that hierarchy is more complicated that 
            it should. Ideally it should be less than 6.
          </li>
          <li>
            Number of classes and class relationships in a subsystem/module: 
            Should be relatively high. This item relates to high cohesion of 
            classes in the same subsystem. If one or more classes in a 
            subsystem don't interact with many of the other classes, they 
            might be better placed in another subsystem.
          </li>
          <li>
            Number of Subsystem and Subsystem Relationships: Should be less 
            than the number in the previous metric.
          </li>
          <li>
            Average Number of comment lines (per method): It is important 
            this metric is high enough to ensure the code is maintainable 
            (1 at the very minimum).
          </li>
          <li>
            Number of times class is reused: If a class is not being reused 
            in different applications (especially an abstract class), it might 
            need to be redesigned.
          </li>
          <li>
            Number of classes and methods thrown away: Should occur at a steady 
            rate throughout most of the development process. If this is not 
            occurring, it may be due because the software development process 
            is following an incremental development instead of an iterative one.
          </li>
        </ul>
      </section> <!-- product quality-->
    </section>
    <section>
      <h1> Software Configuration Management </h1>
      <section>
        <h2>What is SCM?</h2>
        <p>
          Quality Assurance is only one part of the activities that are used to
          improve Software Quality. However QA is per-se not enough as it does
          not define how the software is managed, for instance:
        </p>
        <ul>
          <li>
            How are new versions of the same software managed? 
          </li>
          <li>
            How are different versions of the same software maintained in parallel?
          </li>
          <li>
            Once bugs are detected, how are they tracked and fixed.
          </li>
          <li>
            How can different people cooperate in the software development process?
          </li>
        </ul>
        <p style="background-color: yellow">
          SCM could be defined as a framework for managing the evolution of 
          software throughout all the stages of Software Development Process.
        </p>
        <p>
          There are multiple definitions for SCM and in some cases the SCM
          acronym is used with different meanings (Software/Source Code
          Management, Software/Source Code Change Control Management,
          Software/Source Configuration Management...). Roger Pressman states
          [[SOFTWARE-ENGINEER-PRACTICIONER]] that SCM is a "set of activities
          designed to control change by identifying the work products that are
          likely to change, establishing relationships among them, defining
          mechanisms for managing different versions of these work products,
          controlling the changes imposed, and auditing and reporting on the
          changes made."
        </p>
        <p>
          In summary, SCM is a set of activities intended to guarantee:
        </p>
        <ul>
          <li>Software Versions are properly managed even in parallel.</li>
          <li>Different people can work in the same software project.</li>
          <li>Issues are tracked.</li>
          <li>Software changes are tracked and can be related to issues.</li>
          <li>The software development process follows a set of rules.</li>
          <li>Software releases and deliveries are properly managed.</li>
        </ul>
      </section>
      <section>
        <h2>Why SCM?</h2>
        <p>
          When used effectively during a product's whole life cycle, SCM
          identifies software items to be developed, avoids chaos when changes
          to software occur, provides needed information about the state of
          development, and assists the audit of both the software and the SCM
          processes. Therefore, its purposes are to support software
          development and to achieve better software quality. Additionally, a
          good SCM system should also help to reduce (or at least control)
          costs and effort involved in making changes to a system.
        </p>
      </section>
      <section>
        <h2>Key SCM Activities</h2>
        <p>
          IEEE's (IEEE Std. 828-1990) traditional definition of SCM included
          four key activities: configuration identification, configuration
          control, configuration status accounting and configuration audits.
          However, a successful implementation of SCM also requires careful
          planning and a good release management and processing.  Next figure
          represents all these activities graphically:
        </p>
        <figure>
          <img src='images/unit3-fig1.png'>
          <figcaption>SCM Activities</figcaption>
        </figure>
        <p>
          The following figure provides a breakdown of all the SCM activities 
          into more granular topics.
        </p>
        <figure>
          <img src='images/unit3-fig2.png'>
          <figcaption>SCM Activities breakdown</figcaption>
        </figure>
        <section>
          <h3>Management and Planning</h3>
          <p>
            A successful SCM implementation requires careful planning and
            management. This, in turn, requires an understanding of the
            organizational context for, and the constraints placed on, the
            design and implementation of the SCM process.
          </p>
          <p>
            Some aspects that should be decided during this activity are:
          </p>
          <ul>
            <li>
              The types of documents to be managed and a document-naming
              scheme.
            </li>
            <li>
              Who takes responsibility for the CM procedures and creation of
              baselines?
            </li>
            <li>
              Policies for change control and version management.
            </li>
            <li>
              Tools to be used and process linked to their usage.
            </li>
          </ul>
        </section>
        <section>
          <h3>Configuration Identification</h3>
          <p>
            The software configuration identification activity identifies items
            to be controlled, establishes identification schemes for the items
            and their versions, and establishes the tools and techniques to be
            used in acquiring and managing controlled items. These activities
            provide the basis for the other SCM activities.
          </p>
          <p>
            <u>Configuration Item:</u> A configuration item is any possible
            part of the development or delivery of a system or product that
            it's necessary to identify, produce, store, use and change
            individually.  Many people associate configuration item with a
            source code file, but configuration items are not limited to that,
            many other items could be identified and managed such as:
          </p>
          <ul>
            <li>System data files</li>
            <li>System build files and scripts</li>
            <li>Requirements, Interface, Design specifications</li>
            <li>Test plans, procedures, data sets and results</li>
            <li>User documentation</li>
            <li>Compilers, Linkers, Debuggers</li>
            <li>Shell scripts</li>
            <li>Other related support tools</li>
          </ul>
          <p>
            For each configuration item, additional information apart from the
            item itself is controlled by the SCM. As it is data about data, it
            is called metadata. Every configuration item must have a unique
            identification that is sometimes also called label. Additionally,
            metadata may include additional information such as:
          </p>
          <ul>
            <li>Name</li>
            <li>Version</li>
            <li>Status</li>
            <li>Date</li>
            <li>Location</li>
            <li>...</li>
          </ul>
          <p>
            A first step in controlling change is to identify the software
            items to be controlled. This involves understanding the software
            configuration within the context of the system configuration,
            selecting software configuration items, developing a strategy for
            labelling software items and describing their relationships, and
            identifying the baselines to be used.
          </p>
          <p>
            <u>Software Configuration:</u> A software configuration is the set
            of functional and physical characteristics of software as set forth
            in the technical documentation or achieved in a product.
          </p>
          <p>
            Selecting Configuration Items: It is an important process in which
            a balance must be achieved between providing adequate visibility
            for project control purposes and providing a manageable number of
            controlled items. The items of a configuration should include all
            the items that are part of a given software release.
          </p>
          <p>
            Defining relationships and interfaces between the various
            configuration items is key as it also affects other SCM activities
            such as software building or assessing the impact of suggested
            changes. The identification or labelling scheme used should support
            the need to evolve software items and their relationships (e.g.
            configuration item X requires version A of configuration item Y).
          </p>
          <p>
            Identifying the baselines is another critical task of SCM
            Identification. A software baseline is a set of software
            configuration items formally designated and fixed at a specific
            time during the software life cycle. The term is also used to refer
            to a particular version of a software configuration item that has
            been agreed on. In either case, the baseline can only be changed
            through formal change control procedures. A baseline, together with
            all approved changes to the baseline, represents the current
            approved configuration.
          </p>
        </section>
        <section>
          <h3>Configuration Change Control</h3> 
          <p>
            The software is subject to continuous changes that are coming from
            different sources: 
          </p>
          <ul>
            <li>
              Users that have new needs
            </li>
            <li>
              Developers that identify issues on the software
            </li>
            <li>
              Market forces that identify new business needs and opportunities
            </li>
          </ul>
          <p>
            Change Control takes care of keeping track of these changes and
            ensures that they are implemented in a controlled manner.
          </p>
          <p>
            The most important activity from a Change Control point of view is
            the definition of how are changes made:
          </p>
          <ul>
            <li>
              Can any developer change any configuration items?
            </li>
            <li>
              Do developers need to raise an issue or a Change Request before
              changing a configuration item?
            </li>
            <li>
              Which configurations or baselines developers can modify? E.g.
              some baselines should be read-only (tags), some others may be
              modifiable only after a Change Request has been made (branches)
              and some others may be modifiable with no restriction.
            </li>
            <li>
              Do changes need a third-party approval before they are added to
              the SCM system?
            </li>
            <li>
              How are other developers notified about changes?
            </li>
            <li>
              What is the link to the Issue Tracker system?
            </li>
            <li>
              Is there any need to test anything or build the system before 
              accepting a change?
            </li>
          </ul>
          <p>
            In short, the key thing is having a clear working flow, you can
            find an <a
            href="https://developer.mozilla.org/en-US/Firefox_OS/Developing_Gaia/Submitting_a_Gaia_patch">
            example about the FirefoxOS flow</a>.  the process for making
            changes is clear, it is also important to specify how the revision
            history of configuration items is going to be kept, how other
            developers are going to be notified about those changes:
          </p>
          <ul>
            <li>Maintaining baselines</li>
            <li>Processing changes</li>
            <li>Developing change report forms</li>
            <li>Controlling release of the product</li>
          </ul>
        </section>
        <section>
          <h3>Configuration Status Accounting</h3>
          <p>
            Configuration status accounting main target is recording and
            reporting of information needed for effective management of the
            software configuration.
          </p>
          <p>
            The information that should be available is diverse:
          </p>
          <ul>
            <li>Which are the different available baselines.</li>
            <li>Which is the approved configuration.</li>
            <li>Which are the issues raised for every configuration.</li>
            <li>Which is the status of all the issues and changes.</li>
          </ul>
          <p>
            In order to provide and control all this information a good tool
            support is needed. This could be part of the Configuration Item
            Management system or another independent tool that is integrated
            with it.
          </p>
          <p>
            Reported information can be used by various organizational and
            project elements, including the development team, the maintenance
            team, project management, and software quality activities.
            Reporting can take the form of ad hoc queries to answer specific
            questions or the periodic production of predesigned reports. Some
            information produced by the status accounting activity during the
            course of the life cycle might become quality assurance records.
          </p>
          <p>
            In addition to reporting the current status of the configuration,
            the information obtained by this system can serve as a basis for
            various measurements of interest to management, development, and
            SCM. Examples include the number of change requests per
            configuration item and the average time needed to implement a
            change request, defect arrival pattern per release/component...
          </p>
        </section>
        <section>
          <h3>Configuration Audting</h3>
          <p>
            The purpose of configuration audits is to ensure that the software
            product has been built according to specified requirements
            (Functional Configuration Audit, FCA), to determine whether all the
            items identified as a part of CI are present in the product
            baseline (Physical Configuration Audit, PCA), and whether defined
            SCM activities are being properly applied and controlled (SCM
            system audit or in-process audit). A representative from
            management, the QA department, or the customer usually performs
            such audits. The auditor should have competent knowledge of both
            SCM activities and the project.
          </p>
          <p>
            The author should check the product is complete, consistent (e.g.
            "Are all the correct versions of files used in this current
            release?"), that no outstanding issues exist (e.g. "There are no
            critical defects or CRs") and that the product has passed all the
            required tests to ensure its quality.
          </p>
          <p>
            The output of the audit should specify whether the product's
            performance requirements have been achieved by the product design
            and the product design has been accurately documented in the
            configuration documentation.
          </p>
          <p>
            In order to properly perform this activity is important to:
          </p>
          <ul>
            <li>Define the audit schedule and procedures.</li>
            <li>Identify who will perform the audits.</li>
            <li>Do the audits on the established baselines.</li>
            <li>Generate audit reports.</li>
          </ul>
        </section>
        <section>
          <h3>Release Build, Management and Delivery</h3>
          <p>
            The term "release" is used to refer to a software configuration
            that is distributed outside of the development team. This includes
            internal releases as well as distribution to end-users. When
            different versions of software are available for different platform
            configurations it is frequently necessary to create multiple
            releases for delivery.
          </p>
          <p>
            <u>Building the release:</u>
          </p>
          <p>
            In order to release a software product, the configuration items
            must be combined, packaged with the right configuration and in most
            of the cases built into an executable program that can be installed
            by the customers. Build instructions ensure that the proper build
            steps are taken and in the correct sequence. In addition to
            building software for new releases, it is usually also necessary
            for SCM to have the capability to reproduce previous releases for
            recovery, testing, maintenance, or additional release purposes. 
          </p>
          <p>
            Software is built using particular versions of supporting tools,
            such as compilers. It might be necessary to rebuild an exact copy
            of a previously built software configuration item. In this case,
            the supporting tools and associated build instructions need to be
            under SCM control to ensure availability of the correct versions of
            the tools (i.e. not only source code evolve, but also the tools we
            use).
          </p>
          <p>
            A tool capability is useful for selecting the correct versions of
            software items for a given target environment and for automating
            the process of building the software from the selected versions and
            appropriate configuration data. For large projects with parallel
            development or distributed development environments, this tool
            capability is necessary. Most software engineering environments
            provide this capability.
          </p>
          <p>
            <u>Release Management:</u>
          </p>
          <p>
            Software release management encompasses the identification,
            packaging, and delivery of the elements of a product, for example,
            executable program, documentation, release notes, and configuration
            data.
          </p>
          <p>
            Given that product changes can occur on a continuing basis, one
            concern for release management is determining when to issue a
            release. Some aspects to take such a decision are the severity of
            the problems addressed by the release and the measurements of the
            fault densities of prior releases.
          </p>
          <p>
            The packaging task must identify which product items are to be
            delivered, and then select the correct variants of those items,
            given the intended application of the product. The information
            documenting the physical contents of a release is known as a
            <b>version description</b> document. The <b>release notes</b>
            typically describe new capabilities, known problems, and platform
            requirements necessary for proper product operation.  The package
            to be released also contains <b>installation or upgrading
            instructions</b>. The latter can be complicated because some
            current users might have versions that are several releases old. 
          </p>
          <p>
            Finally, in some cases, the release management activity might need
            to track the distribution of the product to various customers or
            target systems. An example would be a case where the supplier was
            required to notify a customer of newly reported problems. A tool
            capability is needed for supporting these release management
            functions. It is useful to have a connection with the tool
            capability supporting the issue tracker in order to map release
            contents to the issues that have been received. This tool
            capability might also maintain information on various target
            platforms and on various customer environments.
          </p>
        </section>
        <section>
          <h3>Summary</h3>
          <figure>
            <img src='images/unit3-fig3.png'>
            <figcaption>SCM Activities Relationships</figcaption>
          </figure>
        </section>
      </section>
      <section>
        <h2>SCM In Practice</h2>
        <section>
          <h3>Introduction</h3>
          <p>
            This chapter provides a set of best practices or patterns that
            should be used in SCM. There are multiple tools that can be used
            for SCM. Some of them focus on the configuration identification and
            change control, some others pay special attention to the auditing
            and accounting and some others are focused on the build and release
            part. In most of the cases different tools are required and what is
            important is all the tools are properly integrated. For instance, a
            typical situation is using a tool for managing the source code
            (e.g. Subversion or git), another one for keeping track of the
            issues, defects or releases (e.g.  Redmine or Bugzilla), another
            one for Agile Management (e.g Trello) and maybe another one for
            Continous Integration (e.g.  Travis). In such a multi-tool
            environment it is important to ensure that the changes on the
            configuration items can be linked to the issues and releases in the
            issue tracker and the agile manaagement tool.
          </p>
          <p>
            It is important to stress that there are multiple different
            paradigms for managing the source code, being the most important
            distinction whether the system is centralized or distributed.
            Linus Torvalds (who was the creator of git) gave an interesting
            speech in the Google Tech Talk event in which he compared both
            approaches [[LINUS-SCM-GOOGLE]].
          </p>
          <p>
            The patterns described in this chapter try to be generic enough so
            they can be applied in centralized and distributed systems. However
            some of them may also apply to one of them. Additionally, it is
            important to stress that usually, a distributed system can be 
            configured to work in a <a href="http://nvie.com/posts/a-successful-git-branching-model/#decentralized-but-centralized"> 
            centralized way</a>.  Additinally, during the last years 
            distributed systems have proliferated and it seems they have become
            <a href="http://www.joelonsoftware.com/items/2010/03/17.html"> the
            de-facto standard </a> for SCM.
          </p>
        </section>
        <section>
          <h3>Configuration Identification</h3>
          <section>
            <h4>Baselines</h4>
            <p>
              In previous chapter the formal definition of baseline (according
              to IEEE) has been provided. From a more "practical" point of
              view, a baseline is a consistent set of Configuration Items
              (sometimes also called tagging or labelling). A baseline is a
              reference basis for evolution and releasing. 
            </p>
            <p>
              The frequency of the baseline releasing, depends a lot on the 
              software development methodology that is used:
            </p>
            <ul>
              <li>
                Waterfall: Consists of performing the development process in a
                single time. Simplistically: determine user needs, define
                requirements, design the system, implement the system, test,
                fix, and deliver.
              </li>
              <li>
                Incremental: The incremental strategy determines user needs and
                defines the system requirements, then performs the rest of the
                development in a sequence of builds. The first build
                incorporates part of the planned features and subsequent builds
                are released until the system is complete.
              </li>
              <li>
                Evolutionary: Similar to the incremental approach but
                acknowledges that the end-user needs may not fully understood
                and that all the requirements may not be identified before
                starting the development. User needs and requirements are
                partially defined at the beginning and are further refined in
                every build.
              </li>
            </ul>
            <p>
              Obviously, working in an evolutionary manner, requires more
              frequent releases of the baselines than in a waterfall model.
              Hence, although the need of having an easy way to release is good
              in general, it's even more important in agile approaches.
            </p>
          </section>
          <section>
            <h4>Repositories</h4>
            <p>
              A <b>repository</b> is a system that stores the different
              versions of all the configuration items. The repository remembers
              every change ever written to it: every change to every
              configuration item and changes on the repository structure (such
              as the addition, deletion and rearrangement of files and
              directories).
            </p>
            <p>
              Depending on the type of approach (centralized or distributed)
              there may be a centralized repository that is considered as the
              mastercopy of the project.
              <ul>
                <li>
                  In the case of centralized systems, such as SVN, there use to
                  be only a unique repository that is the central one.  All the
                  contributors to the project have working copies of that
                  repository but they don't usually have their own repositories
                  as all the work is done in the central repository.
                </li>
                <li>
                  In distributed systems, any contributor to the project has a
                  repository he/she works with. Contributors' repositories are
                  continuously synchronized (via Pulls and Pull Requests) by
                  the users. However, in most of the cases, there is a
                  centralized repository, that is usually called upstream.  The
                  upstream could be the repository of the organization or the
                  repository of an individual with a very high reputation.
                  Sometimes, small projects start from an individual repository
                  and when they grow, they end-up in an organization
                  repository.
                </li>
              </ul>
            </p>
            <p>
              A <b>workspace</b> is a copy of the repository that developers
              have in their machines and that is used to progress on the
              software development. The changes that developers make in their
              working copies are not available to other developers until they
              have transferred the data to the repository. A working copy does
              not have all the versions of the configuration items but just
              one. However, developers have the opportunity to retrieve from
              the repository any version of any configuration item they are
              interested in.
            </p>
          </section>
          <section>
            <h4>Synchronization</h4>
            <p>
              When a developer wants to create a working copy based on the
              content of the repository, he should perform a "checkout" or
              "clone" of the repository. A checkout is the operation that
              copies all the configuration items of a repository to create a
              new working copy. The checkout operation can be requested in any
              version of the repository, but by default, it requests the latest
              one (a.k.a HEAD). The checkout does not only retrieves the
              content of the configuration items but also all their revisions,
              configuration information and branches.
            </p>
            <p>
              When a developer makes some changes in his working copy that
              wants to submit to the repository (so that other developers can
              use it) he should perform a "commit" operation (a.k.a.
              check-in).  The commit operation allows developers to contribute
              to the repository new versions of one or multiple configuration
              items.  In some systems (e.g. SVN) the commit is directly
              submitted to the repository. However, in some others, such as
              distributed systems (e.g git), a commit needs to be sent to the
              repository.  This could be achieved in different ways:
              <ul>
                <li>
                  By "pushing" the commit from the working copy to the 
                  repository. This is mostly done for the repositories owned
                  by the developer as it requires "push" permission.
                </li>
                <li>
                  By sending a "Pull Request" from a repository to another
                  one. This is useful when you don't have permissions to push
                  directly to a repository on when you want other developers
                  to review your commit before merging them (accepting and 
                  including them in the repository).
                </li>
              </ul>
            </p>
            <p>
              Once a developer has a working copy he can request at any time to
              synchronize with the latest version available in other
              repository. In centralized systems, the synchronization will be
              performed just with one repository, the central one.  However, in
              disitributed systems, developers can (and usually do) synchronize
              with multiple repositories. For instance, a typical Git
              configuration is having a remote repository name upstream
              pointing out to the project upstream repository and another one
              named "origin" that points to the developer repository. This
              operation is called "update" in centralized systems and "pull" in
              distributed ones. When this operation is requested, the
              configuration items that have been changed in the repository are
              updated in the developer working copy.
            </p>
            <p>
              SCMs will never incorporate other people's changes (update), nor
              make your own changes available to others (commit), until you
              explicitly tell it to do so.
            </p>
          </section>
          <section>
            <h4>Revisions</h4>
            <p>
              Different systems have different approaches with regards to
              configuration item versioning and identification. In Subversion
              or Git, every time a commit is performed in the repository a new
              revision of the repository is created. 
            </p>
            <ul>
              <li>
                In Subversion, a revision ID is a number that identifies a 
                version of a repository in a given moment of time and that 
                increases every time a new commit is performed. 
              </li>
              <li>
                In Git, a revision ID is a Hash string that is calculated after 
                the commit is performed, hence two consecutive revisions do not
                have consequetive revision numbers.
              </li>
            </ul>
            <p>
              As revisions are always linked to a commit, they are also called
              "commit IDs". In Git, as they are hashes, they are also referred
              to as "Hash IDs". For instance, the revision identifier of the
              first commit in the repository of these notes is
              <code>53a3797f7f406f15220955f5f6883cbae36e826f</code> as you can
              see <a
              href="https://github.com/dcoloma/software-quality/commit/53a3797f7f406f15220955f5f6883cbae36e826f">
              here</a>.
            </p>
            <p>
              A commit may include one or more configuration items with
              changes, due to that between two subsequent revisions, more than
              one item may differ from one to another.
            </p>
            <p>
              For instance, this 
              <a href="https://github.com/dcoloma/software-quality/commit/f6c4eb2899dea9ba109d243d12a322f395dad257">
              commit </a> includes changes in one file and add 2 new ones and
              it just adds a new revision on top of the <a href="https://github.com/dcoloma/software-quality/commit/7186bba611b2f06c7dc300cc6c8af5a5290fea1e"> 
              previous one</a>.
            </p>
            <p>
              It is important to stress than in modern SCM systems the
              configuration items are not identified individually but as part
              of a revision.  This is an important change with regards to other
              old systems such as CVS (Concurrent Versioning System). In order
              to identify a particular version of a configuration item, the
              revision in which that configuration item version was available
              should be referred to.
            </p>
          </section>
          <section>
            <h4>Branches and Tags</h4>
            <p>
              The master or trunk is the main line of development of the
              workspace, that is the place where the evolution of the software
              product should be done. However, having a unique development
              point in the repository is not enough in most of the software
              products.
            </p>
            <p style="background-color: #EEEEDD">
              <b>Branching: A non-software example.</b> Suppose your job 
              is to maintain a document for a division in your company, 
              a handbook of some sort. One day a different division asks 
              you for the same handbook, but with a few parts "tweaked" 
              for them, since they do things slightly differently. What 
              do you do in this situation? You do the obvious thing: you 
              make a second copy of your document, and begin maintaining 
              the two copies separately. As each department asks you to 
              make small changes, you incorporate them into one copy or 
              the other. You often want to make the same change to both 
              copies. For example, if you discover a typo in the first 
              copy, it's very likely that the same typo exists in the 
              second copy. The two documents are almost the same, after 
              all; they only differ in small, specific ways. Maintaining
              the two branches is an extra burden.
            </p>
            <p>
              As you have seen, maintaining extra branches is expensive,
              hence before creating long-life parallel branches you need 
              to think if there are altenartives to that: configuration 
              parameters, specific modules, different runtime behaviours...
            </p>
            <p>
              When a developer wants to create another development line in 
              the repository he creates a branch. A branch is a line 
              of development that exists independently of another line but 
              shares a common history if you look far enough back in time. 
              A branch always begins life as a copy of something, and moves 
              on from there, generating its own history. However, branches
              that started from a common point and diverged later on can
              merge eventually again.
            </p>
            <p>
              In the Software Development process it is sometimes convenient to
              identify a particular version, release or baseline of the
              software. This is achieved by tags. A tag is a snahpshot of the
              repository at a specific point in history. Typically people use
              this functionality to mark release points (v1.0, and so on).
              Tags are intended not to change by any means. Different SCMs have
              different strategies for implementing tags, but most of them
              implement this feature as a specific branch that does not change
              with the time.
            </p>
          </section>
        </section>
        <section>
          <h3>Best Practices and Patterns</h3>
          <section>
            <h4>Tips for branching</h4>
            <p>
              Before Git was used, branches were used with a lot of care care
              since merging in other SCM systems such as SVN was very
              difficult. Merging is the process by which two configuration
              items are combined into a new one. Depending on the amount of
              configuration items to be combined and on the type of changes
              done in them, and on the SCM system used, merging can be a very
              difficult operation.
            </p>
            <p>
              Branches are created to save some work by allowing developers to
              work in independent features in an independent manner. However,
              that may end up in some times in spending extra time doing a
              difficult merge task. The reason why Git is so successful
              nowadays is that it has simplified the way merges are done and
              hence has enabled developers to create and work on separate
              branches.
            </p>
            <p>
              However, easy merging, does not mean branches should be used
              without care. For instance, in general, overcomplicated
              structures where branches are created from branches different to
              master continuously (arborescent approach) should be avoided.
            </p>
            <p>
              Branching works better when you integrate with the origin of the
              branch as quickly as possible.
            </p>
            <p>
              <b style="color: green">
              Best Practice 1: Simplify the branching model.
              </b>
            </p>
            <p>
              Althoug branching is cheap in systems such as Git that should not
              be an excuse for creating too complex tree structures diverging
              from the master branch. Ask developers to branch from the master
              branch that is the "home codeline" in which you merge all of your
              development on, except in special circumstances. Branching always
              from master reduces merging and synchronization effort by
              requiring fewer transitive change propagations.
            </p>
            <p>
              It is also important that the expected branches are planned in
              advance and that a branch diagram is used.  Having a diagram is
              of huge help to the development as it allows at a glance to have
              a clear understanding of the different branches available and the
              relationship across them. There are many tools for getting such a
              diagram automatically.
            </p>
            <p>
              You can see below an example of such a diagram:
            </p>
            <figure>
              <img src='images/unit3-fig4.png'>
              <figcaption>Branch Diagram Example</figcaption>
            </figure>
            <p>
              <b style="color: green">
                Best Practice 2: Create specific development branches for every 
                feature you implement
              </b>
            </p>
            <p>
              Like shown in the previous diagram (branches Story A and Story
              B), for every feature to be added or for every bug you fix you
              should create a separate branch so you can work in a isolated and
              independent manner.
            </p>
            <p>
            </p>
            <p>
              <b style="color: green">
              Best Practice 3: Development branches should be short-lived.
              </b>
            </p>
            <p>
              More information about when should a development branch be merged
              will be provided in the following sections, but by having a look
              at the diagram is easy understand than the later we merge, the
              more difficult it will be as branches will have diverged more.
            </p>
            <p>
            </p>
            <p>
              <b style="color: green">
              Best Practice 4: When development branches must live for a long time, 
              relatively frequent intermediate merges should be done.
              </b>
            </p>
            <p>
              When you create a develop branch and it's going to take a long
              time before you can merge your changes to master branch, try to
              sync with master frequently so you can avoid your working branch
              to diverge from master. The more time you wait, the more
              difficult the merge would be.
            </p>
            <p>
              <b style="color: green">
              Best Practice 5: Branch Customer Releases.
              </b>
            </p>
            <p>
              When a new software version to users, the "usual" situation is
              that the team must work at least in two versions in parallel:
            </p>
            <ul>
              <li>
                A new version with new features (that is developed in the
                master branch)
              </li>
              <li>
                The released version, where typically only bugfixes should be
                added. This is usually done in a "release" branch. Please note
                that this bugfixes should also land on the master branch if
                that branch is also affected.
              </li>
            </ul>
            <p>
              Due to that, when a version is released to customers a release
              branch should be created. In this way bugfixing can be done on
              the release branch without exposing the customer to new feature
              work in progress on the mainline.
            </p>
            <p>
              The typical workflow for customer releases is:
            </p>
            <ul>
              <li>
                A release branch (e.g. v1.0) is created based on the master
                branch when the team thinks the software is ready for release
                (say, a 1.0 release). In that mmoment the content and history
                of both branches (v1.0 and master) will be the same.
              </li>
              <li>
                The team continues to work in both branches in parallel.
                Depending on the release strategy the product could be released
                at this moment to all the users, maybe to some early/beta users
                or maybe just for another testing round.
              </li>
              <li>
                In any case, if bugs are discovered in either version, fixes
                are ported back and forth as necessary. Usually, as time
                passes, only critical fixes will be landed in the release
                branch, that eventually will be frozen as new releases
                supersede it.
              </li>
              <li>
                Fixing bugs that affect both branches might be done using
                different strategies. A recommended one is to land all the code
                always in the master branch and then uplift or cherrypick those
                changes that affect the release to the release branch.
                Obviously, as time passes and branches diverge there might be
                some bugs that affect only the release branch, in that case, a
                direct merge on the release branch could be done.
              </li>
            </ul>
            <p>
              <b style="color: green">
              Best Practice 6: Branch long-lived parallel efforts.
              </b>
            </p>
            <p>
              Long lived parallel efforts that multiple people will be working
              on should be done in independent branches. Imagine you want to
              experiment with a new feature and you know that before having
              something that can be merged in the master branch a lot of time
              is going to be needed. In that case, it makes sense to create a
              specific branch (similar to the release branch) so that the team
              can work in that feature while others can check your progress.
            </p>
            <p>
              <b style="color: green">
              Best Practice 7: Be always flexible, there may be some 
              very strong reasons for breaking these "rules".
              </b>
            </p>
            <p>
              These are just a set of recommendations, but there are different
              ways to work with branches and all of them are right and wrong at
              the same time as it is impossible to have a perfect framework.
              For instance, some authors [[SUCCESSFUL-GIT-BRANCHING]] promote
              the idea of having an <i>integration</i> branch called
              <code>develop </code> and with an infinite lifetime (as master)
              as shown in the following figure:
              <figure>
                <img src='http://nvie.com/img/git-model@2x.png' width="600px" height="700px">
                <figcaption>Use of integration branches</figcaption>
              </figure>
            </p>
          </section>
          <section>
            <h4>Merging</h4>
            <p>
              When working in multiple branches, the task of combining 
              them into a single line of code (merging) is of endeavour 
              importance.
            </p>
            <p>
              When the work in the two branches to merge has no 
              overlapping configuration items (no configuration item has 
              been modified in both), the merging task is easier. However,
              although no conflicts should occur during the merging, it 
              does not mean that the result of the merge is going to be 
              good enough. Let's have a look at a non-software example:
            </p>
            <p style="background-color: #EEEEDD">
              Imagine you are Dr. Frankenstein and you want to build a human
              being. You have a development team composed by two developers, in
              order to avoid problems when merging their contributions you ask
              one to develop the legs and another one to develop the arms. When
              both have finished their task the merging is done with no
              problem, i.e. 2 arms and 2 legs are assembled in the body.
              However, imagine what happens if the left legs are twice longer
              than the righ leg: the merge worked OK but the result is a
              monster.
            <p> 
              In conclusion, a merge without conflicts can also be a bad merge.
            </p>
            <p>
              When some configuration items are modified in both branches, the
              merging task is not immediate as manual intervention is required
              to suggest how to solve the conflicts that result of modiying
              separately the same file.  A conflict in a merge is said to occur
              when two configuration items have been modified with divergent
              changes.
            </p>
            <p>
              <b style="color: green">
              Best Practice 8: Developers making the changes should be
              the ones responsible to fix the conflicts.
              </b>
            </p>
            <p>
              They are the ones who know better the code they have modified so
              the best way to prevent a Frankestein to be created is asking
              them to ensure the merge work leads to a fully functional result.
            </p>
          </section>
          <section>
            <h4>Working Copies vs. Repository</h4>
            <p>
              Software is developed in teams because concurrent work is needed.
              Nonetheless, the more people in your team, the more potential for
              conflicting changes.
            </p>
            <p>
              In order to minimize the number of conflicts and facilitate the
              work of the team it is important to encourage team members to:
            </p>
            <ul>
              <li>
                Work in features as small as possible.
              </li>
              <li>
                Create the Pull Request as soon as possible but only if they
                work properly to avoid others to suffer chained problems.
              </li>
            </ul>
            <p>
              But finding the right balance for this last issue (check 
              stable code but soon) is usually difficult.
            </p>
            <p>
              Working from a highly tested stable line is not always an option
              when new features are being developed, otherwise the frequency of
              the commits would not be as high as it is needed. However,
              although not being highly tested, at least it is expected that
              the code that is retrieved from the repository has a reasonable
              quality. In order to get to a good trade-off it is important to
              require developers to perform simple procedures before submitting
              code to the codeline, such as a preliminary build, and some level
              of testing
            </p>
            <p>
              The good trade-off is having a development line stable enough for
              the work it needs to do. Do not aim for a perfect active
              development line, but rather for a mainline that is usable and
              active enough for your needs.
            </p>
            <p>
              An active development line will have frequent changes, some well
              tested checkpoints that are guaranteed to be "good", and other
              points in the codeline are likely to be good enough for someone
              to do development on the tip of the line.
            </p>
            <p>
              Some aspects that should be considered by developers are:
            </p>
            <ul>
              <li>
                Work in your development branch and test your changes on it.
              </li>
              <li>
                Before creating a Pull Request run Regression Test to make sure
                that you have not broken anything.
              </li>
              <li>
                Ask for a code review if needed and repeat the previous steps
                iteratively depending on the review feedback.
              </li>
              <li>
                After the review has been positively completed.
              </li>
              <li>
                An Automated Integration Build should be done before accepting
                the Pull Request or righ after accepting it.
              </li>
            </ul>
            <p>
              Many of the concepts we have just described are very related with
              the concept of Continuous Integration and will be explained in
              the next chapter.
            </p>
            <p>
              <b style="color: green">
              Best Practice 9: Before pushing a contribution (Pull Request or
              Direct Push), ensure that the latest version of the repository is
              available in the working copy.
              </b>
            </p>
            <p>
              <b style="color: green">
              Best Practice 10: Think globally by building locally.  Ensure the
              system builds before pushing.
              </b>
            </p>
            <p>
              The only way to truly test that any change is 100% compatible
              with the system is through the centralized integration build.
              However, if we do not test it in our working copy, it is highly
              likely our changes break the build and disturbs the work of other
              developers. Before making a submission to source control,
              developers should build the system using a Private System Build
              that is similar to the centralized build. A private system build
              does take time, but this is time spent by only one person rather
              than each member of the team should there be a problem.
            </p>
            <p>
              <b style="color: green">
              Best Practice 11: Code can be committed with bugs if
              they are known and do not introduce regressions.
              </b>
            </p>
            <p>
              Do not wait to have the final version of your software.
              Sometimes it's better to have the code available in the master
              branch soon (even with known bugs) than waiting extra time to fix
              and land the code later (when more conflicts can happen and less
              time will be spent in testing by other developers)
            </p>
          </section>
        </section>
        <section>
          <h3>Continuous Integration and Building</h3>
          <p>
            Since many people are making changes in the repository, it is 
            impossible for a developer to be 100% sure that the entire 
            system builds correctly after they integrate their changes in 
            the repository even if they create a local build before and
            extensively test it.
          </p>
          <p>
            Continuous Integration (CI) is a software development practice
            where members of a team commit their work frequently, leading to
            multiple integrations per day. Each integration is verified by an
            automated build (including test) to detect integration errors as
            quickly as possible. This approach leads to significantly reduced
            integration problems and allows a team to develop cohesive software
            more rapidly. 
          </p>
          <section>
            <h4> Build Process </h4>
            <p>
              Building is the process of getting the sources turned into a
              running system. This can often be a complicated process involving
              compilation, moving files around, generating configuration files,
              loading schemas into the databases, and so on. However this
              process can (and as a resullt should) be automated.
            </p>
            <p>
              Automated environments for builds are a common feature of
              systems. The Unix world has had <i>make</i> for decades, the Java
              community developed <i>Ant</i>, the .NET community has had
              <i>Nant</i> and now has MSBuild, for node.js and Javascript we
              have now Grunt, Gulp and many more...  What is important,
              regardless of the Programming Language and framework is to make
              sure you can build and launch your system using these scripts
              using a single command.  A common mistake is not to include
              everything in the automated build. This should be avoided by all
              means as anyone should be able to bring in a virgin machine,
              check the sources out of the repository, issue a single command,
              and have a running system on their machine.
            </p>
            <p>
              <b style="color: green">
              Best Practice 12: The full build process should be automated
              and include everything that is required.
              </b>
            </p>
            <p>
              A big build often takes time, and with CI we want to detect
              issues as soon as practical so optimizing build time is key to
              meet this target as in some cases, building a complete sytem
              might take hours. In order to save time, good build tools
              analyzes what needs to be changed as part of the process and
              perform only the required actions. The common way to do this is
              to check the dates of the source and object files and only
              compile if the source date is later.  One of the trickiest
              aspects of building in an incremental way is managing
              depedencies: if one object file changes those that depend on it
              may also need to be rebuilt.
            </p>
            <p>
              <b style="color: green">
              Best Practice 13: Try to minimize the time required to generate
              the build.
              </b>
            </p>
            <p>
              As explained before, multiple tools existe in order to perform
              the build, that depend, for instance, in the OS of the host
              machine for the repository: Make, Ant, Grunt ... There are also
              some cross-platform tools that allow to create a custom
              centralized build process in any OS and SCM system.
            </p>
            <p>
              The build process should take into account that different
              <i>targets</i> or <i>configurations</i> may be supported. For
              instance, desktop software must be usually built for Windows,
              OS-X and Linux so the build system should be able to create
              builds for all these systems.
            </p>
            <p>
              Having a central build ensures the software is always built in
              the same manner. The software build process should be
              reproducible, so the same build could be created as many times as
              needed and as close as possible to the final product build.
            </p>
            <p>
              <b style="color: green">
              Best Practice 14: Have a centrazlied and reproducible build system.
              </b>
            </p>
          </section>
          <section>
            <h4> Self Testing Builds </h4>
            <p>
              A build may be successfully created and it may run, but that
              doesn't mean it does the right thing. Modern statically typed
              languages can catch many bugs, but far more are not detected by
              the compilers.
            </p>
            <p>
              A good way to catch bugs quickly and efficiently is to include
              automated tests in the build process. Testing isn't perfect, of
              course, but it can catch a lot of bugs.
            </p>
            <p>
              The good news is that the rise of TDD has lead to a wide
              availability of automated testing frameworks and tools such as
              the XUnit family, Selenium and plenty of others.
            </p>
            <p>
              Of course the self-testing is not going to find everything as
              tests do not prove the absence of bugs but they help to detect
              bugs early and hence minimize their impact. As in the case of the
              build generation, passing the tests takes time and we should try
              to optimize the testing process (in terms of performance and the
              amount of relevant tests to be passed).
            </p>
          </section>
          <section>
            <h4> Every commit creates a build </h4>
            <p>
              As we are encouraging developers to commit frequently, ensuring
              the mainline stays in a healthy state is an important but
              difficult task.
            </p>
            <p>
              The best way to ensure that is by having regular builds on an
              integration machine and only if this integration build succeeds
              should the commit be considered to be done. Since the developer
              who commits is responsible for this, that developer needs to
              monitor the mainline build so they can fix it if it breaks. Your
              work is not completely done until the mainline build is finished
              and has passed all the self-tests.
            </p>
            <p>
              <b style="color: green">
              Best Practice 15: Create a new build with every commit.
              </b>
            </p>
            <p>
              A continuous integration server acts as a monitor to the
              repository. Every time a commit against the repository is done
              the server automatically checks out the sources onto the
              integration machine, initiates a build, passes the self-test and
              notifies the committer of the result of the build and tests.
            </p>
            <p>
              The best way to monitor the repository is by using tools such as
              <i>hooks</i>. Hooks are a set of actions that could be configured
              in Git to be done every time a user commits a file to the
              repository. A hook could be pre-commit or post-commit, depending
              on whether the hook is executed before the commit or after the
              commit is done respectively. 
            </p>
            <p>
              Pre-commit hooks may be used for intance to reject commits that
              have some errors (e.g.  with the changes the system does not
              build), in that case if the error is detected the commit is
              rejected and notified to the user doing the commit. 
            </p>
            <p>
              If post-commit hooks are used, it's the developer or the
              repository administrator the one responsible to perform
              corrective actions in case the build is not properly generated or
              the tests do not pass.
            </p>
          </section>
          <section>
            <h4> Fix Broken mainline immediately </h4>
            <p>
              A key part of doing a continuous build is that if the mainline
              build fails, it needs to be fixed right away. The whole point of
              working with CI is that you're always developing on a known
              stable base.
            </p>
            <p>
              It's not a terrible thing for the mainline build to break,
              although if it's happening all the time it suggests people aren't
              being careful enough about updating and building locally before a
              commit. When the mainline build does break, however, it's
              important that it gets fixed fast. Usually, the fastest way to
              fix the build is to revert the latest commit from the mainline,
              taking the system back to the last-known good build, this is
              sometimes known as <i> backing out </i> the commit. Unless the
              cause for the breakage is immediately obvious and can be fixed
              really fast, developers should just revert the mainline and debug
              the problem on the working copy leaving the repository clean.
            </p>
            <p>
              <b style="color: green">
              Best Practice 16: Back out any commit that breaks the master
              build immediately.
              </b>
            </p>
          </section>
          <section>
            <h4> Visibility of CI </h4>
            <p>
              Continuous Integration is all about communication, so it is
              important to ensure that everyone can easily see the state of the
              system and the changes that have been made to it.
            </p>
            <p>
              SCM systems such as Git provide us the information about the
              changes done but Git as such does not communicate the state of
              the mainline build. The ideal solution should be providing a web
              site (either integrated in the SCM or as a standalone one) that
              will show you if there's a build in progress and what was the
              state of the last mainline build.  An example of such a system is
              Travis [[TRAVIS-CI]]. 
            </p>
          </section>
        </section>
        <section>
          <h3>Releasing</h3>
          <p>
            A release is a version of the product that is made available to its
            intended customers. External releases are published to end-users
            whereas internal releases are made available only to developers.
            The releases are identified by release numbers which are totally
            independent from the SCM version numbers.
          </p>
          <p>
            Releases can be also classified in full or partial releases,
            depending on whether it requires a complete installation or not
            respectively. Partial releases require a previous full release to
            be installed.
          </p>
          <p>
            Release creation involves collecting all files and documentation
            required to create ystem release. Configuration descriptions have
            to be written for different hardware and installation scripts have
            to be written. The specific release must be documented to record
            exactly what files were used to create it. This allows it to be
            re-created if necessary
          </p>
          <p>
            Release planning is concerned with when to issue a system version
            as a release. The following factors should be taken into account
            for defining a release strategy:
          </p>
          <ul>
            <li>
              Technical Quality of the System: If serious system faults are
              reported which affect the way in which many customers use the
              system, it may be necessary to issue a fault repair release.
              However, minor system faults may be repaired by issuing patches
              (often distributed over the Internet) that can be applied to the
              current release of the system.
            </li>
            <li>
              Platform Changes: You may have to create a new release of a
              software application when a new version of the operating system
              platform is released.
            </li>
            <li>
              Lehman's fifth law: This suggests that the increment of
              functionality that is included in each release is approximately
              constant. Therefore, if there has been a system release with
              significant new functionality, then it may have to be followed by
              a repair release. 
            </li>
            <li>
              Competition: A new system release may be necessary because a
              competing product is available.
            </li>
            <li>
              Marketing Requirements: The marketing departm ent of an
              organisation may have made a commitment for releases to be
              available at a particular date.
            </li>
            <li>
              Customer Change Proposals: For customised systems, customers may
              have made and paid for a specific set of system change proposals
              and they expect a system release as soon as these have been
              implemented
            </li>
          </ul>
        </section>
        <section>
          <h3>Controlling Changes</h3>
          <p>
            There is a need to submit continuously changes to the repository.
            The reasons for checking-in changes are multiple:
          </p>
          <ul>
            <li>
              Defects: A defect has been detected and needs to be fixed.
            </li>
            <li>
              New Features: New features must be added to the software.
            </li>
            <li>
              Improvements: A functionality already existing can be improved.
            </li>
          </ul>
          <p>
            Even in a continuous integration model, it is important to have the
            possibility to control the changes that have been done to the
            configuration items in the repository. Control in this context does
            not mean <i>approval</i> but traceability. I.e. It is not always
            needed that someone approves a change, but what is needed is that
            it is possible to identify for every change committed the reasons
            for it.  Lack of control in the process leads to project failures,
            confusion and chaos. Using a good control mechanism enables
            communication, sharing data and efficiency.
          </p>
          <p>
            Depending on the codeline in which the changes are done, the level
            of information required and the flow that should be followed for
            implementing and approving it should be different.
          </p>
          <p>
            For instance, changes in master should be encouraged rather than
            discouraged. In order to do so, developers should be free to commit
            their changes to the repository if:
          </p>
          <ul>
            <li>
              The change is inline with the product backlog or requirements he
              was developing for.
            </li>
            <li> 
              He raises an issue on the issue tracker describing the reason for
              the changes and the changes themselves.
            </li>
            <li>
              When he performs the commit, he indicates in the commit
              information the related issue (the one he created).
            </li>
            <li>
              After the commit is done, the is sue is either automatically or
              manually marked as resolved.
            </li>
          </ul>
          <p>
            Additionally, anybody in the development team should be free to
            raise additional issues that can be assigned to anybody within the
            team. Giving freedom to the development team (within some limits)
            is usually a good idea.
          </p>
          <p>
            If the changes are going to be applied in a branch that was created
            based on a commercial release, the process usually follows a more
            strict control. For instance:
          </p>
          <ul>
            <li>
              The developer creates an issue in the issue tracker, this kind of
              issues are usually called Change Request (CR). A CR typically has
              the following information:
              <ul>
                <li>
                  Project name, date, requestor, and priority.
                </li>
                <li>
                  Description of the problem.
                </li>
                <li>
                  Affected configuration items, branches and releases.
                </li>
                <li>
                  Suggested fix.
                </li>
                <li>
                  Severity.
                </li>
                <li>
                  Log files, screen shots...
                </li>
              </ul>
            </li>
            <li>
              The CR is then analysed by the configuration control board that
              decides what to do, approve it or reject it.
              <ul>
                <li>
                  In case it is rejected the initiator could review and create
                  a new version (depending on the reasons for the rejection).
                </li>
                <li>
                  In case it is approved, it is assigned to a developer who
                  will implement it and add it to the repository. 
                </li>
              </ul>
            </li>
          </ul>
          <p>
            Regardless of how "controlled" is the implementation of changes in
            the repository, the system used should allow some features such as:
          </p>
          <ul>
            <li>
              Identify which changes have been implemented between two
              different baselines.
            </li>
            <li>
              Check the status of a particular issue, defect or CR for the
              different platforms and releases.
            </li>
            <li>
              Check if the resolution of a problem can be merged in a branch.
            </li>
          </ul>
          <p>
            With respect to the tools, there are multiple tools that allow
            issue tracking. There are commercial ones such as Jira or free ones
            such as Bugzilla or Redmine. The later is a very powerful as it is
            quite flexible and can be integrated with the most popular source
            code management tools such as git and svn. Additionally, other
            Agile management tools can be used such as Trello.
          </p>
          <p>
            Additionally, the Source Code Management tools should also have
            adequate authorization mechanism to ensure the traceability of the
            changes, i.e. identified who made any particular change in the
            repository. For instance, SVN offers and authentication mechanism
            and allows to use others such as LDAP. The important thing is that
            SVN identifies which user has done which commit in order to trace
            back a change to the author. SVN also allows the definition of
            permissions in per-branch or per-configuration item basis, access
            to some branches may be only allowed to some users.
          </p>
        </section>
      </section>
    </section>
    <section>
      <h1> Testing </h1>
      <section>
        <h2>Introduction</h2>
        <p>
          The purpose of software testing is to ensure that the software
          systems work as expected when their target customers and users use
          them.
        </p>
        <p>
          The basic idea of testing involves the <b>execution</b> of software
          and the <b>observation</b> of its behavior or outcome. If a deviation
          from the expected behavior is observed, the execution record is
          analyzed to find and fix the bug(s) that caused the failure. 
        </p>
        <p>
          Testing could be hence defined as a controlled experimentation
          through program execution in a controlled environment before product
          release.  Therefore testing fulfills two primary purposes:
        </p>
        <ul>
          <li>Demonstrate quality or proper behaviour.</li>
          <li>Detect and fix problems.</li>
        </ul>
      </section>
      <section>
        <h2>Types of Testing</h2>
        <p>
          Testing could be categorized in different types based on different
          criteria.
        </p>
        <section>
          <h3>Criteria 1: Functional and Structural Testing</h3>
          <p>
            The main difference between functional and structural testing is
            the knowledge (or lack of knowledge) about the software internals
            and hence the related focus:
          </p>
          <ul>
            <li>
              Functional testing considers that there is no information about software internals or implementation details. Hence, this type of testing is usally knonw as "Black-Box". This affects both the test defintion as well as the test execution.
            </li>
            <li>
              Structural testing considers that the information about the internals of the software is known and should be used for both test definition an execution. This type of testing are usually known as "white-box" testing.
            </li>
          </ul>
          <section>
            <h4>Functional Testing</h4>
            <p>
              When a "black-box" approach is followed, the <b>definition</b> of the test-cases that should be executed does not take into account the structure of the software. The <b>execution</b> of the test cases focuses on the observation of the program external behavior during execution. It checks what is the external output of software based in some inputs.
            </p>
            <p>
              There are different levels in which Black-Box testing can be performed:
            </p>
            <ul>
              <li>
                At the most detailed level, individual program elements can be tested such as functions, methods.
              </li>
              <li>
                At the intermediate level, various program elements or program components may be treated as an interconnected group, and tested accordingly.
              </li>
              <li>
                At the most abstract level, the whole software systems can be treated as a "blackbox", while we focus on the functions or input-output relations instead of the internal implementation.
              </li>
            </ul>
          </section>
          <section>
            <h4>Structural Testing</h4>
            <p>
              Structural testing requires the knowledge of the internals of the software implementation. It verifies the correct implementation of internal units, such as program statements, data structures, blocks... and the relations among them.
            </p>
            <p>
              <b>Defining test cases</b> in a structural way, consists in using the knowledge of the software implementation in order to reduce the number of test cases to be passed. With the tendency of defining the tests cases (even automating them as we will see in the TDD section) before the software is implemented. This kind of techniques are not that useful nowadays.
            </p>
            <p>
              When executing tests in a structural way, as the key focus is the connection between execution behavior and internal units, the observation of the results is not enough and additional software tools are also required. For instance, debuggers, that help us in tracing through program executions. By doing so, the tester can see if a specific statement has been executed, and if the result or behavior is expected.
            </p>
            <p>
              This kind of testing is usually very complex due to the use of these tools. However, its key advantage is that once a problem is detected it is also localized (the failure leads directly to the bug). Because of this complexity, this testing is only done when the root of a bug discovered via functional testing cannot be found or in very late stages of the project.
            </p>
          </section>
        </section>
        <section>
          <h3>Criteria 2: Coverage vs. Usage based testing</h3>
          <p>
            One of the most important decisions that should be taken by the QA (and the whole software and product) team is to decide when to stop testing.
          </p>
          <p>
            Obviously, an easy (but wrong decision) would be stopping based on the resources, e.g. stop when you run out of time or of money or time. As such a decision would lead to quality problems, we need to find a quality-based criteria to decide when our product has passed enough tests. In order to identify when the product has reached the quality goals, there are different points of view:
          </p>
          <ul>
            <li>
              Measure the quality directly by in-use metrics. The issue is that this approach requires actual customer usage.
            </li>
            <li>
              Measure the quality indirectly through the execution of a set of tests that provide a good test coverage.
            </li>
          </ul>
          <section>
            <h4>Usage-Based Statistical Testing (UBST()</h4>
            <p>
              Actual customer usage of software products can be viewed as a form of usage-based testing. Measuring directly the quality in a real environment is the most accurate way to identify if the software quality targets have been achieved. 
            </p>
            <p>
              The so-called beta test makes use of continuous iterations, through controlled software release so that these beta customers help software development and organizations improve their software quality.
            </p>
            <p>
              In usage-based statistical testing (UBST), the overall testing environment resembles the actual operational environment for the software product in the field, and the overall testing sequence, as represented by the orderly execution of specific test cases in a test suite, resembles the usage scenarios, sequences, and patterns of actual software usage by the target customers.
            </p>
            <p>
              Although very useful, as this approach is helpful to detect not only bugs but also other type of prblems, this approach could be dangerous if it is not use with care as it could damage the software vendor's reputation, for instance if the produce released as beta has very bad quality.  Due to that, it is recommended to use this approach mainly for final software stages or when the team feels very confident about software stability.
            </p>
          </section>
          <section>
            <h4>Coverage-Based Testing (CBT)</h4>
            <p>
              Most traditional testing techniques, either Black or White Box, use various forms of test coverage as the stopping criteria.  This means that the testing process is stopped when a set of tests are executed successfully in the software. In this case, the key aspects are identifying which is the required test coverage and what <i>executed successfully</i> means.
            </p>
            <p>
              With respect to coverage, in the case of Functional Testing, it could consist on completing a checklist of major functions based on product specification (system requirements), it could consist in having a minimal number of Test Cases per User Story, etc...
            </p>
            <p>
              In case of Structural Testing, it could consist on completing a checklist of all the product components or all the statements of the software.
            </p>
            <p>
              With respect to what </i>executed successfully</i> means, as we know, it is impossible to have a 100% bug-free software, so in some cases, it is assumed that some bugs could be allowed in order to decide the product to be finished, obviously, this depends on the type of product, the criticality of the bugs, the timescales...
            </p>
          </section>
          <section>
            <h4>Comparing CBT with UBST</h4>
            <p>
              The key differences that distinguish CBT from UBST are the perspective and the related stopping criteria.
            </p>
            <p>
              With regards to the perspective, UBST views the objects of testing from a user's perspective and focuses on the usage scenarios, sequences, patterns, and associated frequencies or probabilities. On the other hand, CBT views the objects from a developer's perspective and focuses covering functional or implementation units and related entities.
            </p>
            <p>
              With regards to the stopping criteria, UBST uses product in use metrics as the exit criterion. CBT uses coverage goals - that are supposed to be an approximations of in-use goals - as the exit criterion.
            </p>
          </section>
        </section>
        <section>
          <h3>Criteria 3: Test Target</h3>
          <p>
            Tests are frequently grouped by where they are added in the software development process, or by the target (element) to be tested.
          </p>
          <section>
            <h4>Unit Testing</h4>
            <p>
              Unit testing refers to tests that verify the functionality of a specific section of code, usually at the function level. In an object-oriented environment, this is usually at the class level, and the minimal unit tests include the constructors and destructors.
            </p>
            <p>
              These types of tests are usually written by developers as they work on code (white-box style), to ensure that the specific function is working as expected. One function might have multiple tests, to catch corner cases or other branches in the code. Unit testing alone cannot verify the functionality of a piece of software, but rather is used to assure that the building blocks the software uses work independently of each other.
            </p>
            <p>
              Unit testing is also called component testing.
            </p>
          </section>
          <section>
            <h4>Integration Testing</h4>
            <p>
              Integration testing is any type of software testing that seeks to verify the interfaces between components against a software design. Software components may be integrated in an iterative way or all together ("big bang"). Normally the former is considered a better practice since it allows interface issues to be localised more quickly and fixed.
            </p>
            <p>
              Integration testing works to expose defects in the interfaces and interaction between integrated components (modules). Progressively larger groups of tested software components corresponding to elements of the architectural design are integrated and tested until the software works as a system.
            </p>
          </section>
          <section>
            <h4>System Testing</h4>
            <p>
              System testing tests a completely integrated system to verify that it meets its requirements.
            </p>
          </section>
          <section>
            <h4>System Integration Testing</h4>
            <p>
              System integration testing verifies that a system is integrated to any external or third-party systems defined in the system requirements.
            </p>
          </section>
        </section>
        <section>
          <h3>Criteria 4: Objectives of testing</h3>
          <p>
            Although testing has a common set of goals, the targets of testing can be very different. Some examples of type testing based on their goals are listed in this section.
          </p>
          <section>
            <h4>Regression Testing</h4>
            <p>
              Regression testing focuses on finding defects after a major code change has occurred. Specifically, it seeks to uncover software regressions, or old bugs that have come back. Such regressions occur whenever software functionality that was previously working correctly stops working as intended. Typically, regressions occur as an unintended consequence of program changes, when the newly developed part of the software collides with the previously existing code. Common methods of regression testing include re-running previously run tests and checking whether previously fixed faults have re-emerged. The depth of testing depends on the phase in the release process and the risk of the added features. They can either be complete, for changes added late in the release or deemed to be risky, to very shallow, consisting of positive tests on each feature, if the changes are early in the release or deemed to be of low risk.
            </p>
          </section>
          <section>
            <h4>Acceptance Testing</h4>
            <p>
              Acceptance testing can mean one of two things:
            </p>
            <ul>
              <li>
                A smoke test is  used as an acceptance test prior to introducing a new build to the main testing process, i.e. before integration or regression.
              </li>
              <li>
                Acceptance test  ing is performed by the customer, often in their lab environment on their own hardware, is known as user acceptance testing (UAT). Acceptance testing may be performed as part of the hand-off process between any two phases of development.[citation needed]
              </li>
            </ul>
          </section>
          <section>
            <h4>Alpha Testing</h4>
            <p>
              Alpha testing is simulated or actual operational testing by potential users/customers or an independent test team at the developers' site. Alpha testing is often employed for off-the-shelf software as a form of internal acceptance testing, before the software goes to beta testing.
            </p> 
          </section>
          <section>
            <h4>Beta Testing</h4>
            <p>
              Beta testing comes after alpha testing and can be considered a form of external user acceptance testing. Versions of the software, known as beta versions, are released to a limited audience outside of the programming team. The software is released to groups of people so that further testing can ensure the product has few faults or bugs. Sometimes, beta versions are made available to the open public to increase the feedback field to a maximal number of future users.A
            </p>
          </section>
        </section>
      </section>
      <section>
        <h2>Test Activities</h2>
        <p>
          As in many other software related activities, the typical plan,
          execute and assess flow is also used in testing as depicted in the
          figure below. 
        </p>
        <figure>
          <img src='images/unit4-fig1.png'>
          <figcaption>
            Test Activities as part of Software develpoment process
           </figcaption>
        </figure>
        <section>
          <h3>Testing Planning and Preparation</h3>
          <p>
            Most of the key decisions about testing are made during this stage.
            During this phase an overall testing strategy is fixed by making
            the following decisions:
          <p>
          <ul>
            <li>
               Overall objectives and goals, which can be refined into specific
               goals for specific testing. Some specific goals include
               reliability for usage-based statistical testing or coverage for
               various traditional testing techniques.
            </li>
            <li>
               Objects to be tested and the specific focus: Functional testing
               views the software product as a black-box and focuses on testing
               the external functional behavior; while structural testing views
               the software product or component as a (transparent) whitebox
               and focuses on testing the internal implementation details.
            </li>
          </ul>
          <p>
            As soon as the first models are being generated (for example, usage
            models, system models, architectural models, etc), they can be used
            to generate test cases: A test case is a collection of entities and
            related information that allows a test to be executed or a test run
            to be performed. The collection of individual test cases that will
            be run in a test sequence until some stopping criteria are
            satisfied is called a test suite. IEEE Standard 610 (1990) defines
            test case as follows:
          </p>
          <ul>
            <li>
              A set of test inputs, execution conditions, and ex  pected
              results developed for a particular objective, such as to exercise
              a particular program path or to verify compliance with a specific
              requirement.
            </li>
            <li>
              (IEEE Std 829-1983) Documentation specifying inputs, predicted
              results, and a set of execution conditions for a test item.
            </li>
          </ul>
          <p>
            According to Ron Patton: "Test cases are the specific inputs that
            you'll try and the procedures that you'll follow when you test the
            software."
          </p>
          <p>
            From a more practical point of view, a test case is composed by:
          </p>
          <ul>
            <li>
              Preconditions that should be established before the test is
              conducted.
            </li>
            <li>
              Clear sequence of actions and input data that constitutes the
              test sequence.
            </li>
            <li>
              Expected Result.
            </li>
          </ul>
          <p>
            On the other hand, a test run, is a dynamic unit of specific test
            activities in the overall testing sequence on a selected testing
            object. Each time a static test case is invoked, an individual
            dynamic test run is created.
          </p>
          <p>
            One aspect that should be considered when planning the test cases
            is the sequencing of the individual test cases and the switch-over
            from one test run to another. Several concerns affect the specific
            test procedure to be used, including:
          </p>
          <ul>
            <li>
              Dependencies among individual test cases. For instance, does a
              test case require the execution of another test case before?
            </li>
            <li>
              Defect detection related sequencing. Many problems can only be
              effectively detected after others have been discovered and fixed.
            </li>
            <li>
              Natural grouping of test cases, such as by functional and
              structural areas or by usage frequencies, can also be used for
              test sequencing and to manage parallel testing.
            </li>
          </ul>
        </section>
        <section>
          <h3>Testing Execution</h3>
          <p>
            The most important activities related with test execution are:
          </p>
          <ul>
            <li>
              Allocating test time and resources.
            </li>
            <li>
              Invoking and running tests, and collecting execution
              information and measurements.
            </li>
            <li>
              Checking testing results and identifying system failures.
            </li>
          </ul>
          <p>
            One of the critical aspects in order to fulfill the objectives of
            testing is checking if the result of the test run is successful or
            not. In order to do so, it must be possible to observe the results
            of the test and determine whether the expected result was achieved
            or not.
          </p>
          <p>
            Is enough with observing the results? In some situations, such as
            in object-oriented software, the execution of a test-run may have
            affected the state of an object. That state might also affect in
            the future the software that has been tested. Due to that, in some
            situations it is helpful to examine the state of some objects
            before and after a test is conducted. The reason for that is that
            only a small percentage of the overall functionality of an object
            can be observed via the return values.
          </p>
          <p>
            This may be in conflict with using a "black-box" testing approach,
            in which only events observable outside can be used to verify the
            results of a test-run. However, the meaning of observed may be
            different for different software projects: outside a method? An
            object? The whole software?
          </p>
          <p>
            When a failure is observed, it needs to be recorded and tracked
            until their resolution. In order to allow developers to trace back
            the failure to the bug causing it, it is important that detailed
            information about failure observations and the related activities
            is registered.
          </p>
          <p>
            But not only failures must be registered, successful executions
            also need to be recorded as it is very important for regression
            testing.
          </p>
          <p>
            In general for every test-run the following information should be
            gathered:  
          </p>
          <ul>
            <li>
              Run identification.
            </li>
            <li>
              Timing. Start and end time.
            </li>
            <li>
              Tester. The tester who attempted the te  st run.
            </li>
            <li>
              Transactions. Transactions handled by the test run.
            </li>
            <li>
              Results. Result of the test run.
            </li>
          </ul>
        </section>
        <section>
          <h3>Testing Analysis and Follow-up</h3>
          <p>
            The results of the testing activities (i.e. the measurement data
            collected during test execution), together with other data about
            the testing and the overall environment provide valuable feedback
            to test execution and other testing and development activities.
          </p>
          <p>
            Obviously, as a consequence of testing, there are some direct
            follow-up activities:
          </p>
          <ul>
            <li>
              Defect fixing. The development team must repair detected defects.
            </li>
            <li>
              Management decisions, such as product release and transition from
              one development phase or sub-phase to another. For instance,
              given the results of testing I can decid if my product mature
              enough for being published.
            </li>
          </ul>
          <p>
            In order to fix an issue, it is important to follow these steps: 
          </p>
          <ol>
            <li>
              Understanding the problem by studying the execution record.
            </li>
            <li>
              Being able to recreate the same problem scenario and observe the
              same problem. 
            </li>
            <li>
              Problem diagnosis too examine what kind of problem it is, where,
              when and possible causes.
            </li>
            <li>
              Fault locating, to identify the exact location(s) of fault(s).
            </li>
            <li>
              Defect fixing, to fix the located fault(s) by adding, removing,
              or correcting certain parts of the code. Sometimes, design and
              requirement changes could also be triggered or propagated from
              the above changes due to logical linkage among the different
              software components.
            </li>
          </ol>
          <p>
            In order to take appropriate management decisions, some analysis
            can be performed on the overall testing results:
          </p>
          <ol>
            <li>
              Reliability analysis for usage-based testing, which can be used
              to assess current product reliability. Sometimes, low reliability
              areas can be identified for focused testing and reliability
              improvement. In case of this type of testing, it is also very
              helpful to detect problems non linked to defects, using the PUM
              metric.
            </li>
            <li>
              Coverage analysis for coverage-based testing, which can be used
              as a surrogate for reliability and used as the stopping
              criterion.
            </li>
            <li>
              Overall defect analysis, which can be used to examine defect
              distribution and to identify high-defect areas for focused
              remedial actions.
            </li>
          </ol>
        </section>
      </section>
      <section>
        <h2>Test Cases and Test Coverage definition</h2>
        <p>
          Exhaustive testing is the execution of every possible test case.
          Rarely can we do exhaustive testing. Even simple systems have too
          many possible test cases. For example, a program with two integer
          inputs on a machine with a 32-bit word would have 264 possible test
          cases. Thus, testing is always executing a very small percentage of
          the possible test cases.
        </p>
        <p>
          Two basic concerns in software testing definition have been already
          introduced (1) what test cases to use (test case selection) and (2)
          how many test cases are necessary (stopping criterion). Test case
          selection can be based on either the specifications (functional), the
          structure of the code (structural), the flow of data (data flow), or
          random selection of test cases. Test case selection can be viewed as
          an attempt to space the test cases throughout the input space. Some
          areas in the domain may be especially error-prone and may need extra
          attention. It has been also mentioned that the stopping criterion can
          be based on a coverage criterion, such as executing <i>N</i> test
          cases in each subdomain, or the stopping criterion can be based on a
          behavior criteria, such as testing until an error rate is less than a
          threshold <i>x</i>.
        </p>
        <p>
          A program can be thought of as a mapping from a domain space to an
          answer space or range. Given an input, which is a point in the domain
          space, the program produces an output, which is a point in the range.
          Similarly, the specification of the program is a map from a domain
          space to an answer space.
        </p>
        <p>
          Please also remember, that a specification is essential to software
          testing.  Correctness in software is defined as the program mapping
          being the same as the specification mapping. A good saying to
          remember is "a program without a specification is always correct". A
          program without a specification cannot be tested against a
          specification, and the program does what it does and does not violate
          its specification.
        </p>
        <section>
          <h3>Test Coverage criterion</h3>
          <p>
            A test coverage criterion is a rule about how to select tests and
            when to stop testing. One basic issue in testing research is how to
            compare the effectiveness of different test coverage criteria. The
            standard approach is to use the subsumes relationship.
          </p>
          <section>
            <h4>Subsumes</h4>
            <p>
              A test criterion A subsumes test coverage criterion B if any test
              set that satisfies criterion A also satisfies criterion B. This
              means that the test coverage criterion A somehow includes the
              criterion B. For example, if one test coverage criterion required
              every statement to be executed and another criterion required
              every statement to be executed and some additional tests, then
              the second criterion would subsume the first criterion.
            </p>
            <p>
              Researchers have identified subsumes relationships among most of
              the conventional criteria. However, although subsumes is a
              characteristic that is used for comparing test criterian, it does
              not measure the relative effectiveness of two criteria. This is
              because most criteria do specify how a set of test cases will be
              chosen. Picking the minimal set of test cases to satisfy a
              criterion is not as effective as choosing good test cases until
              the criterion is met. Thus, a good set of test cases that satisfy
              a "weaker" criterion may be much better than a poorly chosen set
              that satisfy a "stronger" criterion.
            </p>
          </section>
          <section>
            <h4>Functional Testing</h4>
            <p>
              In functional testing, the specification of the software is used
              to identify subdomains that should be tested. One of the first
              steps is to generate a test case for every distinct type of
              output of the program. For example, every error message should be
              generated.  Next, all special cases should have a test case.
              Tricky situations should be tested. Common mistakes and
              misconceptions should be tested. The result should be a set of
              test cases that will thoroughly test the program when it is
              implemented. This set of test cases may also help clarify to the
              developer some of the expected behavior of the proposed software.
            </p>
            <p>
              In the book "The Art of Software Testing", Glenford Myers poses
              the following functional testing problem: Develop a good set of
              test cases for a program that accepts three numbers, <code>a, b,
              c,</code> interprets those numbers as the lengths of the sides of
              a triangle, and outputs the type of the triangle. Myers reports
              that in his experience most software developers will not respond
              with a good test set.
            </p>
            <p>
              An approach to define the test cases for this classic triangle
              problem, is dividing the domain space into three subdomains, one
              for each different type of triangle that we will consider:
              scalene (no sides equal), isosceles (two sides equal), and
              equilateral (all sides equal). We can also identify two error
              situations: a subdomain with bad inputs and a subdomain where the
              sides of those lengths would not form a triangle. Additionally,
              since the order of the sides is not specified, all combinations
              should be tried.  Finally, each test case needs to specify the
              value of the output.  Following table shows a possible solution.
            </p>
            <table width=100%>
              <caption>Test Cases for Triangle Problem - Functional Testing Approach</caption>
              <thead>
                <tr>
                  <th>Subdomain</th>
                  <th>Test Description</th>
                  <th>Test Case</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="3">Scalene</td>
                  <td>Increasing Size</td>
                  <td>(3,4,5) -> Scalene </td>
                </tr>
                <tr>
                  <td>Decreasing Size</td>
                  <td>(5,4,3) -> Scalene </td>
                </tr>
                <tr>
                  <td>Largest is second</td>
                <td>(4,5,3) -> Scalene </td>
                </tr>
                <tr>
                  <td rowspan="6">Isosceles</td>
                  <td>a=b & other side larger</td>
                  <td>(5,5,8) -> Isosceles </td>
                </tr>
                <tr>
                  <td>a=c & other side larger</td>
                  <td>(5,8,5) -> Isosceles </td>
                </tr>
                <tr>
                  <td>b=c & other side larger</td>
                  <td>(8,5,5) -> Isosceles </td>
                </tr>
                <tr>
                  <td>a=b & other side smaller</td>
                  <td>(8,8,5) -> Isosceles </td>
                </tr>
                <tr>
                  <td>a=c & other side smaller</td>
                  <td>(8,5,8) -> Isosceles </td>
                </tr>
                <tr>
                  <td>b=c & other side smaller</td>
                  <td>(5,8,8) -> Isosceles </td>
                </tr>
                <tr>
                  <td>Equilateral</td>
                  <td>a=b=c</td>
                  <td>(5,5,5) -> Equilateral </td>
                </tr>
                <tr>
                  <td rowspan="3">Not a triangle</td>
                  <td>Largest first</td>
                  <td>(6,4,2) -> Not a triangle </td>
                </tr>
                <tr>
                  <td>Largest second</td>
                  <td>(4,6,2) -> Not a triangle </td>
                </tr>
                <tr>
                  <td>Largest third</td>
                  <td>(1,2,3) -> Not a triangle </td>
                </tr>
                <tr>
                  <td rowspan="3">Bad Inputs</td>
                  <td> One bad input</td>
                  <td>(-1,4,2) -> Bad Inputs</td>
                </tr>
                <tr>
                  <td>Two bad inputs</td>
                  <td>(-1,2,0) -> Bad Inputs</td>
                </tr>
                <tr>
                  <td>Three Bad Inputs</td>
                  <td>(0,0,0) -> Bad Inputs</td>
                </tr>
              </tbody>
            </table>
            <p>
              This list of subdomains could be increased to distinguish other
              subdomains that might be considered significant. For example, in
              scalene subdomains, there are actually six different orderings,
              but the placement of the largest might be the most significant
              based on possible mistakes in programming.
            </p>
            <p>
              Note that one test case in each subdomain is usually considered
              minimal but acceptable.
            </p>
          </section>
          <section>
            <h4>Test Matrices</h4>
            <p>
              A way to formalize this identification of subdomains is to build
              a matrix using the conditions that we can identify from the
              specification and then to systematically identify all
              combinations of these conditions as being true or false.
            </p>
            <p>
              The conditions in the triangle problem might be:
            </p>
            <ol>
              <li>
                a=b or a=c or b=c
              </li>
              <li>
                a=b and b=c
              </li>
              <li>
                a >= b + c OR b >= a + c OR c >= a + b
              </li>
              <li>
                a <= 0 or b <= 0 or c <= 0 (equals to a>0 and b>0 and c>0). 
              </li>
            </ol>
            <p>
              These four conditions can be put on the rows of a matrix. The
              columns of the matrix will each be a subdomain. For each
              subdomain, a T will be placed in each row whose condition is true
              and an F when the condition is false. All valid combinations of T
              and F will be used. If there are four conditions, there may be
              2^4 = 8 subdomains (columns). Not all the combinations are
              possible as some of the conditions depend on others to be true or
              false. Additional rows will be used for defining possible values
              of a, b, and c and for the expected output for each subdomain
              test case.
            </p>
            <p>
              Next table shows an example of this matrix:
            </p>
            <table width=100%>
              <caption>
                Test Cases for Triangle Problem - Test Matrices - 
                Functional Testing
              </caption>
              <thead>
                <tr>
                  <th>Conditions</th>
                  <th>1</th>
                  <th>2</th>
                  <th>3</th>
                  <th>4</th>
                  <th>5</th>
                  <th>6</th>
                  <th>7</th>
                  <th>8</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>a=b or a=c or b=c</td>
                  <td>T</td>
                  <td>T</td>
                  <td>T</td>
                  <td>T</td>
                  <td>T</td>
                  <td>F</td>
                  <td>F</td>
                  <td>F</td>
                </tr>
                <tr>
                  <td>a=b and b=c</td>
                  <td>T</td>
                  <td>T</td>
                  <td>F</td>
                  <td>F</td>
                  <td>F</td>
                  <td>F</td>
                  <td>F</td>
                  <td>F</td>
                </tr>
                <tr>
                  <td>a>=b+c or b>=a+c or c>=a+b</td>
                  <td>T</td>
                  <td>F</td>
                  <td>T</td>
                  <td>T</td>
                  <td>F</td>
                  <td>T</td>
                  <td>T</td>
                  <td>F</td>
                </tr>
                <tr>
                  <td>a<=0 or b<=0 or c<=0</td>
                  <td>T</td>
                  <td>F</td>
                  <td>T</td>
                  <td>F</td>
                  <td>F</td>
                  <td>T</td>
                  <td>F</td>
                  <td>F</td>
                </tr>
                <tr>
                  <td>Sample Test Case</td>
                  <td>0,0,0</td>
                  <td>3,3,3</td>
                  <td>0,4,0</td>
                  <td>3,8,3</td>
                  <td>5,8,5</td>
                  <td>0,5,6</td>
                  <td>3,4,8</td>
                  <td>3,4,5</td>
                </tr>
                <tr>
                  <td>Expected Output</td>
                  <td>Bad inputs</td>
                  <td>Equilateral</td>
                  <td>Bad inputs</td>
                  <td>Not a triangle</td>
                  <td>Isosceles</td>
                  <td>Bad inputs</td>
                  <td>Not a triangle</td>
                  <td>Scalene</td>
                </tr>
              </tbody>
            </table>
          </section>
          <section>
            <h4>Structural Testing</h4>
            <p>
              Structural testing coverage is based on the structure of the
              source code. The simplest structural testing criterion is every
              statement coverage, often called C0 coverage.
            </p>
            <section>
              <h5>C0 - Every State Coverage</h5>
              <p>
                This criterion is that every statement of the source code
                should be executed by some test case. The normal approach to
                achieving C0 coverage is to select test cases until a coverage
                tool indicates that all statements in the code have been
                executed.
              </p>
              <p>
                The pseudocode in the following table implements the triangle
                problem. The table also shows which lines are executed by which
                test cases. Note that the first three statements (A, B, and C)
                can be considered parts of the same node.
              </p>
              <table width=100%>
                <caption>
                  Test Cases for Triangle Problem - C0 - Structural Testing
                </caption>
                <thead>
                  <tr>
                    <th>Node</th>
                    <th>Source</th>
                    <th>3,4,5</th>
                    <th>3,5,3</th>
                    <th>0,1,0</th>
                    <th>4,4,4</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>A</td>
                    <td>read a,b,c</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>B</td>
                    <td>type="scalene"</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>C</td>
                    <td>if((a==b) || (b==c) || (a==c)))</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>D</td>
                    <td>type="isosceles"</td>
                    <td></td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>E</td>
                    <td>if((a==b)&&(b==c))</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>F</td>
                    <td>type="equilateral"</td>
                    <td></td>
                    <td></td>
                    <td></td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>G</td>
                    <td>if((a>=b+c) || (b>=a+c) || (c>=a+b)))</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>H</td>
                    <td>type="not a triangle"</td>
                    <td></td>
                    <td></td>
                    <td>*</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>I</td>
                    <td>if((a<=0>) || (b<=0>) || (c<=0>)))</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>J</td>
                    <td>type="bad inputs"</td>
                    <td></td>
                    <td></td>
                    <td>*</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>K</td>
                    <td>print type</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                </tbody>
              </table>
              <p>
                By the fourth test case, every statement has been executed.
                This set of test cases is not the smallest set that would cover
                every statement. However, finding the smallest test set would
                often not find a good test set.
              </p>
            </section>
            <section>
              <h5>C1 - Every Branch Testign</h5>
              <p>
                A more thorough test criterion is every-branch testing, which
                is often called C1 test coverage. In this criterion, the goal
                is to go both ways out of every decision.
              </p>
              <p>
                If we model the program of previous table as a control flow
                graph, this coverage criterion requires covering every arc in
                the following control flow diagram.
              </p>
              <p>
                Next table shows the test cases identified with this criteria.
              </p>
              <figure>
                <img src='images/unit4-fig2.png'>
                <figcaption>Control Flow Graph</figcaption>
              </figure>
              <table width=100%>
                <caption>
                  Test Cases for Triangle Problem - C1 Approach - Structural Testing
                </caption>
                <thead>
                  <tr>
                    <th>Arcs</th>
                    <th>Test Case: (3,4,5)</th>
                    <th>Test Case: (3,5,3)</th>
                    <th>Test Case: (0,1,0)</th>
                    <th>Test Case: (4,4,4)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>ABC-D</td>
                    <td></td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>ABC-E</td>
                    <td>*</td>
                    <td></td>
                    <td></td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>D-E</td>
                    <td></td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>E-F</td>
                    <td></td>
                    <td></td>
                    <td></td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>E-G</td>
                    <td>*</td>
                    <td>*</td>
                    <td>*</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>F-G</td>
                    <td></td>
                    <td></td>
                    <td></td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>G-H</td>
                    <td></td>
                    <td></td>
                    <td>*</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>G-I</td>
                    <td>*</td>
                    <td>*</td>
                    <td></td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>H-I</td>
                    <td></td>
                    <td></td>
                    <td>*</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>I-J</td>
                    <td></td>
                    <td></td>
                    <td>*</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>I-K</td>
                    <td>*</td>
                    <td>*</td>
                    <td></td>
                    <td>*</td>
                  </tr>
                  <tr>
                    <td>J-K</td>
                    <td></td>
                    <td></td>
                    <td>*</td>
                    <td></td>
                  </tr>
                </tbody>
              </table>
            </section>
            <section>
              <h5>Every Path Testign</h5>
              <p>
                  Even more thorough is the every-path testing criterion. A
                  path u is a unique sequence of program nodes that are
                  executed by a test case. In the testing matrix above, there
                  were eight subdomains. Each of these just happens to be a
                  path. In that example, there are sixteen different
                  combinations of T and F.  However, eight of those
                  combinations are infeasible paths.  That is, there is no test
                  case that could have that combination of T and F for the
                  decisions in the program. It can be exceedingly hard to
                  determine if a path is infeasible or if it is just hard to
                  find a test case that executes that path.  
              </p> 
              <p>
                  Most programs with loops will have an infinite number of
                  paths. In general, every-path testing is not reasonable.
              </p>
              <p>
                  Next table shows the eight feasible paths in the triangle
                  pseudocode as well as the test cases required for testing all
                  of them.
              </p>
              <table width=100%>
                <caption>
                  Test Cases for Triangle Problem - Every Path Approach -
                  Structural Testing
                </caption>
                <thead>
                  <tr>
                    <th>Path</th>
                    <th>T/F</th>
                    <th>Test Case</th>
                    <th>Output</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>ABCEGIK</td>
                    <td>FFFF</td>
                    <td>3,4,5</td>
                    <td>Scalene</td>
                  </tr>
                  <tr>
                    <td>ABCEGHIK</td>
                    <td>FFTF</td>
                    <td>3,4,8</td>
                    <td>Not a triangle</td>
                  </tr>
                  <tr>
                  <td>ABCEGIJK</td>
                    <td>FFTT</td>
                    <td>0,5,6</td>
                    <td>Bad inputs</td>
                  </tr>
                  <tr>
                    <td>ABCDEGIK</td>
                    <td>TFFF</td>
                    <td>5,8,5</td>
                    <td>Isosceles</td>
                  </tr>
                  <tr>
                    <td>ABCDEGHIK</td>
                    <td>TFTF</td>
                    <td>3,8,3</td>
                    <td>Not a triangle</td>
                  </tr>
                  <tr>
                    <td>ABCDEGHIJK</td>
                    <td>TFTT</td>
                    <td>0,4,0</td>
                    <td>Bad Inputs</td>
                  </tr>
                  <tr>
                    <td>ABCDEFGIK</td>
                    <td>TTFF</td>
                    <td>3,3,3</td>
                    <td>Equilateral</td>
                  </tr>
                  <tr>
                    <td>ABCDEFGHIJK</td>
                    <td>TTTT</td>
                    <td>0,0,0</td>
                    <td>Bad Inputs</td>
                  </tr>
                </tbody>
              </table>
            </section>
            <section>
              <h5>Multiple Condition Coverage</h5>
              <p>
                A multiple-condition testing criterion requires that each
                primitive relation condition is evaluated both true and false.
                Additionally, all combinations of T/F for the primitive
                relations in a condition must be tried. Note that lazy
                evaluation of expressions will eliminate some combinations.
                For example, in an "and" of two primitive relations, the second
                will not be evaluated if the first one is false.
              </p>
              <p>
                In the pseudocode for the triangle example, there are multiple
                conditions in each decision statement as displayed in the
                tables below. Primitives that are not executed because of lazy
                evaluation are shown with an 'X'.
              </p>
              <table width=100%>
                <caption>
                  Test Cases for Triangle Problem - Multiple Condition:
                  Condition if(a==b||b==c||a==c) - Structural Testing
                </caption>
                <thead>
                  <tr>
                    <th>Combination</th>
                    <th>Possible Test Case</th>
                    <th>Branch</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>TXX</td>
                    <td>3,3,4</td>
                    <td>ABC-D</td>
                  </tr>
                  <tr>
                    <td>FTX</td>
                    <td>4,3,3</td>
                    <td>ABC-D</td>
                  </tr>
                  <tr>
                    <td>FFT</td>
                    <td>3,4,3</td>
                    <td>ABC-D</td>
                  </tr>
                  <tr>
                    <td>FFF</td>
                    <td>3,4,5</td>
                    <td>ABC-E</td>
                  </tr>
                </tbody>
              </table>
              <table width=100%>
                <caption>
                  Test Cases for Triangle Problem - Multiple Condition:
                  Condition (a==b&&b==c) - Structural Testing
                </caption>
                <thead>
                  <tr>
                    <th>Combination</th>
                    <th>Possible Test Case</th>
                    <th>Branch</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>TT</td>
                    <td>3,3,3</td>
                    <td>E-F</td>
                    </tr>
                  <tr>
                    <td>TF</td>
                    <td>3,3,4</td>
                    <td>E-G</td>
                  </tr>
                  <tr>
                    <td>FX</td>
                    <td>4,3,3</td>
                    <td>E-G</td>
                  </tr>
                </tbody>
              </table>

              <table width=100%>
                <caption>
                  Test Cases for Triangle Problem - Multiple Condition:
                  Condition (a>=b+c||b>=a+c||c>=a+b) - Structural Testing
                </caption>
                <thead>
                  <tr>
                    <th>Combination</th>
                    <th>Possible Test Case</th>
                    <th>Branch</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>TXX</td>
                    <td>8,4,3</td>
                    <td>G-H</td>
                  </tr>
                  <tr>
                    <td>FTX</td>
                    <td>4,8,3</td>
                    <td>G-H</td>
                  </tr>
                  <tr>
                    <td>FFT</td>
                    <td>4,3,8</td>
                    <td>G-H</td>
                  </tr>
                  <tr>
                    <td>FFF</td>
                    <td>3,3,3</td>
                    <td>G-I</td>
                  </tr>
                </tbody>
              </table>
              <table width=100%>
                <caption>
                  Test Cases for Triangle Problem - Multiple Condition:
                  Condition (a<=0||b<=0||c<=0) - Structural Testing
                </caption>
                <thead>
                  <tr>
                    <th>Combination</th>
                    <th>Possible Test Case</th>
                    <th>Branch</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>TXX</td>
                    <td>0,4,5</td>
                    <td>I-J</td>
                  </tr>
                  <tr>
                    <td>FTX</td>
                    <td>4,-2,-2</td>
                    <td>I-J</td>
                  </tr>
                  <tr>
                    <td>FFT</td>
                    <td>5,-4,3</td>
                    <td>I-J</td>
                  </tr>
                  <tr>
                    <td>FFF</td>
                    <td>3,3,3</td>
                    <td>I-K</td>
                  </tr>
                </tbody>
              </table>
            </section>
            <section>
              <h5>Subdomain Testing</h5>
              <p>
                Subdomain testing is the idea of partitioning the input domain
                into mutually exclusive subdomains and requiring an equal
                number of test cases from each subdomain. This was basically
                the idea behind the test matrix. Subdomain testing is more
                general in that it does not restrict how the subdomains are
                selected. Generally, if there is a good reason for picking the
                subdomains, then they may be useful for testing. Additionally,
                the subdomains from other approaches might be subdivided into
                smaller subdomains. Theoretical work has shown that subdividing
                subdomains is only effective if it tends to isolate potential
                errors into individual subdomains.
              </p>
              <p>
                Every-statement coverage and every-branch coverage are not
                subdomain tests. There are not mutually exclusive subdomains
                related to the execution of different statements or branches.
                Every-path coverage is a subdomain coverage, since the
                subdomain of test cases that execute a particular path through
                a program is mutually exclusive with the subdomain for any
                other path.
              </p>
              <p>
                For the triangle problem, we might start with a subdomain for
                each output. These might be further subdivided into new
                subdomains based on whether the largest or the bad element is
                in the first position, second position, or third position (when
                appropriate).  Next table shows the subdomains and test cases
                for every subdomain. 
              </p>
              <table width=100%>
                <caption>
                  Test Cases for Triangle Problem - Subdomain Testing
                </caption>
                <thead>
                  <tr>
                    <th>Subdomain</th>
                    <th>Possible Test Case</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Eauilateral</td>
                    <td>3,3,3</td>
                  </tr>
                  <tr>
                    <td>Isosceles first largest</td>
                    <td>8,5,5</td>
                  </tr>
                  <tr>
                    <td>Isosceles second largest</td>
                    <td>5,8,5</td>
                  </tr>
                  <tr>
                    <td>Isosceles third largest</td>
                    <td>5,5,8</td>
                  </tr>
                  <tr>
                    <td>Scalene first largest</td>
                    <td>5,4,3</td>
                  </tr>
                  <tr>
                    <td>Scalene second largest</td>
                    <td>3,5,4</td>
                  </tr>
                  <tr>
                    <td>Scalene third largest</td>
                    <td>3,4,5</td>
                  </tr>
                  <tr>
                    <td>Not a triangle first largest</td>
                    <td>8,3,3</td>
                  </tr>
                  <tr>
                    <td>Not a triangle second largest</td>
                    <td>3,8,4</td>
                  </tr>
                  <tr>
                    <td>Not a triangle third largest</td>
                    <td>4,3,8</td>
                  </tr>
                  <tr>
                    <td>Bad Inputs first largest</td>
                    <td>4,3,0</td>
                  </tr>
                  <tr>
                    <td>Bad Inputs second largest</td>
                    <td>3,4,0</td>
                  </tr>
                  <tr>
                    <td>Bad Inputs third largest</td>
                    <td>-1,4,5</td>
                  </tr>
                </tbody>
              </table>
            </section>
          </section>
        </section>
        <section>
          <h3>Data Flow Testing</h3>
          <p>
            Data flow testing is testing based on the flow of data through a 
            program. Data flows from where it is defined to where it is used.
          </p>
          <p>
            A definition of data, or DEF, is when a value is assigned to a 
            variable. For example, with respect to a variable x, nodes 
            containing statements such as <code>input x</code> and 
            <code>x = 2</code> would both be defiing nodes. 
          </p>
          <p>
            Usage nodes (USE) refer to situations in which a variable is used
            by the software. Two main kinds of use have been identified:
          </p>
          <ul>
            <li>
              The computation use, or C-USE, is when the variable is used in a
              computation (e.g. it appears on the right-hand side of an
              assignment statement) such as in <code>print x</code> or <code>a
              = 2+x</code>. A C-USE is said to occur on the assignment 
              statement.
            </li>
            <li>
              The predicate use, or P-USE, is when the variable is used in the
              condition of a decision statement (e.g. <code>if x>6</code>).  A
              P-USE is assigned to both branches out of the decision statement. 
            </li>
          </ul>
          <p>
            There are also three other types of usage node, which are all, in
            effect, subclass of the C-USE type:
          </p>
          <ul>
            <li>
              O-use: output use - the value of the variable is output to the
              external environment (for instance, the screen or a printer
              <code>print(x)</code>).
            </li>
            <li>
              L-use: location use - the value of the variable is used, for
              instance, to determine which position of an array is used (e.g.
              <code>a[x]</code>).
            </Li>
            <li>
              I-use: iteration use - the value of the variable is used to
              control the number of iterations made by a loop (for example:
              <code>for (int i = 0;i <= x; i++)</code>)
            </li>
          </ul>
          <p>
            A definition free path, or def-free, is a path from a definition of
            a variable to a use of that variable that does not include another
            definition of the variable.
          </p>
          <p>
            Next figure depicts the Control Flow Graph of Triangle Problem and
            is annotated with the definitions and uses of the variables type,
            a, b, and c.
          </p>
          <figure>
            <img src='images/unit4-fig2b.png'>
            <figcaption>Control Flow Graph</figcaption>
          </figure>
          <p>
            More details about the control-flow procedure and examples can be
            found in the paper "Data Flow Testing - CS-399: Advanced Topics in
            Computer Science, Mark New (321917)"
          </p>
        </section>
        <section>
          <h3>Random Testing</h3>
          <p>
            Random testing is accomplished by randomly selecting the test
            cases. This approach has the advantage of being fast and it also
            eliminates biases of the testers. Additionally, statistical
            inference is easier when the tests are selected randomly. Often the
            tests are selected randomly from an operational profile.
          </p>
          <p>
            For example, for the triangle problem, we could use a random number
            generator and group each successive set of three numbers as a test
            set. We would have the additional work of determining the expected
            output. One problem with this is that the chance of ever generating
            an equilateral test case would be very small. If it actually
            happened, we would probably start questioning our pseudo random
            number generator.
          </p>
          <section>
            <h4>Operational Profile</h4>
            <p>
              Testing in the development environment is often very different
              than execution in the operational environment. One way to make
              these two more similar is to have a specification of the types
              and the probabilities that those types will be encountered in the
              normal operations. This specification is called an operational
              profile. By drawing the test cases from the operational profile,
              the tester will have more confidence that the behavior of the
              program during testing is more predictive of how it will behave
              during operation.
            </p>
            <p>
              A possible operational profile for the triangle problem is shown
              in next table:
            </p>
            <table width=100%>
              <caption>Test Cases for Triangle Problem - Multiple Condition: Condition 1 - Structural Testing</caption>
              <thead>
                <tr>
                  <th>#</th>
                  <th>Description</th>
                  <th>Probability</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>1</td>
                  <td>Equilateral</td>
                  <td>20%</td>
                </tr>
                <tr>
                  <td>2</td>
                  <td>Isosceles - Obtuse</td>
                  <td>10%</td>
                </tr>
                <tr>
                  <td>3</td>
                  <td>Isosceles - Right</td>
                  <td>20%</td>
                </tr>
                <tr>
                  <td>4</td>
                  <td>Scalene - Right</td>
                  <td>10%</td>
                </tr>
                <tr>
                  <td>5</td>
                  <td>Scalene - All Acute</td>
                  <td>25%</td>
                </tr>
                <tr>
                  <td>6</td>
                  <td>Scalene - Obtuse Angle</td>
                  <td>15%</td>
                </tr>
              </tbody>
            </table>
          </section>
          <section>
            <h4>Statistical Inference from testing</h4>
            <p>
              If random testing has been done by randomly selecting test cases
              from an operational profile, then the behavior of the software
              during testing should be the same as its behavior in the
              operational environment.
            </p>
            <p>
              For instance, if we selected 1000 test cases randomly using an
              operational profile and found three errors, we could predict that
              this software would have an error rate of less than three
              failures per 1000 executions in the operational environment.
            </p>
          </section>
        </section>
        <section>
          <h3>Boundary Testing</h3>
          <p>
            Often errors happen at boundaries between domains. In source code,
            decision statements determine the boundaries. If a decision
            statement is written as <code>x<1</code> instead of
            <code>x<0</code>, the boundary has shifted. If a decision is
            written <code>x=<1</code>, then the boundary, <code>x=1</code>, is
            in the true subdomain. In the terminology of boundary testing, we
            say that the on tests are in the true domain and the off tests are
            values of x greater than 1 and are in the false domain.
          </p>
          <p>
            If a decision is written <code>x<1</code> instead of
            <code>x=<1</code>, then the boundary, x=1, is now in the false
            subdomain instead of in the true subdomain.
          </p>
          <p>
            Boundary testing is aimed at ensuring that the actual boundary
            between two subdomains is as close as possible to the specified
            boundary. Thus, test cases are selected on the boundary and off the
            boundary as close as reasonable to the boundary. The standard
            boundary test is to do two on tests as far apart as possible and
            one off test close to the middle of the boundary.
          </p>
          <p>
            Next figure shows a simple boundary. The arrow indicates that the
            on tests of the boundary are in the subdomain below the boundary.
            The two on tests are at the ends of the boundary and the off test
            is just above the boundary halfway along the boundary.
          </p>
          <figure>
            <img src='images/unit4-fig3.png'>
            <figcaption>Boundary Conditions</figcaption>
          </figure>
          <p>
            In the triangle example, for the primitive conditions, a>=b+c or
            b>=a+c or c >= a + b, we could consider the boundary. Since these
            are in three variables as a plane in 3D space. The on tests would
            be two (or more) widely separated tests that have equality - for
            example, (8,1,7) and (8,7,1). These are both true. The off test
            would be in the other domain (false) and would be near the middle -
            for example, (7.9, 4,4).
          </p>
        </section>
      </section>
      <section>
        <h2>Test Automation</h2>
        <p>
          For large software systems, the test coverage required to ensure a
          proper quality may be huge. Due to that, it is impossible to run all
          the tests manually and mechanisms to automate the tests are used.
          However, it should be noted that in many cases (if not all) a full
          automation of the procedure is impossible due to the need of manual
          intervention or analysis of the results. Hence, when automation is
          used, it should be assessed in which areas of the software
          functionality is going to lead to the higher benefits.
        </p>
        <p>
          Among the three major test activities, preparation, execution, and 
          follow-up, execution is a prime candidate for automation.
        </p>
        <p>
          The testing that programmers do is generally called unit testing 
          (aka Object Testing):
        </p>
        <ul>
          <li>
            It starts by testing individual objects in isolation.
          </li>
          <li>
            The idea of Object Testing is to take a single object and test it
            by itself, without worrying about the role it plays in the
            surrounding system.
          </li>
          <li>
            If you build each object to behave cor rectly according to a
            defined specification then when you piece those objects together
            there is a much greater change that the system will behave the way
            you want.
          </li>
        </ul>
        <p>
          The rhythm of an Object Test is similar to any other test:
        </p>
        <ul>
          <li>
            Prepare: create the object.
          </li>
          <li>
            Execute: invoke a method.
          </li>
          <li>
            Assess: check the result of the invocation.
          </li>
        </ul>
        <p>
          In order to facilitate this process, a number of frameworks have been
          built for different programming languages such as:
        </p>
        <ul>
          <li>
            QUnit: Unit Testing Framework for JavaScript.
          </li>
          <li>
            JUnit: Unit Testing Framework for Java.
          </li>
          <li>
           NUnit: Unit Testing Framework for .NET.
          </li>
        </ul>
        <p>
          And many more to over 30 programming languages and environments.
        </p>
        <p>
          Although the implementations are different for every environment, the
          concepts are the same in any of these frameworks that are known in
          the abstract as xUnit.
        </p>
        <p>
          Most software developers just want to write code; testing is simply a
          necessary evil in our line of work. Automated tests provide a nice
          safety net so that we can write code more quickly, but we will run
          the automated tests frequently only if they are really easy to run.
        </p>
        <p>
          What makes tests easy to run? Four specific goals answer this
          question:
        </p>
        <ul>
          <li>
            They must be Fully Automated Tests so they can be run with out any
            effort.
          </li>
          <li>
            They must be Self-Checking Tests so they can detect and report any
            errors without manual inspection.
          </li>
          <li>
            They must be Repeatable Tests so they can be run multiple times
            with the same result.
          </li>
          <li>
            Ideally, each test should be an Independent Test that can be run by
            itself.
          </li>
        </ul>
        <p>
          With these four goals satisfied, one click of a button (or keyboard
          shortcut) is all it should take to get the valuable feedback the
          tests provide. Let's look at these goals in a bit more detail.
        </p>
        <section>
          <h3>Goal1: Fully Automated Tests</h3>
          <p>
            A test that can be run without any Manual Intervention is a Fully
            Automated Test. Satisfying this criterion is a prerequisite to
            meeting many of the other goals. Yes, it is possible to write Fully
            Automated Tests that don't check the results and that can be run
            only once. The main() program that runs the code and directs print
            statements to the console is a good example of such a test. 
          </p>
        </section>
        <section>
          <h3>Goal2: Self-checking Tests</h3>
          <p>
            A Self-Checking Test has encoded within it everything that the test
            needs to verify that the expected outcome is correct. The Test
            Runner "calls us" only when a test did not pass; as a consequence,
            a clean test run requires zero manual effort. Many members of the
            xUnit family provide a Graphical Test Runner (see Test Runner) that
            uses a green bar to signal that everything is OK; a red bar
            indicates that a test has failed and warrants further
            investigation.A
          </p>
        </section>
        <section>
          <h3>Goal3: Repeatable Tests</h3>
          <p>
            A Repeatable Test can be run many times in a row and will produce
            exactly the same results without any human intervention between
            runs.  Unrepeatable Tests increase the overhead of running tests
            significantly.  This outcome is very undesirable because we want
            all developers to be able to run the tests very frequently, as
            often as after every "save".  Unrepeatable Tests can be run only
            once before whoever is running the tests must perform a Manual
            Intervention. Just as bad are non determinstic Tests that produce
            different results at different times; they force us to spend lots
            of time chasing down failing tests.  The power of the red bar
            diminishes significantly when we see it regularly without good
            reason. All too soon, we begin ignoring the red bar, assuming that
            it will go away if we wait long enough. Once this happens, we have
            lost a lot of the value of our automated tests, because the
            feedback indicating that we have introduced a bug and should fix it
            right away disappears. The longer we wait, the more effort it takes
            to find the source of the failing test.
          </p>
          <p>
            Tests that run only in memory and that use only local variables or
            fields are usually repeatable without us expending any additional
            effort. Unrepeatable Tests usually come about because we are using
            a Shared Fixture of some sort. In such a case, we must ensure that
            our tests are <i>self-cleaning</i> as well. When cleaning is
            necessary, the most consistent and foolproof strategy is to use a
            generic Automated Teardown mechanism. Although it is possible to
            write teardown code for each test, this approach can result in
            Erratic Tests when it is not implemented correctly in every test.
          </p>
        </section>
        <section>
          <h3>Goal4: Simplicity</h3>
          <p>
            Coding is a fundamentally difficult activity because we must keep a
            lot of information in our heads as we work. When we are writing
            tests, we should stay focused on testing rather than coding of the
            tests. This means that tests must be simple - simple to read and
            simple to write. They need to be simple to read and understand
            because testing the automated tests themselves is a complicated
            endeavor. They can be tested properly only by introducing the very
            bugs that they are intended to detect; this is hard to do in an
            automated way so it is usually done only once (if at all), when the
            test is first written. For these reasons, we need to rely on our
            eyes to catch any problems that creep into the tests, and that
            means we must keep the tests simple enough to read quickly.
          </p>
          <p>
            Of course, if we are changing the behavior of part of the system,
            we should expect a small number of tests to be affected by our
            modifications. We want to Minimize Test Overlap so that only a few
            tests are affected by any one change. Contrary to popular opinion,
            having more tests pass through the same code doesn't improve the
            quality of the code if most of the tests do exactly the same thing.
          </p>
          <p>
            Tests become complicated for two reasons:
          </p>
          <ul>
            <li>
              We try to verify too much functionality in a single test.
            </li>
            <li>
              Too large an "expressiveness gap" separates the test scripting
              language (e.g. Java) and the before/after relationships between
              domain concepts that we are trying to express in the test.
            </li>
          </ul>
          <p>
            The tests should be small and test one thing at a time. Keeping
            tests simple is particularly important during test-driven
            development because code is written to pass one test at a time and
            we want each test to introduce only one new bit of behavior.  We
            should strive to Verify One Condition per Test by creating a
            separate Test Method for each unique combination of pre-test state
            and input. 
          </p>
          <p>
            The major exception to the mandate to keep Test Methods short
            occurs with customer tests that express real usage scenarios of the
            application. Such extended tests offer a useful way to document how
            a potential user of the software would go about using it; if these
            interactions involve long sequences of steps, the Test Methods
            should reflect this reality.
          </p>
        </section>
        <section>
          <h3>Goal5: Maintainability</h3>
          <p>
            Tests should be maintained along with the rest of the software.
            Testware must be much easier to maintain that production software
            as otherwise:
          </p>
          <ul>
            <li>It will slow the development down.</li>
            <li>It will get left behind.</li>
            <li>It will have less value.</li>
            <li>Developers will go back to manual testing.</li>
          </ul>
        </section>
      </section>
      <section>
        <h2>Test Driven Development</h2>
        <section>
          <h3>What is TDD</h3>
          <p>
            The steps of test first design (TFD) are overviewed in the UML
            activity diagram of next figure. The first step is to quickly add a
            test, basically just enough code to fail. Next you run your tests,
            often the complete test suite although for sake of speed you may
            decide to run only a subset, to ensure that the new test does in
            fact fail. You then update your functional code to make it pass the
            new tests. The fourth step is to run your tests again.  If they
            fail you need to update your functional code and retest.  Once the
            tests pass the next step is to start over (you may first need to
            refactor any duplication out of your design as needed, which is
            what converts TFD into TDD).
          </p>
          <figure>
            <img src='images/unit4-fig4.png'>
            <figcaption>TFD Steps</figcaption>
          </figure>
          <p>
            Dean Leffingwell describes TDD with this simple formula:
          </p>
          <p>
            TDD = Refactoring + TFD.
          </p>
          <p>
            TDD completely turns traditional development around. When you first
            go to implement a new feature, the first question that you ask is
            whether the existing design is the best design possible that
            enables you to implement that functionality. If so, you proceed via
            a TFD approach. If not, you refactor it locally to change the
            portion of the design affected by the new feature, enabling you to
            add that feature as easy as possible. As a result you will always
            be improving the quality of your design, thereby making it easier
            to work with in the future.
          </p>
          <p>
            Instead of writing functional code first and then your testing code
            as an afterthought, if you write it at all, you instead write your
            test code before your functional code.  Furthermore, you do so in
            very small steps - one test and a small bit of corresponding
            functional code at a time. A programmer taking a TDD approach
            refuses to write a new function until there is first a test that
            fails because that function isn't present. In fact, they refuse to
            add even a single line of code until a test exists for it. Once the
            test is in place they then do the work required to ensure that the
            test suite now passes (your new code may break several existing
            tests as well as the new one). This sounds simple in principle, but
            when you are first learning to take a TDD approach it proves
            require great discipline because it is easy to "slip" and write
            functional code without first writing a new test.
          </p>
          <p>
            An underlying assumption of TDD is that you have a testing
            framework available to you.  Agile software developers often use
            the xUnit family of open source tools, such as JUnit or VBUnit,
            although commercial tools are also viable options.  Without such
            tools TDD is virtually impossible. Next figure presents a UML state
            chart diagram for how people typically work with the xUnit tools
            (source Keith Ray).  
          </p>
          <figure>
            <img src='images/unit4-fig5.png'>
            <figcaption>Testing via xUnit</figcaption>
          </figure>
          <p>
            Kent Beck, who popularized TDD, defines two simple rules for TDD
            (Beck 2003):
          </p>
          <ul>
            <li>
              First, you should write new business code only when an automated
              test has failed.
            </li>
            <li>
              Second, you should eliminate any duplic  ation that you find.
            </li>
          </ul>
          <p>
            Beck explains how these two simple rules generate complex
            individual and group behavior:
          </p>
          <ul>
            <li>
              You design organically, with the running code providing feedback
              between decisions.
            </li>
            <li>
              You write your own tests because you can't wait 20 times per day
              for someone else to write them for you. 
            </li>
            <li>
              Your development environment must provide rapid response to small
              changes (e.g you need a fast compiler and regression test suite).
            </li>
            <li>
              Your designs must consist of highly cohesive, loosely coupled
              components (e.g. your design is highly normalized) to make
              testing easier (this also makes evolution and maintenance of your
              system easier too).
            </li>
          </ul>
          <p>
            For developers, the implication is that they need to learn how to
            write effective unit tests.
          </p>
        </section>
        <section>
          <h3>TDD also improves documentation</h3>
          <p>
            Most programmers don't read the written documentation for a system,
            instead they prefer to work with the code. And there's nothing
            wrong with this. When trying to understand a class or operation
            most programmers will first look for sample code that already
            invokes it.  Well-written unit tests do exactly this - they provide
            a working specification of your functional code - and as a result
            unit tests effectively become a significant portion of your
            technical documentation. The implication is that the expectations
            of the pro-documentation crowd need to reflect this reality.
            Similarly, acceptance tests can form an important part of your
            requirements documentation. This makes a lot of sense when you stop
            and think about it. Your acceptance tests define exactly what your
            stakeholders expect of your system, therefore they specify your
            critical requirements. Your regression test suite, particularly
            with a test-first approach, effectively becomes detailed executable
            specifications.
          </p>
          <p>
            Are tests sufficient documentation? Very likely not, but they do
            form an important part of it. For example, you are likely to find
            that you still need user, system overview, operations, and support
            documentation. You may even find that you require summary
            documentation overviewing the business process that your system
            supports.  When you approach documentation with an open mind, I
            suspect that you will find that these two types of tests cover the
            majority of your documentation needs for developers and business
            stakeholders.  Furthermore, they are an important part of your
            overall efforts to remain as agile as possible regarding
            documentation.
          </p>
        </section>
        <section>
          <h3>Why TDD</h3>
          <p>
            A significant advantage of TDD is that it enables you to take small
            steps when writing software. This is far more productive than
            attempting to code in large steps. For example, assume you add some
            new functional code, compile, and test it. Chances are pretty good
            that your tests will be broken by defects that exist in the new
            code. It is much easier to find, and then fix, those defects if
            you've written two new lines of code than two thousand. The
            implication is that the faster your compiler and regression test
            suite, the more attractive it is to proceed in smaller and smaller
            steps. I generally prefer to add a few new lines of functional
            code, typically less than ten, before I recompile and rerun my
            tests.
          </p>
          <p>
            The act of writing a unit test is more an act of design than of
            verification. It is also more an act of documentation than of
            verification.  The act of writing a unit test closes a remarkable
            number of feedback loops, the least of which is the one pertaining
            to verification of function.
          </p>
          <p>
            The first reaction that many people have to agile techniques is
            that they're ok for small projects, perhaps involving a handful of
            people for several months, but that they wouldn't work for "real"
            projects that are much larger. That's simply not true.  Beck (2003)
            reports working on a Smalltalk system taking a completely
            test-driven approach which took 4 years and 40 person years of
            effort, resulting in 250,000 lines of functional code and 250,000
            lines of test code. There are 4000 tests running in under 20
            minutes, with the full suite being run several times a day.
            Although there are larger systems out there, it's clear that TDD
            works for good-sized systems.
          </p>
        </section>
        <section>
          <h3>A simple example</h3>
          <p>
            You are asked to write a code to extract the UK postal area code
            from any given full UK postcode. Example Input "SS17 7HN", output
            should be "SS", for input "B43 4RW", output should be "B".
          </p>
          <p>
            Before starting coding, you need to create the tests and for doing
            so you need to think about the structure of the software.  An
            obvious approach is creating a class named <i>PostCode</i> with a
            method that retrieves the postal area code (e.g.  areaCode()). In
            that way, in order to calculate the are code something similar to:
          </p>
          <p>
            <code>postCode = new PostCode("SS17 THN");</code>
          </p>
          <p>
            <code>string area = postCode.areaCode();</code>
          </p>
          <p>
            Hence, the first thing that should be done is developing the test
            cases for such a solution, as you have been already provided with
            two examples, a good idea is using them as the test cases:
          </p>
          <ul>
            <li>
              Test Case 1: ("SS17 THN") -> "SS"
            </li>
            <li>
              Test Case 2: ("B43 4RW") -> "B"
            </li>
          </ul>
          <p>
            Depending on the programming language, you should build those test
            cases through the xUnit tool. If you try to execute them, both are
            going to fail because the class PostCode does not exist yet.
          </p>
          <p>
            The next step is building an empty class PostCode with a method
            postalArea that always return the string <i>postcode</i>.
          </p>
          <p>
            In this case, again the tests are going to fail, as the return
            value is not the expected one.
          </p>
          <p>
            In the following iteration we could implement the solution for the
            first part of the problem, when the postal code is 8 chars long,
            the area code is composed by the 2 first ones. If we try again to
            run the tests, the first one ("SS17 THN") will complete
            successfully whereas the second one will fail. We have added a very
            reduce functionality, and we have nearly immediately checks that
            what we has added is correct.
          </p>
          <p>
            In the following iteration we could implement the solution for the
            second part of the problem, when the postal code is 7 chars long,
            the area code is composed by the firs ones. If we try again to run
            the tests, both will complete successfully. Again, we have just
            added very few lines code (e.g. could be 2-3), and have checked
            immediately that what we has added is correct.
          </p>
          <p>
            Imagine now that you have been told that there are postal codes of
            8 digits in which only the first one denotes the area, e.g.  "W1Y2
            3RD", now you should first create a new test case for that new
            example. Obviously, it will fail as it will return "W1" instead of
            "W " but adding the funcionality to implement the new feature
            should be quite safe as it would be easy to check if in the process
            of implementing it you are breaking the existing features.
          </p>
        </section>
      </section>
    </section>
    <section>
      <h1> QA Activities beyond Testing </h1>
      <p>
        Although testing is one of the key QA activities, there are many
        additional actions that could be taken in order to assure that the
        quality of the product meets some targets. Some of them are going to be
        analysed in this chapter.
      </p>
      <section>
        <h2>Defect Prevention and Process Improvement</h2>
        <p>
          The best way to avoid defects is preventing them and the most common
          technique for doing so is <i>Defect Causal Analysis (DCA)</i>.  This
          type of analysis consists in identifying the causes of defects and
          other problems and taking action to prevent them from occurring in
          the future by improving the process and reducing the causes that
          originate the defects.
        </p>
        <p>
          DCA can be seen as a systematic process to identify and analyze
          causes associated with the occurrence of specific defect types,
          allowing the identification of improvement opportunities for the
          organizational process assets and the implementation of actions to
          prevent the occurrence of that same defect type in future projects.
          DCA is used by many companies, for instance, HP has extesively used
          it with very good results [[SOFTWARE-FAILURE-ANALYSIS-HP]].
        </p>
        <p>
          There are multiple methodologies to implement a DCA system, but in
          general, the following activities should be conducted in all of them:
        </p>
        <ol>
          <li>
            <p>
              <b>Defect Identification:</b> Defects are found by QA activities
              specifically intended to detect defects such as Design review,
              Code Inspection, function and unit testing.
            </p>
          </li>
          <li>
            <p>
              <b>Defect Classification:</b> Once defects are identified they
              need to be classified. There are multiple ways and techniques to
              classify defects, for instance: Requirements, Design, Logical and
              Documentation.  These categories can be again divided in second
              and third levels depending on the complexity and size of the
              product.
            </p>
            <p>
              Orthogonal Defect Classification (ODC) [[ODC]] is one of the most
              important techiques used for clasifying defects.  It means that a
              defect is categorized into classes that collectively point to the
              part of the process which needs attention, much like
              characterizing a point in a Cartesian system of orthogonal axes
              by its (x, y, z) coordinates.  
            </p>
          </li>
          <li>
            <p>
              <b>Defect Analysis:</b> After defects are logged and classified,
              the next step is to review and analyze them using root cause
              analysis (RCA) techniques.
            </p>
            <p>
              As doing a defect analysis for all the defects is a big effort.
              A useful tool before doing this kind of analysis is a Pareto
              chart.  This kind of charts shows the defect type with the
              highest frequency of occurrence of defects. It shows the
              frequencies of occurrences of the various categories of problems
              encountered, in order to determine which of the existing problems
              occur most frequently. The problem categories or causes are shown
              on the x-axis of the bar graph and the cumulative percentage is
              shown on the y-axis of the graph. Such a diagram helps us to
              identify the defect types should be given higher priority and
              must be attended first.
            </p>
            <p>
              For instance, the following picture shows an example of a Pareto
              diagram:
            </p>
            <figure>
              <img src='images/unit5-fig1-pareto.gif'>
              <figcaption>Pareto Diagram Example</figcaption>
            </figure>
            <p>
              Root-cause analysis is the process of finding the activity or
              process which causes the defects and find out ways of eliminating
              or reducing the effect of that by providing remedial measures.
            </p>
            <p>
              Defects are analyzed to determine their origins. A collection of
              such causes will help in doing the root cause analysis. One of
              the tools used to facilitate root cause analysis is a simple
              graphical technique called cause-and-effect diagram / fishbone
              diagram which is drawn for sorting and relating factors that
              contribute to a given situation.
            </p>
            <p>
              It is important that this process uses the knowledge and
              expertise of the team and that it considers that the target is
              providing information and analysis in a way that helps
              implementing changes in the prcoesses that help prevent defects
              later on.
            </p>
            <p>
              For instance, the following picture shows an example of a
              Fishbone diagram:
            </p>
            <figure>
              <img src='images/unit5-fig2-fishbone.gif'>
              <figcaption>Fishbone Diagram Example</figcaption>
            </figure>
          </li>
          <li>
            <p>
              <b>Defect Prevention:</b> Once the causes of the defects are
              known it is key to identify actions that can be put in place to
              cut down these causes.  This can be achieved, for intstance with
              meetings where all the possible causes are identified from the
              cause-and-effect diagram and debated among the team. All
              suggestions are listed and then the ones that are identified as
              the main reasons for causes are separated out. For these causes,
              possible preventive actions are discussed and finally agreed
              among project team members.
            </p>
          </li>
          <li>
            <p>
              <b>Process Improvement:</b> Once the preventive actions have been
              identified, they need to be put in place and verify their
              effectiveness, for instance by observing the Defect Density and
              comparing it with previous projects.
            </p>
          </li>
        </ol>
        <p>
          You can find some examples and more details about this process at
          [[DEFECT-PREVENTION-NUTSHELL]] and
          [[DEFECT-ANALYSIS-AND-PREVENTION]].
        </p>
      </section>
      <section>
        <h2>Code Inspection and Formal Verification</h2>
        <p>
          During many years, people considered that the only consumers of
          software were machines and human beings were not intended to review
          the code after it was written. This attitude began to change in the
          early 1970s through the efforts of many developers who saw the value
          in reading code as part of a QA culture.
        </p>
        <p>
          Nowadays, not all the companies apply techniques based in reading
          code as part of their Software Development (including QA) process,
          but the concept of studying program code as part of defect removal
          preocess is widely accepted as benefitial. Of course, the likelihood
          of those techniques being successful depend on multiple: factors the
          size or complexity of the software, the size of the development team,
          the timeline for development and, of course, the background and
          culture of the programming team.
        </p>
        <p>
          Part of the skepticism for this kind of methods is because many
          people believe that tasks lead by humans could lead to worse results
          than mathematical proofs conducted by a computer. However, it has
          been proven that simple and informal code review techniques
          contribute substantially to productivity and reliability in three
          major ways.
          <ul>
            <li>
              We know that the earlier errors are found, the lower the costs of
              correcting them and the higher the probability of fixing them
              them properly. We have just studied in previous section that the
              best way to reduce defects is preventing them, the second best
              way to reduce them is by conducting code reviews that detect the
              defects just while they are being injected.
            </li>
            <li>
              Lower error-correction costs: when an error is found it is
              usually located precisely in the code as opposed to black box
              testing where you only receive an unexpected result. Moreover,
              this process frequently exposes a batch of errors, allowing the
              errors to be corrected later together. Computer-based testing, on
              the other hand, normally exposes only a symptom of the error
              (e.g. the program does not terminate or the program prints a
              meaningless result), so developers need to identify the roots
              behind the symptnm which makes errors to be detected and
              corrected one by one.
            </li>
            <li>
              This kind of technique changes also the mind of developers.
              People reviewing code is also learning in parallel, people whose
              code is being reviewed is not only fixing defects but learning
              new techniques, paradigms, etc. that could be applied in future
              projects.
            </li>
          </ul>
        </p>
        <p>
          Code Reviews are generally effective in finding from 30 to 70 percent
          of the logic-design and coding errors in typical programs.  They are
          not effective, however, in detecting high-level design errors, such
          as errors made in the requirements analysis process.  Note that a
          success rate of 30 to 70 percent doesn't mean that up to 70 percent
          of all errors might be found but up to 70% of the defects that are
          going to be detected (remember we don't know how many defectsi are in
          a software).
        </p>
        <p>
          Of course, a possible criticism of these statistics is that the human
          processes find only the <i>easy</i> errors (those that would be
          trivial to find with computer-based testing) and that the difficult,
          obscure, or tricky errors can be found only by computer-based
          testing. However, some testers using these techniques have found that
          the human processes tend to be more effective than the computer-based
          testing processes in finding certain types of errors, while the
          opposite is true for other types of errors. This means that reviews
          and computer-based testing are complementary; error-detection
          efficiency will suffer if one or the other is not present.
        </p>
        <p>
          Different ways of performing code reviews exist and in the following
          sections we are going to assess few of them.
        </p>
        <section>
          <h3>Formal Code Inspection</h3>
          <p>
            For historical reasons, <i>formal</i> reviews are usually called
            <i>inspections</i>. This is due to the work Michael Fagan conducted
            and presented in his 1976 study at IBM regarding the efficacy of
            peer reviews. We are going to called them <i>Formal Code Inspection
            </i> to distinguish them from other types of Code Reviews.
          </p>
          <p>
            There is always a inspection team that usually consists of four
            people. One of them plays the role of moderator who should be an
            expert programmer, but not the author of the program (he does not
            need to be familiar with the software either).
          </p>
          <p>
            Moderator duties include: 
          </p>
          <ul>
            <li>
              Distributing materials for, and scheduling, the inspection
              session.
            </li>
            <li>
              Leading the session.
            </li>
            <li>
              Recording all errors found.
            </li>
            <li>
              Ensuring that the errors are subsequently corrected.
            </li>
          </ul>
          <p>
            The rest of the team is the developer of the code, a software
            architect (could be the architecture of the software) and a Quality
            Assurance engineer.
          </p>
          <p>
            The Inspection Agenda is distributed some days in advance of the
            Inspect Session. Together with the agenda, the moderator
            distributes the software, specification and any relevant material
            to the inspection team so they can become familiar with the
            material before the meeting takes place.
          </p>
          <p>
            During the review session the moderator ensures that two key
            activities take place:
            <ol>
              <li>
                The programmer describes, statement by statement, the logic of
                the software. Other participants are free (and encouraged) to
                raise questions in order to determine whether errors exist. It
                is likely that the developer himself, instead of the rest of
                team, is the one that find many of the errors identified during
                this stage. In other words, the simple act of reading aloud a
                program to an audience seems to be a remarkably effective
                error-detection technique.
              </li>
              <li>
                The program is analyzed with respect to checklists of
                historically common programming errors.
              </li>
            </ol>
          </p>
          <p>
            When the session is over, the programmer receives an error list
            that includes all the errors that have been discovered. Hence, the
            session is focused on <b> finding defects not fixing them.</b>
            Despite that, in some occasions, when a problem is discovered, the
            review team could propose and discuss some design changes. When
            some of the detected defects require significant changes in the
            code, the review team could agree to have follow-up meetings in
            order to review again the code after the changes are implemented.
          </p>
          <p>
            The list of errors is not only used by the developer in order to
            fix them; it is also used by moderator to verify if the error
            checklist could be improved with the results.
          </p>
          <p>
            The review sessions are typically very dynamic and hence the
            moderator should be responsible not only for reviewing the code but
            also to keeping it focused so time is used efficiently (these
            sessions should be of 90-120 minutes maximum).
          </p>
          <p>
            This kind of approaches requires of the right attitude, specially
            from the developer whose work is going to be under scrutiny. He
            must forget about his ego and think about the process as a way to
            improve the quality of his work and improve his development skills,
            as he usually receives a lot of feedback about programming styles,
            algorithms and techniques. But it is not only the developer but
            also the rest of the team the ones who could learn by such an open
            exchange of ideas.
          </p>
          <p>
            The following diagram describes theis process grafically:
          </p>
          <figure>
            <img src='images/unit5-fig3-formal.png'>
            <figcaption>Formal Code Inspections Flow</figcaption>
          </figure>
          <p>
            The following tables describes some checklists used in formal code
            reviews as explained in [[ART-OF-TESTING]].
          </p>
          <figure>
            <img src='images/unit5-fig4-checklist1.png'>
            <figcaption>Checklist for Formal Inspections - 1</figcaption>
          </figure>
          <figure>
            <img src='images/unit5-fig5-checklist2.png'>
            <figcaption>Checklist for Formal Inspections - 2 </figcaption>
          </figure>
        </section>
        <section>
          <h3>Walkthrough</h3>
          <p>
            A Walkthrough is quite similar to "Formal Code Inspections" as it
            is also very formal, it is conducted by a team, and it takes place
            during a pre-scheduled session of 90-120 minutes. However, there is
            a key difference: the procedure during the meeting.  Instead of
            simply reading the software and use checklists, the participants
            "play computer", which means that a person that is designated as
            the tester comes to the meeting with a set of pre-defined test
            cases for the software.  During the meeting, each test case is
            mentally executed; that is, the test data are "walked through" the
            logic of the program. The state of the program is monitored on
            paper or a whiteboard.
          </p>
          <p>
            The test cases must not be a complete set of test cases, especially
            because every mental execution of a test case use to take a lot of
            time. The test cases themselves are not the critical thing; they
            are just an excuse for questioning the developer about the
            assumptions and decisions taken.  
          </p>
          <p>
            Although the size of the team is quite similar (three to five), the
            role of the participants is slightly different. Apart from the
            author of the software and a moderator, there are two key roles in
            walkthroughs: a tester role (that is the one responsible for
            guiding the execution of the test cases) and a secretary that
            writes down all the errors found. Additionally, other participants
            are welcome, typically experience programmers.
          </p>
        </section>
        <section>
          <h3>Over the shoulder Review</h3>
          <p>
            The two formal approaches described formerly, are good, and help to
            detect many defects. Additionally, they provide extra metrics and
            information about the effectiveness of the reviews themselves.
            However, this require a lot of effort, and consumes a lot of extra
            developer time. Many studies during the last yeasr have shown that
            there are other less formal methods that could achieve similar
            results but requiring less training and time.
          </p>
          <p>
            The first one we are going to study is over-the-shoulder reviews.
            This is the most common and informal of code reviews. An <i>
            over-the-shoulder</i> review is just that: a developer standing
          over the developer's computer while the author walks the reviewer
          through a set of code changes.
          </p>
          <p>
            Typically the author "drives" the review by sitting at the computer
            opening various files, pointing out the changes and explaining why
            it was done that way. Multiple tools can be used by the developer
            and it's usual to move back and forth between files.
          </p>
          <p>
            If the reviewer sees something wrong, they can take different
            actions, such as doing a little of "pair-programming" while the
            developer implements the fix or just take note of the issue to be
            solved offline.
          </p>
          <p>
            With cooperation tools such as videoconferencing, desktop sharing
            and so on, it is possible to perform this kind of reviews remotely
            but obviously, they are not so effective as the greatest asset of
            this technique is the closeness between developers and the easyness
            to take ad-hoc actions taken the opportunity of being together.
          </p>
          <p>
            The key advantage of this approach is its simplicity: no special
            training is required and can be done at any time without any
            preparation. It also encourages human interaction and encourages
            people to cooperate. Reviewers tend to be more verbose and brave
            when speaking than when they need to record their reviews in a
            system such as DataBase.
          </p>
          <p>
            Of course it has some drawbacks. The first one is that due to its
            informal nature, it is really difficult to be enforced, i.e.  there
            is no way (document, tool, etc...) to check if such a review has
            been conducted. The second one is that, as the author is the one
            leading the whole process he might omit parts of the code. The
            third one is the lack of traceability to check that the detected
            defects have been properly addressed.  
          </p>
          <p>
            The following diagram describes this process grafically
          </p>
          <figure>
            <img src='images/unit5-fig6-over_the_shoulder.png'>
            <figcaption>Over the shoulder reviews workflow</figcaption>
          </figure>
        </section>
        <section>
          <h3>Offline Reviews</h3>
          <p>
            This is the second-most common form of informal code review, and
            the technique preferred by most open-source projects. Here, whole
            files, or changes are packaged up (ZIP file, URL, Pull Request,
            etc...) by the author and sent to reviewers via e-mail or any other
            tool.  Reviewers examine the files offline, ask questions and
            discuss with the author and other developers, and suggest changes.
          </p>
          <p>
            Collecting the files to be reviewed was formerly a difficul task
            but nowadays, with Source Code Management systems such as Git, it
            is extremely easy to identify the files that the developer has
            modified and hence the changes he wants to merge into the main
            repository.  
          </p>
          <p>
            But SCM tools have helped not only to identifying the changes made
            by the developer, but also in other multiple areas such as:
            <ul>
              <li>
                Sending E-mail notifications: Request for review, review done,
                comments need to be addressed, etc...
              </li>
              <li>
                Recording review comments: Things to be changed, result of the
                review, whether the changes have been implemented or not, etc.
                This is key for having a way to enforce reviews.
              </li>
              <li>
                Combined display: Allow developers and reviewers to easily
                check the differences between files allowing different views.
              </li>
              <li>
                Discussion: Sometimes it's needed some type of discussion
                between the developer and the reviewer in order to undertand a
                bit more the code, clarify reasons behind some decisions, etc.
              </li>
            </ul>
          </p>
          <p>
            Obviously, the main advantage with respect to <i>over-the-shoulder
            </i> reviews is that it can work perfectly with developers that are
            not based in the same place, either across a building or across an
            ocean. Additionally, by using this technique is extremely easy to
            allow multiple reviewers to review the code in parallel, in many
            cases, if the reviews are done in an SCM system, even anyone with
            access to the SCM could comment in the review, even if he/she is
            not a reviewer.
          </p>
          <p>
            The main disadvantage with an over-the-shoulder review is that it
            takes longer as it usually requires different interactions, this
            could be especially painful if people are in different timezones.
          </p>
          <p> 
            In general, we could say that offline code reviews, done properly
            integrated in an SCM gets a good balance between speed,
            effectiveness and traceability.
          </p>
          <p>
            The following diagram describes this process grafically
          </p>
          <figure>
            <img src='images/unit5-fig7-offline.png'>
            <figcaption>Offline Code Reviews Workflow</figcaption>
          </figure>
        </section>
        <section>
          <h3>Pair Programming</h3>
          <p>
            Pair Programming it is a development process that incorporates
            continuous code review in the development process itself. It
            consists in two developers writing code at a single terminal with
            only one developer typing at a time and continuous free-form
            discussion and review.
          </p>
          <p>
            Studies of pair-programming have shown it to be very effective at
            both finding bugs and promoting knowledge transfer. However, having
            the reviewing developer so involved in the development itself is
            seen by many people as a risk to be biased: it's going to be more
            difficult for him to go a step back and critique the code from a
            fresh point of view. However, it could be argued that deep
            knowledge and understanding also provides him the capabiltiy to
            provide more effetive comments.
          </p>
          <p>
            The key difference with the other techniques mentioned above is
            that introducing this way of working affects not only how QA
            Activities are performed but also development ones (i.e. you could
            combine all the other review techniques with different ways of
            developing code). Adopting this way of working requires evaluating
            properly how are developers going to work in such an environment
            and the time required for working in this way.  
          </p> 
        </section> 
        <section> 
          <h3>Code Review Techniques: Summary</h3>
          <p>
            Each of the types of review is useful in its own way. Offline
            reviews strike a balance between time invested and ease of
            implementation.  In any case, and any kind of code review is better
            than nothing, but it should be also acknowledged that code reviews
            are not enough to guarantee the quality of a final product.
          </p>
        </section>
      </section>
      <section>
        <h2>Assertion Driven Development & Design by contract</h2>
        <section>
          <h3>Deffensive Programming</h3>
          <p>
            Deffensive Programing consists in including in the software as many
            checks as possible, even if they are redundant (e.g. checks made by
            callers and callees). Sometimes it's said, "maybe they don't help,
            but they don't harm either".
          </p>
          <p>
            The problem with this way of working, is that, in some cases, it
            ends-up adding a lot of redundancy "just in case", which means
            adding unnecessary complexity and increasing software size. The
            bigger and more complex a software is, the easier defects can
            affect it.
          </p>
          <p>
            The ideas behind deffensive software, are interesting, but in order
            to make these ideas have a possitive effect, a more systematic
            approach should be followed.
          </p>
        </section>
        <section>
          <h3>Contract Concept</h3>
          <p>
            A contract, in the real world, is an agreement between two parties
            in which each party expect some benefits from the contract if they
            meet some obligations. Both are linked, i.e. if the obligations are
            not met by any of the parties, there is no guarantee the benefits
            will happen. Those benefits and obligations are clearly documented
            so that there are no misunderstanding between the parties.
          </p>
          <p>
            Imagine a courier company that has a express service within Madrid
            city. That express service can be only done if the customer meets
            some conditios (e.g. the package is within the limits, the address
            is valid and in Madrid, the user pays...). If the customer meets
            this conditions, he gets the benefit of the package being delivered
            in 4 hours. If the customer does not meet them, there is no
            guarantee he can get the express deliver benefits. The following
            table shows the obligations/benefits of this example:
          </p>
          <table>
            <thead>
              <tr>
                <th>
                  Party
                </th>
                <th>
                  Obligations
                </th>
                <th>
                  Benefits
                </th>
              </tr>
            </thead>
            <tr>
              <td>
                Client
              </td>
              <td>
                Provide letter or package of no more thant 5 Kilograms, each
                dimension no more than 2 meters. Pay 100 Euros. Provide a valid
                recipient address in Madrid.
              </td>
              <td>
                Get package delivered witouth any damage to recipient in 4
                hours or less.
              </td>
            </tr>
            <tr>
              <td>
                Supplier
              </td>
              <td>
                Deliver package to recipient in four hours or less.
              </td>
              <td>
                No need to deal with deliveries too big, too heavy or unpaid.
              </td>
            </tr>
          </table>
          <p>
            One important remark, is that when a contract is exhaustive, there
            is a guarantee that all the obligations are related to the
            benefits.  This is also called the "No hidden clause" rule. This
            does not mean that the contract could not refer to external laws,
            best practices, regulation... it only means they do not need to be
            explicitly stated. For instance, in case the courier fails to meet
            their obligations, it is highly likely a law establishes a
            compensation to the customer.
          </p>
        </section>
        <section>
          <h3>Contracts in Software</h3>
          <p>
            It is easy to understand how the concept of contracts in the real
            world could be extrapolated to software development. In software
            every task can be split in multiple sub-tasks, the idea of
            sub-tasks is similar of contracting something to a company. I
            create a function, module, etc... that handles this part that is
            essential to meet the complete task.
          </p>
          <pre>
           task is
           do
             subtask1:
             subtask2:
             subtask3:
           end
          </pre>
          <p>
            If all the subtasks are completed correctly, the task will be also
            finished successfully. If there is a contract between the task and
            the subtasks, the task will have some guarantees about the
            completion. Subtasks in software developmentare typically
            functions, object methods...
          </p>
          <p>
            Please also think about the Spotify way of working in which they
            created an architecture that manage every team to deliver different
            parts of Spotify client independently. It is quite similar, they
            have divided the main task (the Spotify client) in multiple
            subtasks (the components of the architecture). If all the
            components behave properly, the final task will be working properly
            too.
          </p>
        </section>
        <section>
          <h3>Design By Contract</h3>
          <p>
            Design by Contract (DbC) is based on the definition of formal,
            precise and verifiable interface specifications for every software
            component. These specifications extend the ordinary definition of
            abstract types with preconditions, postconditions and invariants.
            Those specifications are also known as contracts.
          </p>
          <p>
            A software contract could be defined as the set of three different
            things:
            <ul>
              <li>
                Preconditions: A certain conditions to be guaranteed on entry
                by any client module that calls it. It is an obligation for the
                client module and a benefit for the supplier (no need to handle
                cases outside of the precondition)
              </li>
              <li>
                Postconditions: Gurantee a certain property on exit. This is an
                obligation for the supplier and a benefit for the client.
              </li>
              <li>
                Class-invariant: Guarantee that certain properties are not
                going to be changed on exit.
              </li>
            </ul>
          </p>
          <p>
            This could be formalized as three questions developers must try to
            solve when implementing a function:
            <ul>
              <li>
                What does contract expect?
              </li>
              <li>
                What does contract gurantee?
              </li>
              <li>
                What does contract maintain?
              </li>
            </ul>
          </p>
        </section>
        <section>
          <h3>Using Design By Contract</h3>
          <p>
            The ideal environment for <i>Design by Contract</i> is one in which
            the language developers use has support for it in a native way.
            Unfortunately not too many of them support this capability, being
            Eiffel the most known one. For those languages, the contract is
            part of the function definition. For instance, see an Eiffel
            example below:
          </p>
          <pre>
          class ACCOUNT create
            make
            feature
              ... Attributes as before:
                  balance , minimum_balance , owner , open ...
              deposit (sum: INTEGER) is
                    -- Deposit sum into the account.
                require
                  sum >= 0
                do
                  add (sum)
                ensure
                  balance = old balance + sum
                end
          </pre>
          <p>
            In programming languages with no direct support, in most of the
            cases assertions are used as a way to implement DbC techniques.
            There are libraries that try to simplify the process of defining
            these assertions. An assertion is a predicate used to indicate that
            if the software is a correct status, the predicate should be always
            true at that place. If an assertion evaluates to false, that should
            mean that the software is in a wrong status (e.g. the contract has
            been broken).
          </p>
          <p>
            Of course, the functions can still do some checkings, but only for
            conditions that are not part of the contract. The idea of DbC is
            removing any duplication and minimizing the amount of code
            necessary to check that the contract is met.
          </p>
        <section>
          <h3>Monitoring the assertions</h3>
          <p>
            One question that could be raised is "What happens if one of these
            conditions fails during execution?". This depens on whether
            assertions are monitored or not during runtime (and this use to be
            customizable depending on developer needs), but it is not a
            critical aspect. The target of DbC is implementing reliable
            software that work, what happens when they do not work is
            interesting, but not the main target.
          </p>
          <p>
            Developer can choose from various levels of assertion monitoring:
            no checking, preconditions only, pre and postconditions, conditions
            and invariants...
          </p>
          <p>
            If a developer decides not to check assertions, the assertions or
            contracts do not have any impact on system execution. If a
            condition is not met, then the software could be in an error
            situation and no extra actions will be taken, these are just bugs.
            In most of the cases this is the typical configuration for released
            products.
          </p>
          <p>
            If a developer decides to check assertions, the effect of
            assertions not met is typically an exception being fired. The
            typical use case for enabling assertion checking is debugging, i.e.
            detecting defects not in blind but based on consistency conditions.
            In most of cases this is the typical configuration for released
            products. 
          </p>
          <p>
            There might be also special treating of these exceptions, for
            instance in Eiffel routines a <i>rescue</i> clause which expresses
            the alternate behaviour of the routine (and is similar to clauses
            that occur in human contracts, to allow for exceptional, unplanned
            circumstances).  When a routine includes a rescue clause, any
            exception occurring during the routine's execution interrupts the
            execution of the body and starts the execution of the rescue
            clause. This could be used for shielding the code inn some
            situations.
          </p>
        </section>
      </section>
      <section>
        <h2>Fault Tolerance and Failure Containment</h2>
        <p>
          There is no practical way to guarantee that a given software has no
          bugs. It doesn't matter how good are our tools, methodologies and
          engineers... It doesn't matter how deep inspections and testings are
          either. In many cases the presence of bugs is tolerated as something
          "natural", but there are some systems were the reliability and
          security requirements are so important that extra measures should be
          taken to mitigate the consequences of undetected bugs.
        </p>
        <p>
          When a system has extreme reliability requirements, fault tolerant
          solutions should be put in place. The idea behind fault tolerant
          solutions consists in breaking the bug/failure cause/effect
          relationship. A result of this is a increase of the reliability as
          reliability is inversely proportional to the frequency of failures.
          These techniques are usually expensive as they typically require
          redundancy of sort so they are only in systems that require it.
        </p>
        <p>
          For instance, the software used in the flight control systems is one
          example of software with very extreme requirements about failures.
          The report [[CHALLENGES-FAULT-TOLERANT-SYSTEMS]] provides more
          details about the challenges that this kind of systems pose to
          software developers.
        </p>
        <p>
          However, in some situations, failures cannot be prevented and hence
          reliability cannot be improved. However, there are ways to minimize
          the consequences of failures with the target of maximizing safety.
          It is important we don't confuse reliability with safety: for
          instance a medical system could not be 100% reliable but it should be
          100% safe. The techniques intended to increase system safety are
          called failure containment techniques.
        </p>
        <p>
          In this chapter we are going to study both type of techniques.
        </p>
          <section>
          <h3>Fault Tolerance</h3>
          <p>
            Fault Tolerance techniques are used to tolerate software faults and
            prevent system failures from occurring when a fault occurs.  These
            kind of techniques are used in software that is very sensitive to
            failures such as aerospacial software, nuclear power, healthcare...
          </p>
          <section>
            <h4>Single Version Software Environments (No Redundancy)</h4>
            <p>
              In this case, only one instance of the software exists, and it
              tries to detect faults and recover from them without the need to
              replicate the software. This kind of techniques are really
              difficult to be implemented, as it has been studied that
              efficient fault tolerant systems require some kind of redundancy
              as we will see in next section.
            </p>
          </section>
          <section>
            <h4>Multiple Version Software Environments (Redundancy)</h4>
            <p>
              Redundancy in real world activities is the best way to increase
              reliability: multiple engines in a plane, lights in a car...  For
              instance, the NASA performed a research to calculate the
              possibility of survival in a mission depending of the amount of
              redundant equipment in the spacecraft and the results
              demonstrated that the survival chances are extremely dependent on
              redundancy as show in the figure below.
            </p>
            <figure>
              <img src='images/unit5-fig8-survival-vs-redundancy.svg' width='500px' height='400px'>
              <figcaption>Mission Survival vs. Material Redundancy (source NASA)</figcaption>
            </figure>
            <p>
              The same is true for software systems but with some caveats.
              Redundancy in software only works if the redundant system works
              when the original one fails: this is normal in hardware systems
              but not in software. If I have two identical software systems and
              the first one fails, it is extremely likely the second one fails
              too. Due to this is important that redundant software systems are
              uncorrelated. Designing uncorrelated systems usually requires two
              teams, working isolated, with different techniques... This means
              duplicating at least the development cost.
            </p>
            <p>
              In these systems, those multiple instances of the software
              developed independently can work in different configurations:
              N-Version programming (NVP), Recovery Blocks (RcB), N self
              checking programming (NSCP)...
            </p>
            <section>
              <h5>Software Fault Tolerance</h5>
              <ol type="A">
                <li>
                  Recovery blocks: Use repeated executions (or redundancy over
                  time) as the basic mechanism for fault tolerance. The
                  software includes a set of "recovery points" in which the
                  status is recorded so that they could be used as fallbacks in
                  case something goes wrong. When a piece of code is executed,
                  a "test acceptance" is internally executed, if the result is
                  OK, a new "recovery point" is set-up, if the result is not
                  acceptable, then the software returns to the previous
                  "recovery point" and an alternative to the faulty code is
                  enacted. This process continues until the "acceptance test"
                  is passed or no more alternatives are available, which leads
                  to a failure. Some key characteristics about this scheme that
                  is depicted in <a href='#fig-recovery-blocks'></a>: 
                  <ul type="a">
                    <li>
                      It is a backward error recovery technique: when an
                      error occurs, appropriate actions are taken to react
                      but no preventing action is taken.
                    </li>
                    <li>
                      It is a "serial technique" in which the same
                      functionality (the recovery block) is never executed in
                      parallel.
                    </li>
                    <li>
                      The "acceptance test" algorithm is the critical
                      part to success as well as the availability of recovery
                      blocks designed in different ways to the original code.
                    </li>
                  </ul>
                  <figure>
                    <img src='images/unit1-fig4-recoveryblocks.png' height='400px' width='500px'>
                    <figcaption>Recovery Blocks</figcaption>
                  </figure>
                </li>
                <li>
                  <p>
                    NVP (N-version programming):
                  </p>
                  <p>
                    This technique uses parallel redundancy, where N copies,
                    each of a different version, of codes fulfilling the same
                    functionality are running in parallel with the same
                    inputs. When all of those N-copies have completed the
                    operation, an adjudication process (decision unit) takes
                    place to determine (based in a more or less complex vote)
                    the output.
                  </p>
                  <p>
                    Some key characteristics about this scheme that is
                    depicted in 
                    <a href='#fig-n-version-programming'></a>
                  </p>
                  <ul type="a">
                    <li> 
                      It is a forward error recovery technique: preventive
                      actions are taken. Even if no error occurs the same
                      functionality is executed multiple times.
                    </li>
                    <li>
                      It is a "parallel technique" in which the same
                      functionality is always executed in parallel by
                      different versions of the same functionality.
                    </li>
                    <li>
                      The "decision unit" algorithm is the critical part
                      to success as well as the availability of different
                      versions of the same code designed in different ways.
                    </li>
                  </ul>
                  <figure>
                    <img src='images/unit1-fig5-Nversion.png' height='400px' width='500px'>
                    <figcaption>N version programming</figcaption>
                  </figure>
                  <p>
                    Obviously a wide range of different variants of those
                    systems have been proposed based in multiple combinations
                    of them [[COST-EFFECTIVE-FAULT-TOLERANCE]] and multiple
                    comparisons between the performance are also available
                    [[PERFORMANCE-RB-NVP-SCOP]].
                  </p>
                </li>
              </ol>
              <p>
                What do you think are the key advantages and disadvantages
                of the two fault tolerance techniques described (Recovery
                Blocks & N-Version)? Exercise 3: Recovery Blocks vs.
                N-Version
              </p>
            </section>
          </section>
          <section>
            <h4>Types of recovery</h4>
            <p>
              Error Recovery is the key part in fault tolerant systems.
              However, although the most important one, it's the last step in a
              series of 4 parts:
            </p>
            <ol>
              <li>
                Error Detection: This consists in the ability to detect that
                the Software is an erronous state, for instance, via asserions
                as it has been studied in this unit.
              </li>
              <li>
                Error Diagnosis: Assess the causes for the error situation in
                which the Software has fallen.
              </li>
              <li>
                Error Containment: Before trying to correct the error, we
                should stop the error propagation, so further damages will not
                happen.
              </li>
              <li>
                Error Recovery: Consists in replacing the erroneous state with
                an error-free state.
              </li>
            </ol>
            <p>
              Depending on how the new error-free state is calculated we could
              distinguish two approaches:
            </p>
            <ul>
              <li>
                Backward Recovery: In this mechanisms the system tries to go
                back to a previously saved state that the system knows it is
                correct. Recoding or saving these states is called
                checkpointing. Some systems instead of recording the states
                completely keep deltas with respect to previous states, which
                is know as differential checkpointing. Recovery blocks is one
                example of this type of technique.
              </li>
              <li>
                Forward Recovery: In this approach, the system tries to find a
                new state from which the system can continue the operation.
                This state can be calculated by Error Compensation. Error
                Compensation relies on redundancy based algorithms in which the
                redundancy system provides a set of potential results from
                which a compensation is executed to derive an answer deemed as
                acceptable. An example of this technique is N-Version
                programming. This techniques are more complex and requires more
                resources so they are mostly used when the system is critical
                with respect to delays (i.e. there is no time for a backward
                recovery).
              </li>
            </ul>
          </section>
        </section>
        <section>
          <h3>Failure Containment</h3>
          <p>
            There is software that is used in safety critical systems, that
            have severe consequences in case a failure occurs. In those
            situations it is very important to avoid some of the potential
            accidents or at lt
          </p>
          <p>
            Various specific techniques are used for this kind of systems, most
            of them based on the analysis of the potential hazards linked to
            the failures: 
          </p>
          <ul>
            <li>
              Hazard Elimination through substitution, simplification,
              decoupling, elimination of specific human errors and reduction of
              hazardous materials or conditions. These techniques reduce
              certain defect injections or substitute non-hazardous ones for
              hazardous ones. The general approach is similar to the defect
              prevention and defect reduction techniques surveyed earlier, but
              with a focus on those problems involved in hazardous situations.
            </li>
            <li>
              Hazard Reduction through design for controllability (for example,
              automatic pressure release in boilers), us of locking devices
              (for example, hardware/software interlocks), and failure
              minimization using safety margins and redundancy. These
              techniques are similar to fault tolerance, where local failures
              are contained without leading to system failures.
            </li>
            <li>
              Hazard control through reducing exposure, isolation and
              containments (for example barriers between the system and the
              environment), protection systems (active protection activated in
              case of hazard), and fail-safe design (passive protection, fail
              in a safe state without causing further damages). These
              techniques reduce the severity of failures, therefore weakening
              the link between failures and accidents.
            </li>
            <li>
              Damage control through escape routes, safe abandonment of
              products and materials, and devices for limiting physical damages
              to equipment or people. These techniques reduce the severity of
              accidents, thus limiting the damages cause by these accidents and
              related software failures.
            </li>
          </ul>
          <p>
            Notice that both hazard control and damage control above are
            post-failure activities that attempt to <b>contain</b> the failures
            so that they will not lead to accidents or the accident damage can
            be controlled or minimized. All these techniques are usually very
            expensive and process/technology intensive, hence they should be
            only applied when safety matters and deal with rare conditions
            related to accidents.  
          </p>
        </section>
      </section>
    </section>
  </body>
</html>
